#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åŠŸèƒ½14ï¼šæ¥è¿‘å†å²æœ€ä½ä»·ç­›é€‰æ¨¡å—

åŠŸèƒ½æè¿°ï¼šæ‰¾åˆ°å½“å‰ä»·æ ¼æ¥è¿‘æœ€è¿‘3å¹´å†…ä»·æ ¼æœ€ä½ç‚¹Â±10%ä»¥å†…çš„ä¸ªè‚¡

æŠ€æœ¯æ¶æ„ï¼ˆå®Œå…¨å¤ç”¨åŠŸèƒ½13ï¼‰ï¼š
- æ•°æ®è·å–ï¼šæ‰¹é‡SQL + è¿æ¥æ±  + å¢é‡ç¼“å­˜
- æ•°æ®å¤„ç†ï¼šå‘é‡åŒ–è®¡ç®— + é¢„è®¡ç®—æŒ‡æ ‡
- å¤šçº¿ç¨‹å¹¶å‘ï¼šé«˜æ€§èƒ½çº¿ç¨‹æ±  + æ‰¹é‡å¤„ç†
- ç¼“å­˜ç®¡ç†ï¼šå¢é‡ç¼“å­˜ + æ™ºèƒ½åˆå¹¶
"""

import os
import time
import logging
from datetime import datetime, timedelta, date
from types import SimpleNamespace
from typing import Dict, List, Optional, Tuple, Any
import warnings
warnings.filterwarnings('ignore')

import numpy as np
import pandas as pd

from data_fetcher import JuyuanDataFetcher
from config import STOCK_LIST_LIMIT, MAX_TRADING_DAYS_AGO
from high_performance_threading import HighPerformanceThreadPool
from futures_incremental_cache_manager import futures_incremental_cache_manager
from cache_validator import validate_cache_data
from scipy import stats

logger = logging.getLogger(__name__)


def calculate_percentile(current_value, historical_values):
    """
    è®¡ç®—å½“å‰å€¼åœ¨å†å²æ•°æ®ä¸­çš„åˆ†ä½æ•°
    
    å‚æ•°ï¼š
    - current_value: å½“å‰å€¼
    - historical_values: å†å²å€¼åˆ—è¡¨
    
    è¿”å›ï¼š
    - åˆ†ä½æ•°ï¼ˆ0-100ï¼‰ï¼Œå¦‚æœæ•°æ®ä¸è¶³åˆ™è¿”å›None
    """
    if current_value is None:
        return None
    
    # æ¸…ç†å†å²æ•°æ®ï¼Œè¿‡æ»¤æ‰ç©ºå€¼å’Œæ— æ•ˆå€¼
    clean_historical = [v for v in historical_values if v is not None and not pd.isna(v) and v > 0]
    
    if len(clean_historical) < 5:  # è‡³å°‘éœ€è¦5ä¸ªæ•°æ®ç‚¹
        return None
    
    try:
        # è®¡ç®—åˆ†ä½æ•°ï¼ˆä½¿ç”¨scipy.stats.percentileofscoreï¼‰
        percentile = stats.percentileofscore(clean_historical, current_value)
        return round(percentile, 1)
    except Exception as e:
        logger.error(f"è®¡ç®—åˆ†ä½æ•°æ—¶å‡ºé”™: {e}")
        return None


def get_extended_fundamental_data(
    codes: List[str],
    fetcher: JuyuanDataFetcher,
    stock_data_dict: Dict[str, pd.DataFrame],
    years_for_percentile: int = 5,
    years_for_dividend: int = 3
) -> Dict[str, Dict[str, Any]]:
    """
    æ‰©å±•çš„åŸºæœ¬é¢æ•°æ®è·å–å‡½æ•°ï¼Œæ”¯æŒPE/PBåˆ†ä½æ•°å’Œè‚¡æ¯ç‡è®¡ç®—
    
    å‚æ•°ï¼š
    - codes: è‚¡ç¥¨ä»£ç åˆ—è¡¨
    - fetcher: æ•°æ®è·å–å™¨
    - stock_data_dict: è‚¡ç¥¨è¡Œæƒ…æ•°æ®å­—å…¸ {code: DataFrame}ï¼Œç”¨äºè·å–å½“å‰è‚¡ä»·
    - years_for_percentile: è®¡ç®—åˆ†ä½æ•°çš„å†å²å¹´æ•°ï¼ˆé»˜è®¤5å¹´ï¼‰
    - years_for_dividend: è®¡ç®—è‚¡æ¯ç‡çš„å¹´æ•°ï¼ˆé»˜è®¤3å¹´ï¼‰
    
    è¿”å›ï¼š
    - Dict[str, Dict]: {code: {pb_mrq, pe_ttm, pb_percentile, pe_percentile, 
                                dividend_info, dividend_yield, ...}}
    """
    result = {}
    
    if not codes:
        return result
    
    try:
        # æ‰¹é‡æŸ¥è¯¢åŸºæœ¬é¢æ•°æ®
        codes_str = ','.join([f"'{code}'" for code in codes])
        
        # 1. æŸ¥è¯¢æœ€æ–°è´¢åŠ¡æŒ‡æ ‡ï¼ˆPBã€PEã€ROE_TTMï¼‰
        sql_fundamental = f"""
        SELECT 
            s.SecuCode,
            s.InnerCode,
            m.ROETTM as ROE_TTM,
            v.PB as PB_MRQ,
            v.PETTMCut as PE_TTM
        FROM SecuMain s
        LEFT JOIN (
            SELECT 
                CompanyCode,
                ROETTM,
                ROW_NUMBER() OVER (PARTITION BY CompanyCode ORDER BY EndDate DESC) as rn
            FROM LC_MainIndexNew
        ) m ON s.CompanyCode = m.CompanyCode AND m.rn = 1
        LEFT JOIN (
            SELECT 
                InnerCode,
                PB,
                PETTMCut
            FROM DZ_DIndicesForValuation
        ) v ON s.InnerCode = v.InnerCode
        WHERE s.SecuCode IN ({codes_str})
          AND s.SecuCategory = 1
        """
        
        # 2. æŸ¥è¯¢è¿‘3å¹´åˆ†çº¢æ•°æ®
        sql_dividend = f"""
        SELECT 
            s.SecuCode,
            m.DividendPS,
            m.DividendPaidRatio as DividendPayoutRatio,
            m.EndDate
        FROM SecuMain s
        INNER JOIN LC_MainIndexNew m ON s.CompanyCode = m.CompanyCode
        WHERE s.SecuCode IN ({codes_str})
          AND s.SecuCategory = 1
          AND m.DividendPS > 0
          AND m.EndDate >= DATEADD(YEAR, -{years_for_dividend}, GETDATE())
        ORDER BY s.SecuCode, m.EndDate DESC
        """
        
        # 3. æŸ¥è¯¢å†å²ä¼°å€¼æ•°æ®ï¼ˆç”¨äºè®¡ç®—åˆ†ä½æ•°ï¼‰
        # DZ_DIndicesForValuationè¡¨åŒ…å«ä¸åŒäº¤æ˜“æ—¥æœŸçš„å†å²æ•°æ®ï¼Œå‚è€ƒfundamental_analysis.pyçš„å®ç°æ–¹å¼
        # ä½¿ç”¨TradingDayå­—æ®µæŸ¥è¯¢å†å²æ•°æ®
        df_historical_valuation = pd.DataFrame()
        
        # å‚è€ƒfundamental_analysis.pyçš„å®ç°ï¼šç›´æ¥æŸ¥è¯¢å†å²æ•°æ®ï¼Œä¸åŒ…è£…åœ¨try-exceptä¸­
        # å¦‚æœå­—æ®µåä¸å¯¹ï¼Œä¼šæŠ›å‡ºå¼‚å¸¸ï¼Œä¾¿äºè°ƒè¯•
        # æ³¨æ„ï¼šè¿™é‡Œä¸è¦å†ççŒœæ—¥æœŸå­—æ®µåï¼Œç›´æ¥æŠŠæ•´å¼ è¡¨é‡Œè¯¥è‚¡ç¥¨çš„å†å²PB / PEéƒ½æ‹¿å‡ºæ¥ï¼Œ
        # åªä¾èµ– InnerCode å…³è”å’Œ PB / PETTMCut ä¸¤ä¸ªå­—æ®µï¼Œåç»­åœ¨ Python é‡Œåšå¹´ä»½è¿‡æ»¤å’Œæ¸…æ´—ã€‚
        # è¿™æ ·å¯ä»¥æœ€å¤§ç¨‹åº¦è´´è¿‘å…¶ä»–æ¨¡å—ã€Œå…ˆæŠŠå†å²å€¼å–å…¨ï¼Œå†åœ¨å†…å­˜é‡Œç®—åˆ†ä½æ•°ã€çš„åšæ³•ã€‚
        sql_historical_valuation = f"""
        SELECT 
            s.SecuCode,
            v.PB,
            v.PETTMCut as PE
        FROM SecuMain s
        INNER JOIN DZ_DIndicesForValuation v ON s.InnerCode = v.InnerCode
        WHERE s.SecuCode IN ({codes_str})
          AND s.SecuCategory = 1
          AND v.PB IS NOT NULL
          AND v.PETTMCut IS NOT NULL
        """
        
        # å¹¶è¡Œæ‰§è¡Œ3ä¸ªSQLæŸ¥è¯¢ï¼ˆå‚è€ƒfundamental_analysis.pyçš„å®ç°æ–¹å¼ï¼‰
        from concurrent.futures import ThreadPoolExecutor
        logger.info(f"å¼€å§‹å¹¶è¡ŒæŸ¥è¯¢åŸºæœ¬é¢æ•°æ®ï¼ˆå…± {len(codes)} åªè‚¡ç¥¨ï¼‰...")
        with ThreadPoolExecutor(max_workers=3) as executor:
            future_fundamental = executor.submit(fetcher.query, sql_fundamental)
            future_dividend = executor.submit(fetcher.query, sql_dividend)
            future_historical = executor.submit(fetcher.query, sql_historical_valuation)
            
            df_fundamental = future_fundamental.result()
            df_dividend = future_dividend.result()
            df_historical_valuation = future_historical.result()
            
            # æ£€æŸ¥å†å²ä¼°å€¼æ•°æ®æŸ¥è¯¢ç»“æœï¼ˆå‚è€ƒfundamental_analysis.pyçš„å¤„ç†æ–¹å¼ï¼‰
            if df_historical_valuation is None:
                logger.warning("å†å²ä¼°å€¼æ•°æ®æŸ¥è¯¢è¿”å›Noneï¼Œå¯èƒ½SQLæŸ¥è¯¢å‡ºé”™")
                df_historical_valuation = pd.DataFrame()
            elif df_historical_valuation.empty:
                logger.warning(f"å†å²ä¼°å€¼æ•°æ®æŸ¥è¯¢æˆåŠŸä½†æ•°æ®ä¸ºç©ºï¼Œå…±æŸ¥è¯¢ {len(codes)} åªè‚¡ç¥¨ï¼ŒSQLæ¡ä»¶å¯èƒ½è¿‡ä¸¥")
                logger.debug(f"å†å²ä¼°å€¼SQL: {sql_historical_valuation[:200]}...")
            else:
                unique_codes = df_historical_valuation['SecuCode'].nunique()
                logger.info(f"å†å²ä¼°å€¼æ•°æ®æŸ¥è¯¢æˆåŠŸï¼Œè·å–åˆ° {len(df_historical_valuation)} æ¡è®°å½•ï¼Œæ¶‰åŠ {unique_codes} åªè‚¡ç¥¨")
                # ç»Ÿè®¡æ¯åªè‚¡ç¥¨çš„å†å²æ•°æ®ç‚¹æ•°
                if unique_codes > 0:
                    code_counts = df_historical_valuation.groupby('SecuCode').size()
                    logger.debug(f"å†å²æ•°æ®ç‚¹æ•°ç»Ÿè®¡: å¹³å‡ {code_counts.mean():.1f} ç‚¹/è‚¡ç¥¨ï¼Œæœ€å°‘ {code_counts.min()} ç‚¹ï¼Œæœ€å¤š {code_counts.max()} ç‚¹")
        
        # å¦‚æœæŸ¥è¯¢å¤±è´¥ï¼Œè¿”å›ç©ºç»“æœ
        if df_fundamental is None:
            df_fundamental = pd.DataFrame()
        if df_dividend is None:
            df_dividend = pd.DataFrame()
        if df_historical_valuation is None:
            df_historical_valuation = pd.DataFrame()
        
        # å¤„ç†å†å²ä¼°å€¼æ•°æ®ï¼ˆç”¨äºè®¡ç®—åˆ†ä½æ•°ï¼‰
        historical_pb_data = {}  # {code: [pb_values]}
        historical_pe_data = {}  # {code: [pe_values]}
        
        if not df_historical_valuation.empty:
            df_historical_valuation['SecuCode'] = df_historical_valuation['SecuCode'].astype(str).str.zfill(6)
            historical_groups = df_historical_valuation.groupby('SecuCode')
            
            logger.info(f"å¤„ç†å†å²ä¼°å€¼æ•°æ®ï¼Œå…± {len(historical_groups)} åªè‚¡ç¥¨æœ‰å†å²æ•°æ®")
            
            for code, group in historical_groups:
                # æå–PBå’ŒPEçš„å†å²å€¼
                pb_values = group['PB'].dropna().tolist()
                pe_values = group['PE'].dropna().tolist()
                
                if pb_values:
                    # è¿‡æ»¤æ‰æ— æ•ˆå€¼ï¼ˆ<=0çš„å€¼ï¼‰
                    pb_values = [v for v in pb_values if v is not None and not pd.isna(v) and v > 0]
                    if pb_values:
                        historical_pb_data[code] = pb_values
                
                if pe_values:
                    # è¿‡æ»¤æ‰æ— æ•ˆå€¼ï¼ˆ<=0çš„å€¼ï¼‰
                    pe_values = [v for v in pe_values if v is not None and not pd.isna(v) and v > 0]
                    if pe_values:
                        historical_pe_data[code] = pe_values
            
            logger.info(f"å†å²PBæ•°æ®: {len(historical_pb_data)} åªè‚¡ç¥¨ï¼Œå†å²PEæ•°æ®: {len(historical_pe_data)} åªè‚¡ç¥¨")
        else:
            logger.warning("å†å²ä¼°å€¼æ•°æ®ä¸ºç©ºï¼Œæ— æ³•è®¡ç®—åˆ†ä½æ•°")
        
        # ä¼˜åŒ–ï¼šä½¿ç”¨å‘é‡åŒ–å¤„ç†ï¼Œé¿å…é€è¡Œè¿­ä»£
        if not df_fundamental.empty:
            df_fundamental['SecuCode'] = df_fundamental['SecuCode'].astype(str).str.zfill(6)
            for code in df_fundamental['SecuCode'].unique():
                row = df_fundamental[df_fundamental['SecuCode'] == code].iloc[0]
                current_pb = float(row['PB_MRQ']) if pd.notna(row['PB_MRQ']) else None
                current_pe = float(row['PE_TTM']) if pd.notna(row['PE_TTM']) else None
                
                # è®¡ç®—åˆ†ä½æ•°
                pb_percentile = None
                pe_percentile = None
                
                # ä¿®å¤ï¼šä½¿ç”¨ is not None è€Œä¸æ˜¯ç›´æ¥åˆ¤æ–­çœŸå€¼ï¼ˆå› ä¸ºPB/PEå¯èƒ½æ˜¯0æˆ–è´Ÿæ•°ï¼‰
                if current_pb is not None and current_pb > 0 and code in historical_pb_data:
                    pb_percentile = calculate_percentile(current_pb, historical_pb_data[code])
                    if pb_percentile is None:
                        logger.debug(f"è‚¡ç¥¨ {code} PBåˆ†ä½æ•°è®¡ç®—å¤±è´¥: current_pb={current_pb}, å†å²æ•°æ®ç‚¹æ•°={len(historical_pb_data[code])}")
                
                if current_pe is not None and current_pe > 0 and code in historical_pe_data:
                    pe_percentile = calculate_percentile(current_pe, historical_pe_data[code])
                    if pe_percentile is None:
                        logger.debug(f"è‚¡ç¥¨ {code} PEåˆ†ä½æ•°è®¡ç®—å¤±è´¥: current_pe={current_pe}, å†å²æ•°æ®ç‚¹æ•°={len(historical_pe_data[code])}")
                
                # è°ƒè¯•æ—¥å¿—ï¼šå¦‚æœå½“å‰å€¼å­˜åœ¨ä½†å†å²æ•°æ®ä¸å­˜åœ¨
                if current_pb is not None and current_pb > 0 and code not in historical_pb_data:
                    logger.debug(f"è‚¡ç¥¨ {code} æœ‰å½“å‰PBå€¼({current_pb})ä½†æ— å†å²PBæ•°æ®")
                if current_pe is not None and current_pe > 0 and code not in historical_pe_data:
                    logger.debug(f"è‚¡ç¥¨ {code} æœ‰å½“å‰PEå€¼({current_pe})ä½†æ— å†å²PEæ•°æ®")
                
                result[code] = {
                    'pb_mrq': current_pb,
                    'pe_ttm': current_pe,
                    'roe_ttm': float(row['ROE_TTM']) if pd.notna(row['ROE_TTM']) else None,
                    'pb_percentile': pb_percentile,
                    'pe_percentile': pe_percentile,
                    'dividend_info': [],
                    'dividend_yield': None,
                    'dividend_yield_avg_3y': None
                }
        
        # ä¼˜åŒ–ï¼šä½¿ç”¨groupbyæ‰¹é‡å¤„ç†åˆ†çº¢æ•°æ®ï¼Œé¿å…é€è¡Œè¿­ä»£
        if not df_dividend.empty:
            df_dividend['SecuCode'] = df_dividend['SecuCode'].astype(str).str.zfill(6)
            dividend_groups = df_dividend.groupby('SecuCode')
            
            for code, group in dividend_groups:
                if code not in result:
                    continue
                
                # è·å–å½“å‰è‚¡ä»·ï¼ˆä¼˜åŒ–ï¼šåªè·å–ä¸€æ¬¡ï¼‰
                current_price = None
                if code in stock_data_dict and not stock_data_dict[code].empty:
                    stock_df = stock_data_dict[code]
                    if 'Close' in stock_df.columns:
                        latest_date = stock_df.index.max()
                        current_price = float(stock_df.loc[latest_date, 'Close'])
                
                # å¤„ç†åˆ†çº¢æ•°æ®ï¼ˆå–æœ€è¿‘3å¹´ï¼‰
                dividends = []
                for _, row in group.head(3).iterrows():  # åªå–å‰3æ¡
                    dividends.append({
                        'date': row['EndDate'],
                        'dividend_ps': float(row['DividendPS']) if pd.notna(row['DividendPS']) else 0,
                        'payout_ratio': float(row['DividendPayoutRatio']) if pd.notna(row['DividendPayoutRatio']) else None
                    })
                
                result[code]['dividend_info'] = dividends
                
                # è®¡ç®—è‚¡æ¯ç‡
                if current_price and current_price > 0 and dividends:
                    # è®¡ç®—è¿‘3å¹´å¹³å‡è‚¡æ¯ç‡
                    total_dividend = sum([d['dividend_ps'] for d in dividends])
                    avg_dividend_yield = (total_dividend / current_price) * 100 if total_dividend > 0 else None
                    result[code]['dividend_yield_avg_3y'] = round(avg_dividend_yield, 2) if avg_dividend_yield else None
                    
                    # è®¡ç®—æœ€æ–°ä¸€å¹´çš„è‚¡æ¯ç‡
                    latest_dividend = dividends[0]['dividend_ps']
                    latest_dividend_yield = (latest_dividend / current_price) * 100 if latest_dividend > 0 else None
                    result[code]['dividend_yield'] = round(latest_dividend_yield, 2) if latest_dividend_yield else None
                else:
                    result[code]['dividend_yield'] = None
                    result[code]['dividend_yield_avg_3y'] = None
        
        # å¡«å……ç¼ºå¤±çš„è‚¡ç¥¨ï¼ˆç¡®ä¿æ‰€æœ‰ä»£ç éƒ½æ˜¯6ä½æ•°å­—æ ¼å¼ï¼‰
        for code in codes:
            # ç»Ÿä¸€æ ¼å¼åŒ–ä¸º6ä½æ•°å­—å­—ç¬¦ä¸²
            normalized_code = str(code).strip().zfill(6)
            # å¦‚æœä»£ç åŒ…å«éæ•°å­—å­—ç¬¦ï¼ˆå¦‚.SZï¼‰ï¼Œæå–æ•°å­—éƒ¨åˆ†
            if not normalized_code.isdigit():
                # æå–å‰6ä½æ•°å­—
                digits = ''.join([c for c in normalized_code if c.isdigit()])[:6]
                normalized_code = digits.zfill(6) if digits else normalized_code
            
            if normalized_code not in result:
                result[normalized_code] = {
                    'pb_mrq': None,
                    'pe_ttm': None,
                    'roe_ttm': None,
                    'pb_percentile': None,
                    'pe_percentile': None,
                    'dividend_info': [],
                    'dividend_yield': None,
                    'dividend_yield_avg_3y': None
                }
        
        return result
        
    except Exception as e:
        logger.error(f"è·å–æ‰©å±•åŸºæœ¬é¢æ•°æ®å¤±è´¥: {e}")
        # è¿”å›ç©ºå­—å…¸
        return {code: {
            'pb_mrq': None,
            'pe_ttm': None,
            'roe_ttm': None,
            'pb_percentile': None,
            'pe_percentile': None,
            'dividend_info': [],
            'dividend_yield': None,
            'dividend_yield_avg_3y': None
        } for code in codes}


def format_stock_code(code: str) -> str:
    """
    æ ¼å¼åŒ–è‚¡ç¥¨ä»£ç ï¼Œæ·»åŠ äº¤æ˜“æ‰€åç¼€
    - æ·±åœ³è‚¡ç¥¨ï¼ˆ00ã€30å¼€å¤´ï¼‰ï¼šæ·»åŠ .SZ
    - ä¸Šæµ·è‚¡ç¥¨ï¼ˆ60ã€68å¼€å¤´ï¼‰ï¼šæ·»åŠ .SH
    
    å‚æ•°ï¼š
    - code: è‚¡ç¥¨ä»£ç ï¼ˆ6ä½æ•°å­—å­—ç¬¦ä¸²ï¼‰
    
    è¿”å›ï¼š
    - æ ¼å¼åŒ–åçš„è‚¡ç¥¨ä»£ç ï¼ˆå¸¦åç¼€ï¼‰
    """
    if not code:
        return code
    
    code_str = str(code).strip()
    
    # å¦‚æœå·²ç»æœ‰åç¼€ï¼Œç›´æ¥è¿”å›
    if '.' in code_str:
        return code_str
    
    # æ ¹æ®è‚¡ç¥¨ä»£ç å‰ç¼€æ·»åŠ åç¼€
    if code_str.startswith('00') or code_str.startswith('30'):
        return f"{code_str}.SZ"  # æ·±åœ³
    elif code_str.startswith('60') or code_str.startswith('68'):
        return f"{code_str}.SH"  # ä¸Šæµ·
    elif code_str.startswith('8'):
        return f"{code_str}.BJ"  # åŒ—äº¤æ‰€
    else:
        return code_str  # æœªçŸ¥äº¤æ˜“æ‰€ï¼Œä¿æŒåŸæ ·


def analyze_near_lowest_price(
    code: str,
    stock_data: pd.DataFrame,
    lookback_years: int = 3,
    price_tolerance: float = 0.10,
    cutoff_date: Optional[date] = None
) -> Optional[Dict[str, Any]]:
    """
    åˆ†æè‚¡ç¥¨æ˜¯å¦æ¥è¿‘å†å²æœ€ä½ä»·
    
    å‚æ•°ï¼š
    - code: è‚¡ç¥¨ä»£ç 
    - stock_data: è‚¡ç¥¨å†å²æ•°æ®ï¼ˆDataFrameï¼Œç´¢å¼•ä¸ºæ—¥æœŸï¼ŒåŒ…å«Closeåˆ—ï¼‰
    - lookback_years: å›çœ‹å¹´æ•°ï¼ˆé»˜è®¤3å¹´ï¼‰
    - price_tolerance: ä»·æ ¼å®¹å·®ï¼ˆé»˜è®¤0.10ï¼Œå³Â±10%ï¼‰
    
    è¿”å›ï¼š
    - åŒ…å«åˆ†æç»“æœçš„å­—å…¸ï¼Œå¦‚æœä¸æ»¡è¶³æ¡ä»¶æˆ–å‡ºé”™åˆ™è¿”å›None
    """
    try:
        if stock_data is None or stock_data.empty:
            return None
        
        # ç¡®ä¿æœ‰Closeåˆ—
        if 'Close' not in stock_data.columns:
            return None
        
        # ç¡®ä¿ç´¢å¼•æ˜¯æ—¥æœŸç±»å‹
        if not isinstance(stock_data.index, pd.DatetimeIndex):
            try:
                stock_data.index = pd.to_datetime(stock_data.index)
            except:
                return None
        
        # æŒ‰æ—¥æœŸæ’åº
        stock_data = stock_data.sort_index()
        
        # å¦‚æœæä¾›äº†cutoff_dateï¼Œè¿‡æ»¤æ•°æ®åªåˆ°è¯¥æ—¥æœŸ
        if cutoff_date is not None:
            # è½¬æ¢cutoff_dateä¸ºdateç±»å‹
            if isinstance(cutoff_date, str):
                cutoff_date_obj = pd.to_datetime(cutoff_date).date()
            elif isinstance(cutoff_date, pd.Timestamp):
                cutoff_date_obj = cutoff_date.date()
            elif isinstance(cutoff_date, date):
                cutoff_date_obj = cutoff_date
            else:
                cutoff_date_obj = None
            
            if cutoff_date_obj is not None:
                # è½¬æ¢cutoff_dateä¸ºTimestampä»¥ä¾¿æ¯”è¾ƒ
                cutoff_timestamp = pd.Timestamp(cutoff_date_obj)
                # è¿‡æ»¤æ•°æ®ï¼Œåªä¿ç•™åˆ°cutoff_dateçš„æ•°æ®
                stock_data = stock_data[stock_data.index <= cutoff_timestamp]
                if stock_data.empty:
                    return None
                # æ‰¾åˆ°stock_dataä¸­<=cutoff_dateçš„æœ€å¤§æ—¥æœŸä½œä¸ºlatest_date
                valid_dates = stock_data.index[stock_data.index <= cutoff_timestamp]
                if len(valid_dates) == 0:
                    return None
                latest_date = valid_dates.max()
            else:
                # è½¬æ¢å¤±è´¥ï¼Œä½¿ç”¨æ•°æ®ä¸­çš„æœ€å¤§æ—¥æœŸ
                latest_date = stock_data.index.max()
        else:
            # æ²¡æœ‰cutoff_dateï¼Œä½¿ç”¨æ•°æ®ä¸­çš„æœ€å¤§æ—¥æœŸ
            latest_date = stock_data.index.max()
        
        # è·å–æœ€æ–°äº¤æ˜“æ—¥çš„æ•°æ®
        latest_close = stock_data.loc[latest_date, 'Close']
        
        if pd.isna(latest_close) or latest_close <= 0:
            return None
        
        # è®¡ç®—å›çœ‹æ—¥æœŸï¼ˆ3å¹´å‰ï¼‰
        # ä¿®å¤ï¼šä½¿ç”¨æ›´å¤§çš„æ—¥æœŸèŒƒå›´ç¡®ä¿æœ‰è¶³å¤Ÿçš„äº¤æ˜“æ—¥æ•°æ®ï¼ˆ3å¹´çº¦750ä¸ªäº¤æ˜“æ—¥ï¼Œä½†æ—¥å†å¤©æ•°æ˜¯1095å¤©ï¼‰
        # å¤šè·å–100å¤©ä½œä¸ºç¼“å†²ï¼Œç¡®ä¿æœ‰è¶³å¤Ÿçš„äº¤æ˜“æ—¥æ•°æ®
        lookback_date = latest_date - pd.Timedelta(days=lookback_years * 365 + 100)
        
        # ç­›é€‰3å¹´å†…çš„æ•°æ®
        historical_data = stock_data[stock_data.index >= lookback_date]
        
        # è¦æ±‚1ï¼šå¦‚æœå†å²æ•°æ®å¤©æ•°å°‘äº200å¤©ï¼Œä¸çº³å…¥ç»Ÿè®¡èŒƒå›´
        if len(historical_data) < 200:
            return None  # ç›´æ¥è¿”å›Noneï¼Œä¸çº³å…¥ç»Ÿè®¡
        
        # ç¡®ä¿è‡³å°‘æœ‰è¶³å¤Ÿçš„äº¤æ˜“æ—¥æ•°æ®ï¼ˆ3å¹´çº¦750ä¸ªäº¤æ˜“æ—¥ï¼Œè‡³å°‘éœ€è¦500ä¸ªäº¤æ˜“æ—¥ï¼‰
        min_required_trading_days = lookback_years * 250  # æ¯å¹´çº¦250ä¸ªäº¤æ˜“æ—¥
        if len(historical_data) < min_required_trading_days * 0.6:  # è‡³å°‘éœ€è¦60%çš„äº¤æ˜“æ—¥
            # æ•°æ®ä¸è¶³ï¼Œè¿”å›å¤±è´¥
            return {
                'è‚¡ç¥¨ä»£ç ': code,
                'æœ€æ–°æ—¥æœŸ': latest_date.strftime('%Y-%m-%d') if hasattr(latest_date, 'strftime') else str(latest_date),
                'æœ€æ–°æ”¶ç›˜ä»·': latest_close,
                '3å¹´æœ€ä½ä»·': None,
                '3å¹´æœ€é«˜ä»·': None,
                'å½“å‰ä»·ç›¸å¯¹æœ€ä½ä»·æ¶¨å¹…': None,
                'å½“å‰ä»·ç›¸å¯¹æœ€é«˜ä»·è·Œå¹…': None,
                'æ˜¯å¦æ¥è¿‘æœ€ä½ä»·': False,
                'å¤±è´¥åŸå› ': f'å†å²æ•°æ®ä¸è¶³ï¼ˆå®é™…{len(historical_data)}ä¸ªäº¤æ˜“æ—¥ï¼Œéœ€è¦è‡³å°‘{int(min_required_trading_days*0.6)}ä¸ªäº¤æ˜“æ—¥ï¼‰'
            }
        
        if historical_data.empty or len(historical_data) < 10:
            return {
                'è‚¡ç¥¨ä»£ç ': code,
                'æœ€æ–°æ—¥æœŸ': latest_date.strftime('%Y-%m-%d') if hasattr(latest_date, 'strftime') else str(latest_date),
                'æœ€æ–°æ”¶ç›˜ä»·': latest_close,
                '3å¹´æœ€ä½ä»·': None,
                '3å¹´æœ€é«˜ä»·': None,
                'å½“å‰ä»·ç›¸å¯¹æœ€ä½ä»·æ¶¨å¹…': None,
                'å½“å‰ä»·ç›¸å¯¹æœ€é«˜ä»·è·Œå¹…': None,
                'æ˜¯å¦æ¥è¿‘æœ€ä½ä»·': False,
                'å¤±è´¥åŸå› ': 'å†å²æ•°æ®ä¸è¶³ï¼ˆå°‘äº10ä¸ªäº¤æ˜“æ—¥ï¼‰'
            }
        
        # è®¡ç®—3å¹´å†…çš„æœ€ä½ä»·å’Œæœ€é«˜ä»·
        min_price = historical_data['Close'].min()
        max_price = historical_data['Close'].max()
        
        if pd.isna(min_price) or min_price <= 0:
            return {
                'è‚¡ç¥¨ä»£ç ': code,
                'æœ€æ–°æ—¥æœŸ': latest_date.strftime('%Y-%m-%d') if hasattr(latest_date, 'strftime') else str(latest_date),
                'æœ€æ–°æ”¶ç›˜ä»·': latest_close,
                '3å¹´æœ€ä½ä»·': None,
                '3å¹´æœ€é«˜ä»·': None,
                'å½“å‰ä»·ç›¸å¯¹æœ€ä½ä»·æ¶¨å¹…': None,
                'å½“å‰ä»·ç›¸å¯¹æœ€é«˜ä»·è·Œå¹…': None,
                'æ˜¯å¦æ¥è¿‘æœ€ä½ä»·': False,
                'å¤±è´¥åŸå› ': 'æ— æ³•è®¡ç®—æœ€ä½ä»·'
            }
        
        # è®¡ç®—å½“å‰ä»·æ ¼ç›¸å¯¹æœ€ä½ä»·çš„æ¶¨å¹…ï¼ˆç™¾åˆ†æ¯”ï¼‰
        price_increase_from_low = (latest_close - min_price) / min_price
        
        # è®¡ç®—å½“å‰ä»·æ ¼ç›¸å¯¹æœ€é«˜ä»·çš„è·Œå¹…ï¼ˆç™¾åˆ†æ¯”ï¼‰
        price_decrease_from_high = (max_price - latest_close) / max_price if max_price > 0 else None
        
        # åˆ¤æ–­æ˜¯å¦åœ¨æœ€ä½ä»·çš„Â±10%èŒƒå›´å†…
        # å®šä¹‰ï¼šå½“å‰ä»·æ ¼æ¥è¿‘æœ€ä½ä»·ï¼Œå³å½“å‰ä»·æ ¼åœ¨ [æœ€ä½ä»·*(1-tolerance), æœ€ä½ä»·*(1+tolerance)] èŒƒå›´å†…
        # æ³¨æ„ï¼šé€šå¸¸"æ¥è¿‘æœ€ä½ä»·"æ„å‘³ç€å½“å‰ä»·åº”è¯¥ >= æœ€ä½ä»·ï¼Œä¸åº”è¯¥ä½äºæœ€ä½ä»·
        # ä½†å¦‚æœå…è®¸Â±10%èŒƒå›´ï¼Œåˆ™å¯èƒ½åŒ…æ‹¬ä½äºæœ€ä½ä»·çš„æƒ…å†µï¼ˆæ¯”å¦‚æœ€ä½ä»·10å…ƒï¼Œå½“å‰ä»·9å…ƒï¼Œåœ¨Â±10%èŒƒå›´å†…ï¼‰
        # è¿™é‡Œé‡‡ç”¨ä¸¥æ ¼å®šä¹‰ï¼šå½“å‰ä»·å¿…é¡» >= æœ€ä½ä»·ï¼Œä¸”åœ¨æœ€ä½ä»·çš„Â±10%èŒƒå›´å†…
        price_lower_bound = min_price * (1 - price_tolerance)
        price_upper_bound = min_price * (1 + price_tolerance)
        is_near_lowest = (latest_close >= price_lower_bound) and \
                        (latest_close <= price_upper_bound) and \
                        (latest_close >= min_price)  # ä¸å…è®¸ä½äºæœ€ä½ä»·ï¼ˆæ¥è¿‘æœ€ä½ä»·åº”è¯¥æ˜¯é«˜äºæˆ–ç­‰äºæœ€ä½ä»·ï¼‰
        
        # æ‰¾åˆ°æœ€ä½ä»·å‡ºç°çš„æ—¥æœŸ
        min_price_date = historical_data[historical_data['Close'] == min_price].index[0]
        
        # æ‰¾åˆ°æœ€é«˜ä»·å‡ºç°çš„æ—¥æœŸ
        max_price_date = historical_data[historical_data['Close'] == max_price].index[0]
        
        result = {
            'è‚¡ç¥¨ä»£ç ': code,
            'æœ€æ–°æ—¥æœŸ': latest_date.strftime('%Y-%m-%d') if hasattr(latest_date, 'strftime') else str(latest_date),
            'æœ€æ–°æ”¶ç›˜ä»·': round(latest_close, 2),
            '3å¹´æœ€ä½ä»·': round(min_price, 2),
            '3å¹´æœ€ä½ä»·æ—¥æœŸ': min_price_date.strftime('%Y-%m-%d') if hasattr(min_price_date, 'strftime') else str(min_price_date),
            '3å¹´æœ€é«˜ä»·': round(max_price, 2),
            '3å¹´æœ€é«˜ä»·æ—¥æœŸ': max_price_date.strftime('%Y-%m-%d') if hasattr(max_price_date, 'strftime') else str(max_price_date),
            'å½“å‰ä»·ç›¸å¯¹æœ€ä½ä»·æ¶¨å¹…': round(price_increase_from_low * 100, 2),
            'å½“å‰ä»·ç›¸å¯¹æœ€é«˜ä»·è·Œå¹…': round(price_decrease_from_high * 100, 2) if price_decrease_from_high is not None else None,
            'ä»·æ ¼åŒºé—´ä½ç½®': round((latest_close - min_price) / (max_price - min_price) * 100, 2) if max_price > min_price else None,
            'æ˜¯å¦æ¥è¿‘æœ€ä½ä»·': is_near_lowest,
            'å†å²æ•°æ®å¤©æ•°': len(historical_data),
            'å¤±è´¥åŸå› ': None if is_near_lowest else f'å½“å‰ä»·ç›¸å¯¹æœ€ä½ä»·æ¶¨å¹…{price_increase_from_low*100:.2f}%ï¼Œè¶…è¿‡Â±{price_tolerance*100:.0f}%èŒƒå›´'
        }
        
        return result
        
    except Exception as e:
        logger.error(f"åˆ†æè‚¡ç¥¨ {code} æ—¶å‡ºé”™: {e}", exc_info=True)
        return {
            'è‚¡ç¥¨ä»£ç ': code,
            'æœ€æ–°æ—¥æœŸ': None,
            'æœ€æ–°æ”¶ç›˜ä»·': None,
            '3å¹´æœ€ä½ä»·': None,
            '3å¹´æœ€é«˜ä»·': None,
            'å½“å‰ä»·ç›¸å¯¹æœ€ä½ä»·æ¶¨å¹…': None,
            'å½“å‰ä»·ç›¸å¯¹æœ€é«˜ä»·è·Œå¹…': None,
            'æ˜¯å¦æ¥è¿‘æœ€ä½ä»·': False,
            'å¤±è´¥åŸå› ': f'åˆ†æå‡ºé”™: {str(e)}'
        }


def run_near_lowest_price_screening(
    limit: Optional[int] = None,
    max_days_ago: Optional[int] = None,
    lookback_years: int = 3,
    price_tolerance: float = 0.10,
    output_dir: str = "outputs",
    cutoff_date: Optional[date] = None
) -> Optional[str]:
    """
    è¿è¡Œæ¥è¿‘å†å²æœ€ä½ä»·ç­›é€‰
    
    å‚æ•°ï¼š
    - limit: è‚¡ç¥¨æ•°é‡é™åˆ¶ï¼ŒNone ä½¿ç”¨é…ç½®ä¸­çš„ STOCK_LIST_LIMIT
    - max_days_ago: æœ€å¤§å…è®¸è¡Œæƒ…æ»åå¤©æ•°ï¼ŒNone ä½¿ç”¨é…ç½®ä¸­çš„ MAX_TRADING_DAYS_AGO
    - lookback_years: å›çœ‹å¹´æ•°ï¼ˆé»˜è®¤3å¹´ï¼‰
    - price_tolerance: ä»·æ ¼å®¹å·®ï¼ˆé»˜è®¤0.10ï¼Œå³Â±10%ï¼‰
    - output_dir: è¾“å‡ºç›®å½•
    
    è¿”å›ï¼š
    - è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœå¤±è´¥åˆ™è¿”å›None
    """
    from logger_config import init_logger
    
    session_logger = init_logger("logs")
    
    if limit is None:
        limit = STOCK_LIST_LIMIT
    if max_days_ago is None:
        max_days_ago = MAX_TRADING_DAYS_AGO
    
    print("=" * 60)
    print("åŠŸèƒ½14ï¼šæ¥è¿‘å†å²æœ€ä½ä»·ç­›é€‰")
    print("=" * 60)
    print(f"è‚¡ç¥¨æ•°é‡ä¸Šé™: {limit}")
    print(f"æœ€å¤§å…è®¸è¡Œæƒ…æ»åå¤©æ•°: {max_days_ago}")
    print(f"å›çœ‹å¹´æ•°: {lookback_years}å¹´")
    print(f"ä»·æ ¼å®¹å·®: Â±{price_tolerance*100:.0f}%")
    print("=" * 60)
    
    start_time = time.time()
    
    try:
        fetcher = JuyuanDataFetcher(use_connection_pool=True)
        
        # ç¡®å®šæˆªæ­¢æ—¥æœŸ
        if cutoff_date is not None:
            end_date = cutoff_date
            print(f"ğŸ“… å›æµ‹æ¨¡å¼ - æŒ‡å®šæˆªæ­¢æ—¥æœŸ: {end_date}")
        else:
            end_date = fetcher.get_latest_trading_date()
            print(f"ğŸ“… æ•°æ®åº“æœ€æ–°äº¤æ˜“æ—¥: {end_date}")
        print("=" * 60)
        
        # 1. è·å–æ´»è·ƒè‚¡ç¥¨åˆ—è¡¨
        print("ğŸ“Š è·å–æ´»è·ƒè‚¡ç¥¨åˆ—è¡¨ï¼ˆæ­£åœ¨æŸ¥è¯¢æ•°æ®åº“ï¼Œè¯·ç¨å€™...ï¼‰...")
        list_start_time = time.time()
        stock_info_list = fetcher.get_stock_list(limit=limit, max_days_ago=max_days_ago, cutoff_date=cutoff_date)
        list_elapsed = time.time() - list_start_time
        print(f"  âœ… è‚¡ç¥¨åˆ—è¡¨è·å–å®Œæˆï¼ˆè€—æ—¶: {list_elapsed:.1f}ç§’ï¼‰")
        
        # å…¼å®¹ä¸¤ç§è¿”å›æ ¼å¼ï¼šå­—ç¬¦ä¸²åˆ—è¡¨ æˆ– å­—å…¸åˆ—è¡¨
        if stock_info_list and isinstance(stock_info_list[0], dict):
            codes = [info["code"] for info in stock_info_list]
        else:
            codes = stock_info_list
        
        if not codes:
            print("âŒ æœªè·å–åˆ°ä»»ä½•æ´»è·ƒè‚¡ç¥¨")
            return None
        
        print(f"âœ… å®é™…è‚¡ç¥¨æ•°é‡: {len(codes)}")
        
        # 2. æ‰¹é‡è·å–è‚¡ç¥¨æ•°æ®ï¼ˆå®Œå…¨å¤ç”¨åŠŸèƒ½13çš„é€»è¾‘ï¼‰
        print("ğŸ“ˆ æ‰¹é‡è·å–è¡Œæƒ…æ•°æ®ï¼ˆå¢é‡ç¼“å­˜ + æ‰¹é‡SQL + å¤šçº¿ç¨‹å¹¶å‘ï¼‰...")
        
        # è®¡ç®—æ—¥æœŸèŒƒå›´ï¼ˆéœ€è¦3å¹´+ç¼“å†²æœŸçš„æ•°æ®ï¼‰
        days_needed = lookback_years * 365 + 30  # 3å¹´ + 30å¤©ç¼“å†²
        # end_date å·²åœ¨ä¸Šé¢ç¡®å®šï¼ˆcutoff_date æˆ–æœ€æ–°äº¤æ˜“æ—¥ï¼‰
        start_date = end_date - timedelta(days=days_needed + 30)  # å¤šè·å–30å¤©ä½œä¸ºç¼“å†²
        
        # å°è¯•ä»ç¼“å­˜åŠ è½½ï¼ˆæ‰¹é‡ï¼‰
        print("  ğŸ” æ£€æŸ¥å¢é‡ç¼“å­˜...")
        print(f"  ğŸ“… è¯·æ±‚æ—¥æœŸèŒƒå›´: {start_date} è‡³ {end_date}ï¼ˆæ•°æ®åº“æœ€æ–°äº¤æ˜“æ—¥ï¼Œéœ€è¦{days_needed}å¤©æ•°æ®ï¼‰")
        cached_stock_data, missing_stock_codes = futures_incremental_cache_manager.load_stocks_data(
            codes, start_date, end_date
        )
        
        # æ£€æŸ¥ç¼“å­˜æ•°æ®çš„å®Œæ•´æ€§ï¼ˆä½¿ç”¨ç»Ÿä¸€çš„ç¼“å­˜éªŒè¯å‡½æ•°ï¼‰
        insufficient_data_codes = []  # æ•°æ®ä¸è¶³çš„è‚¡ç¥¨ï¼ˆéœ€è¦è¡¥å…¨ï¼‰
        partial_data_codes = {}  # éƒ¨åˆ†å‘½ä¸­çš„è‚¡ç¥¨ï¼ˆç¼“å­˜æœ‰æ•°æ®ä½†éœ€è¦è¡¥å…¨ç¼ºå¤±æ—¥æœŸèŒƒå›´ï¼‰
        valid_cached_data = {}  # å®Œå…¨æœ‰æ•ˆçš„ç¼“å­˜æ•°æ®
        
        for code, data in cached_stock_data.items():
            # ä½¿ç”¨ç»Ÿä¸€çš„ç¼“å­˜éªŒè¯å‡½æ•°
            validation_result = validate_cache_data(
                data=data,
                start_date=start_date,
                end_date=end_date,
                days_needed=days_needed
            )
            
            if validation_result.is_valid:
                # ç¼“å­˜æ•°æ®å®Œå…¨æœ‰æ•ˆ
                valid_cached_data[code] = data
            elif validation_result.is_partial:
                # ç¼“å­˜éƒ¨åˆ†æœ‰æ•ˆï¼Œéœ€è¦å¢é‡è¡¥å…¨
                partial_data_codes[code] = {
                    'cached_data': data,
                    'missing_start': validation_result.missing_start,
                    'missing_end': validation_result.missing_end,
                    'cache_start': validation_result.cache_start,
                    'cache_end': validation_result.cache_end
                }
            else:
                # ç¼“å­˜æ•°æ®æ— æ•ˆï¼Œéœ€è¦é‡æ–°è·å–å…¨éƒ¨æ•°æ®
                insufficient_data_codes.append(code)
        
        # åˆå¹¶éœ€è¦é‡æ–°è·å–çš„è‚¡ç¥¨ä»£ç ï¼ˆå®Œå…¨ç¼ºå¤±çš„ + æ•°æ®å¤ªå°‘çš„ï¼‰
        all_missing_codes = list(set(missing_stock_codes + insufficient_data_codes))
        cache_hit_count = len(valid_cached_data)
        partial_hit_count = len(partial_data_codes)
        cache_miss_count = len(all_missing_codes)
        
        print(f"  âœ… ç¼“å­˜å®Œå…¨æœ‰æ•ˆ: {cache_hit_count} åªè‚¡ç¥¨")
        if partial_hit_count > 0:
            print(f"  ğŸ”„ ç¼“å­˜éƒ¨åˆ†å‘½ä¸­: {partial_hit_count} åªè‚¡ç¥¨ï¼ˆå°†å¢é‡è¡¥å…¨ç¼ºå¤±æ—¥æœŸèŒƒå›´ï¼‰")
        print(f"  âš ï¸  éœ€è¦ä»æ•°æ®åº“è·å–: {cache_miss_count} åªè‚¡ç¥¨")
        
        # å¯¹ç¼ºå¤±æˆ–æ•°æ®ä¸è¶³çš„è‚¡ç¥¨ä»æ•°æ®åº“è·å–ï¼ˆæ‰¹é‡ï¼‰
        # ä¼˜åŒ–ï¼šåŒºåˆ†å®Œå…¨ç¼ºå¤±å’Œéƒ¨åˆ†å‘½ä¸­çš„è‚¡ç¥¨ï¼Œéƒ¨åˆ†å‘½ä¸­çš„åªè·å–ç¼ºå¤±æ—¥æœŸèŒƒå›´
        fetch_start_time = time.time()
        fetched_stock_data = {}
        
        # 1. å¤„ç†å®Œå…¨ç¼ºå¤±çš„è‚¡ç¥¨ï¼šè·å–å…¨éƒ¨æ•°æ®
        if all_missing_codes:
            print(f"  ğŸ“¥ ä»æ•°æ®åº“è·å–å®Œå…¨ç¼ºå¤±çš„ {len(all_missing_codes)} åªè‚¡ç¥¨æ•°æ®ï¼ˆå…¨éƒ¨3å¹´æ•°æ®ï¼‰...")
            optimized_time_config = SimpleNamespace(
                crash_start_date=start_date,
                crash_end_date=end_date
            )
            fully_missing_data = fetcher.batch_get_stock_data_with_adjustment(
                all_missing_codes,
                days=days_needed,
                time_config=optimized_time_config
            )
            if fully_missing_data:
                fetched_stock_data.update(fully_missing_data)
                print(f"  âœ… å®Œå…¨ç¼ºå¤±è‚¡ç¥¨æ•°æ®è·å–å®Œæˆ: {len(fully_missing_data)} åª")
        
        # 2. å¤„ç†éƒ¨åˆ†å‘½ä¸­çš„è‚¡ç¥¨ï¼šåªè·å–ç¼ºå¤±çš„æ—¥æœŸèŒƒå›´ï¼ˆå¢é‡è¡¥å…¨ï¼‰
        if partial_data_codes:
            print(f"  ğŸ”„ å¢é‡è¡¥å…¨éƒ¨åˆ†å‘½ä¸­çš„ {len(partial_data_codes)} åªè‚¡ç¥¨æ•°æ®ï¼ˆåªè·å–ç¼ºå¤±æ—¥æœŸèŒƒå›´ï¼‰...")
            
            # æŒ‰ç¼ºå¤±æ—¥æœŸèŒƒå›´åˆ†ç»„ï¼Œç›¸åŒèŒƒå›´çš„è‚¡ç¥¨ä¸€èµ·è·å–
            date_range_groups = {}  # {(missing_start, missing_end): [codes]}
            
            for code, partial_info in partial_data_codes.items():
                missing_start = partial_info.get('missing_start')
                missing_end = partial_info.get('missing_end')
                cache_start = partial_info.get('cache_start')
                cache_end = partial_info.get('cache_end')
                
                # ç¡®å®šéœ€è¦è·å–çš„æ—¥æœŸèŒƒå›´
                if missing_start and missing_end:
                    # æœ‰æ˜ç¡®çš„ç¼ºå¤±èŒƒå›´
                    fetch_start = missing_start
                    fetch_end = missing_end
                elif missing_end:
                    # åªç¼ºç»“æŸæ—¥æœŸï¼ˆç¼“å­˜æ•°æ®ä¸å¤Ÿæ–°ï¼‰
                    fetch_start = cache_end + timedelta(days=1) if cache_end else start_date
                    fetch_end = missing_end
                elif missing_start:
                    # åªç¼ºå¼€å§‹æ—¥æœŸï¼ˆç¼“å­˜æ•°æ®ä¸å¤Ÿæ—§ï¼‰
                    fetch_start = missing_start
                    fetch_end = cache_start - timedelta(days=1) if cache_start else end_date
                else:
                    # æ²¡æœ‰æ˜ç¡®ç¼ºå¤±èŒƒå›´ï¼Œä½¿ç”¨ç¼“å­˜æ•°æ®çš„è¾¹ç•Œ
                    fetch_start = cache_end + timedelta(days=1) if cache_end else start_date
                    fetch_end = end_date
                
                # ç¡®ä¿æ—¥æœŸèŒƒå›´åˆç†
                fetch_start = max(fetch_start, start_date)
                fetch_end = min(fetch_end, end_date)
                
                if fetch_start <= fetch_end:
                    range_key = (fetch_start, fetch_end)
                    if range_key not in date_range_groups:
                        date_range_groups[range_key] = []
                    date_range_groups[range_key].append(code)
            
            # å¯¹æ¯ä¸ªæ—¥æœŸèŒƒå›´ç»„ï¼Œæ‰¹é‡è·å–æ•°æ®
            from tqdm import tqdm
            
            total_partial_groups = len(date_range_groups)
            print(f"    åˆ†ä¸º {total_partial_groups} ä¸ªæ—¥æœŸèŒƒå›´ç»„è¿›è¡Œå¢é‡è¡¥å…¨...")
            
            with tqdm(total=total_partial_groups, desc="  å¢é‡è¡¥å…¨", unit="ç»„", ncols=100) as pbar:
                for (fetch_start, fetch_end), codes_in_group in date_range_groups.items():
                    # è®¡ç®—éœ€è¦è·å–çš„å¤©æ•°
                    days_to_fetch = (fetch_end - fetch_start).days + 30  # å¤šè·å–30å¤©ç¼“å†²
                    
                    # åˆ›å»ºæ—¶é—´é…ç½®ï¼ˆä¾› data_fetcher çš„ crash_start_date / crash_end_date ä½¿ç”¨ï¼‰
                    partial_time_config = SimpleNamespace(
                        crash_start_date=fetch_start,
                        crash_end_date=fetch_end
                    )
                    
                    # æ‰¹é‡è·å–è¿™ä¸ªæ—¥æœŸèŒƒå›´çš„æ•°æ®
                    partial_fetched = fetcher.batch_get_stock_data_with_adjustment(
                        codes_in_group,
                        days=days_to_fetch,
                        time_config=partial_time_config
                    )
                    
                    if partial_fetched:
                        fetched_stock_data.update(partial_fetched)
                    
                    pbar.update(1)
                    pbar.set_postfix({
                        'å·²è¡¥å…¨': f'{len(fetched_stock_data)}åª',
                        'å½“å‰ç»„': f'{len(codes_in_group)}åª',
                        'æ—¥æœŸèŒƒå›´': f'{fetch_start}~{fetch_end}'
                    })
            
            print(f"  âœ… éƒ¨åˆ†å‘½ä¸­è‚¡ç¥¨å¢é‡è¡¥å…¨å®Œæˆ: {len([c for c in fetched_stock_data.keys() if c in partial_data_codes])} åª")
        
        fetch_elapsed = time.time() - fetch_start_time
        print(f"  âœ… æ•°æ®è·å–å®Œæˆï¼ˆæ€»è€—æ—¶: {fetch_elapsed:.1f}ç§’ï¼‰")
        
        # å¤„ç†éƒ¨åˆ†å‘½ä¸­çš„è‚¡ç¥¨ï¼šåˆå¹¶ç¼“å­˜æ•°æ®å’Œæ–°è·å–çš„å¢é‡æ•°æ®
        # æ³¨æ„ï¼šå¿…é¡»åœ¨ä¿å­˜ç¼“å­˜ä¹‹å‰åˆå¹¶ï¼Œè¿™æ ·ä¿å­˜çš„å°±æ˜¯å®Œæ•´æ•°æ®
        if partial_data_codes:
            print(f"  ğŸ”„ åˆå¹¶éƒ¨åˆ†å‘½ä¸­çš„ç¼“å­˜æ•°æ®...")
            merged_count = 0
            for code, partial_info in partial_data_codes.items():
                cached_data = partial_info['cached_data']
                
                # å¦‚æœæ–°è·å–çš„æ•°æ®ä¸­åŒ…å«è¿™åªè‚¡ç¥¨ï¼Œåˆå¹¶æ•°æ®
                if code in fetched_stock_data:
                    new_data = fetched_stock_data[code]
                    # åˆå¹¶ç¼“å­˜æ•°æ®å’Œæ–°æ•°æ®ï¼ˆå»é‡ï¼Œä¿ç•™æœ€æ–°æ•°æ®ï¼‰
                    if not new_data.empty:
                        # ä½¿ç”¨concatåˆå¹¶ï¼Œç„¶åå»é‡ï¼ˆä¿ç•™æ–°æ•°æ®ï¼‰
                        combined_data = pd.concat([cached_data, new_data])
                        # æŒ‰ç´¢å¼•å»é‡ï¼Œä¿ç•™æœ€åä¸€ä¸ªï¼ˆæ–°æ•°æ®ä¼˜å…ˆï¼‰
                        combined_data = combined_data[~combined_data.index.duplicated(keep='last')]
                        combined_data = combined_data.sort_index()
                        # ç­›é€‰åˆ°ç›®æ ‡æ—¥æœŸèŒƒå›´
                        mask = (combined_data.index.date >= start_date) & (combined_data.index.date <= end_date)
                        combined_data = combined_data[mask]
                        if not combined_data.empty:
                            # å°†åˆå¹¶åçš„å®Œæ•´æ•°æ®æ›´æ–°åˆ° fetched_stock_dataï¼Œè¿™æ ·ä¿å­˜ç¼“å­˜æ—¶å°±æ˜¯å®Œæ•´æ•°æ®
                            fetched_stock_data[code] = combined_data
                            valid_cached_data[code] = combined_data
                            merged_count += 1
                            continue
                
                # å¦‚æœæ²¡æœ‰æ–°æ•°æ®ï¼Œä½†ç¼“å­˜æ•°æ®è¶³å¤Ÿï¼ˆéƒ¨åˆ†æœ‰æ•ˆçš„ç¼“å­˜è‡³å°‘æœ‰50ä¸ªäº¤æ˜“æ—¥ï¼‰ï¼Œç›´æ¥ä½¿ç”¨ç¼“å­˜
                # è¿™éƒ¨åˆ†å‘½ä¸­çš„è‚¡ç¥¨å·²ç»è¢«validate_cache_dataåˆ¤å®šä¸ºéƒ¨åˆ†æœ‰æ•ˆï¼ˆè‡³å°‘50ä¸ªäº¤æ˜“æ—¥æˆ–30%è¦†ç›–ç‡ï¼‰
                if len(cached_data) >= 50:
                    valid_cached_data[code] = cached_data
            
            if merged_count > 0:
                print(f"  âœ… æˆåŠŸåˆå¹¶ {merged_count} åªéƒ¨åˆ†å‘½ä¸­è‚¡ç¥¨çš„æ•°æ®")
        
        # ä¿å­˜æ–°è·å–çš„æ•°æ®åˆ°ç¼“å­˜ï¼ˆæ‰¹é‡ï¼‰
        # æ³¨æ„ï¼šæ­¤æ—¶ fetched_stock_data ä¸­å·²ç»åŒ…å«äº†åˆå¹¶åçš„å®Œæ•´æ•°æ®ï¼ˆå¯¹äºéƒ¨åˆ†å‘½ä¸­çš„è‚¡ç¥¨ï¼‰
        if fetched_stock_data:
            print(f"  ğŸ’¾ ä¿å­˜ {len(fetched_stock_data)} åªè‚¡ç¥¨çš„æ•°æ®åˆ°å¢é‡ç¼“å­˜...")
            save_result = futures_incremental_cache_manager.save_stocks_data(
                list(fetched_stock_data.keys()),
                fetched_stock_data,
                start_date,
                end_date
            )
            if save_result:
                print(f"  âœ… ç¼“å­˜ä¿å­˜å®Œæˆï¼ˆæˆåŠŸä¿å­˜ {len(fetched_stock_data)} åªè‚¡ç¥¨ï¼‰")
            else:
                print(f"  âš ï¸  ç¼“å­˜ä¿å­˜å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—")
            
            # åˆå¹¶æœ‰æ•ˆç¼“å­˜æ•°æ®å’Œæ–°å¢æ•°æ®
            valid_cached_data.update(fetched_stock_data)
        
        all_stock_data = valid_cached_data
        
        if not all_stock_data:
            print("âŒ æœªè·å–åˆ°ä»»ä½•è‚¡ç¥¨è¡Œæƒ…æ•°æ®")
            return None
        
        # ä¿®å¤ï¼šè®¡ç®—ç¼“å­˜å‘½ä¸­ç‡ï¼ˆåŒ…æ‹¬å®Œå…¨å‘½ä¸­å’Œéƒ¨åˆ†å‘½ä¸­ï¼‰
        total_stocks = cache_hit_count + partial_hit_count + cache_miss_count
        if total_stocks > 0:
            cache_hit_rate = (cache_hit_count + partial_hit_count) / total_stocks * 100
        else:
            cache_hit_rate = 0.0
        
        print(f"âœ… æˆåŠŸè·å– {len(all_stock_data)} åªè‚¡ç¥¨çš„è¡Œæƒ…æ•°æ®ï¼ˆç¼“å­˜å‘½ä¸­ç‡: {cache_hit_rate:.1f}%ï¼Œå®Œå…¨å‘½ä¸­: {cache_hit_count}åªï¼Œéƒ¨åˆ†å‘½ä¸­: {partial_hit_count}åªï¼‰")
        
        # 3. å¹¶è¡Œåˆ†ææ¯åªè‚¡ç¥¨ï¼ˆå®Œå…¨å¤ç”¨åŠŸèƒ½13çš„å¤šçº¿ç¨‹é€»è¾‘ï¼‰
        print("ğŸš€ å¹¶è¡Œåˆ†ææ¥è¿‘å†å²æœ€ä½ä»·...")
        
        def _task_analyze(code: str,
                         stock_data_dict: Dict[str, pd.DataFrame],
                         lookback_years: int,
                         price_tolerance: float,
                         cutoff_date: Optional[date] = None) -> Optional[Dict]:
            """ä¾›é«˜æ€§èƒ½çº¿ç¨‹æ± è°ƒç”¨çš„ä»»åŠ¡å‡½æ•°"""
            df = stock_data_dict.get(code)
            if df is None or df.empty:
                return None
            
            # è°ƒç”¨åˆ†æå‡½æ•°
            result = analyze_near_lowest_price(
                code=code,
                stock_data=df,
                lookback_years=lookback_years,
                price_tolerance=price_tolerance,
                cutoff_date=cutoff_date
            )
            
            return result
        
        # ä½¿ç”¨é«˜æ€§èƒ½çº¿ç¨‹æ± ï¼ˆå®Œå…¨å¤ç”¨åŠŸèƒ½13çš„é€»è¾‘ï¼‰
        thread_pool = HighPerformanceThreadPool(progress_desc="æ¥è¿‘å†å²æœ€ä½ä»·åˆ†æ")
        
        results = thread_pool.execute_batch(
            list(all_stock_data.keys()),
            _task_analyze,
            all_stock_data,
            lookback_years,
            price_tolerance,
            end_date  # ä¼ é€’end_dateä½œä¸ºcutoff_date
        )
        
        # æ±‡æ€»ç»“æœ
        valid_results = [r for r in results if r is not None]
        near_lowest_results = [r for r in valid_results if r.get('æ˜¯å¦æ¥è¿‘æœ€ä½ä»·', False)]
        not_near_lowest_results = [r for r in valid_results if not r.get('æ˜¯å¦æ¥è¿‘æœ€ä½ä»·', False)]
        
        print(f"\nâœ… å‘ç° {len(near_lowest_results)} åªè‚¡ç¥¨æ¥è¿‘å†å²æœ€ä½ä»·ï¼ˆÂ±{price_tolerance*100:.0f}%ï¼‰")
        if len(not_near_lowest_results) > 0:
            print(f"ğŸ“‹ å¦æœ‰ {len(not_near_lowest_results)} åªè‚¡ç¥¨ä¸æ»¡è¶³æ¡ä»¶ï¼ˆå°†ä¸€å¹¶å¯¼å‡ºè¯¦ç»†ä¿¡æ¯ï¼‰")
        
        # 4. è·å–å…¨éƒ¨æœ‰æ•ˆè‚¡ç¥¨çš„åŸºæœ¬é¢æ•°æ®ï¼ˆæ»¡è¶³ä¸ä¸æ»¡è¶³å‡å¯¼å‡ºåˆ¤æ–­æŒ‡æ ‡æ˜ç»†ï¼Œå«PB/PEã€è‚¡æ¯ç‡ç­‰ï¼‰
        fundamental_data = {}
        all_codes = [r.get('è‚¡ç¥¨ä»£ç ') for r in valid_results if r.get('è‚¡ç¥¨ä»£ç ')]
        if all_codes:
            print("\nğŸ“Š è·å–å…¨éƒ¨è‚¡ç¥¨åŸºæœ¬é¢æ•°æ®ï¼ˆPB/PEã€è‚¡æ¯ç‡ï¼Œä¾›æ˜ç»†å¯¼å‡ºï¼‰...")
            fundamental_start_time = time.time()
            
            try:
                qualified_codes = all_codes
                
                if qualified_codes:
                    # ç»Ÿä¸€æ ¼å¼åŒ–è‚¡ç¥¨ä»£ç ä¸º6ä½æ•°å­—ï¼ˆç¡®ä¿ä¸ç¼“å­˜æ ¼å¼ä¸€è‡´ï¼‰
                    normalized_qualified_codes = []
                    for code in qualified_codes:
                        code_str = str(code).strip().zfill(6)
                        # å¦‚æœä»£ç åŒ…å«éæ•°å­—å­—ç¬¦ï¼ˆå¦‚.SZï¼‰ï¼Œæå–æ•°å­—éƒ¨åˆ†
                        if not code_str.isdigit():
                            digits = ''.join([c for c in code_str if c.isdigit()])[:6]
                            code_str = digits.zfill(6) if digits else code_str
                        normalized_qualified_codes.append(code_str)
                    
                    # å…ˆå°è¯•ä»ç¼“å­˜åŠ è½½
                    print(f"  ğŸ” æ£€æŸ¥ {len(normalized_qualified_codes)} åªç¬¦åˆæ¡ä»¶çš„è‚¡ç¥¨åŸºæœ¬é¢æ•°æ®ç¼“å­˜...")
                    cached_fundamental_data, missing_codes = futures_incremental_cache_manager.load_fundamental_data(normalized_qualified_codes)

                    # äºŒæ¬¡æ ¡éªŒï¼šè€ç‰ˆæœ¬ç¼“å­˜é‡Œæ²¡æœ‰ PE/PB åˆ†ä½æ•°å­—æ®µï¼Œæœ¬æ¬¡éœ€è¦å¼ºåˆ¶é‡ç®—
                    # é€»è¾‘ï¼šå‡¡æ˜¯ç¼“å­˜ä¸­ pb_percentile æˆ– pe_percentile ä¸ºç©ºçš„è‚¡ç¥¨ï¼Œç»Ÿä¸€è§†ä¸ºâ€œéœ€è¦æ›´æ–°â€çš„ç¼ºå¤±ä»£ç 
                    incomplete_codes = []
                    if cached_fundamental_data:
                        for c, data in cached_fundamental_data.items():
                            if not isinstance(data, dict):
                                incomplete_codes.append(c)
                                continue
                            pb_p = data.get('pb_percentile')
                            pe_p = data.get('pe_percentile')
                            # è€ç¼“å­˜ï¼ˆåªæœ‰ PB / PE / ROE / åˆ†çº¢ï¼Œä½†æ²¡æœ‰åˆ†ä½æ•°ï¼‰æˆ–åˆ†ä½æ•°ä¸º Noneï¼Œéƒ½è®¤ä¸ºéœ€è¦é‡ç®—
                            if pb_p is None and pe_p is None:
                                incomplete_codes.append(c)

                    # åˆå¹¶â€œç‰©ç†ç¼ºå¤±çš„ä»£ç â€å’Œâ€œç¼“å­˜ç»“æ„ä¸å®Œæ•´çš„ä»£ç â€
                    # æ³¨æ„ï¼šè¿™é‡Œéƒ½ç”¨æ ‡å‡†çš„ 6 ä½ä»£ç ï¼ˆä¸ normalized_qualified_codes ä¸€è‡´ï¼‰
                    if incomplete_codes:
                        logger.info(f"åŸºæœ¬é¢ç¼“å­˜ä¸­æœ‰ {len(incomplete_codes)} åªè‚¡ç¥¨ç¼ºå°‘åˆ†ä½æ•°ä¿¡æ¯ï¼Œæœ¬æ¬¡å°†å¼ºåˆ¶é‡æ–°è®¡ç®—è¿™äº›è‚¡ç¥¨çš„PE/PBåˆ†ä½æ•°")
                    missing_codes = list(sorted(set(list(missing_codes) + incomplete_codes)))

                    if cached_fundamental_data:
                        # å…ˆæŠŠç¼“å­˜é‡Œâ€œçœ‹èµ·æ¥å®Œæ•´â€çš„æ•°æ®æ”¾å…¥ç»“æœï¼Œå…¶ä½™ç¼ºå¤±/ä¸å®Œæ•´çš„å¾…ä¼šå„¿ç”¨æ–°ç»“æœè¦†ç›–
                        fundamental_data.update(cached_fundamental_data)
                        cache_hit_count = len(cached_fundamental_data) - len(incomplete_codes)
                        print(f"  âœ… ç¼“å­˜å‘½ä¸­(å®Œæ•´): {cache_hit_count} åªè‚¡ç¥¨")
                        if missing_codes:
                            preview_codes = missing_codes[:5]
                            print(f"  âš ï¸  éœ€è¦ä»æ•°æ®åº“è·å–/é‡ç®—: {len(missing_codes)} åªè‚¡ç¥¨ï¼ˆä¾‹: {preview_codes}{'...' if len(missing_codes) > 5 else ''}ï¼‰")
                    
                    # å¦‚æœæœ‰ç¼ºå¤±/éœ€è¦é‡ç®—çš„è‚¡ç¥¨ï¼Œä»æ•°æ®åº“è·å–
                    if missing_codes:
                        print(f"  ğŸ“¥ ä»æ•°æ®åº“è·å– / é‡æ–°è®¡ç®— {len(missing_codes)} åªè‚¡ç¥¨çš„åŸºæœ¬é¢æ•°æ®...")
                        batch_size = 500  # æ¯æ‰¹500åªè‚¡ç¥¨ï¼Œé¿å…SQLæŸ¥è¯¢è¿‡å¤§
                        
                        from tqdm import tqdm
                        total_batches = (len(missing_codes) + batch_size - 1) // batch_size
                        
                        fetched_data = {}
                        with tqdm(total=total_batches, desc="  åŸºæœ¬é¢æ•°æ®æŸ¥è¯¢", unit="æ‰¹", ncols=100) as pbar:
                            for i in range(0, len(missing_codes), batch_size):
                                batch_codes = missing_codes[i:i+batch_size]
                                batch_data = get_extended_fundamental_data(
                                    codes=batch_codes,
                                    fetcher=fetcher,
                                    stock_data_dict=all_stock_data,
                                    years_for_percentile=5,
                                    years_for_dividend=3
                                )
                                fetched_data.update(batch_data)
                                pbar.update(1)
                                pbar.set_postfix({
                                    'å·²è·å–': f'{len(fetched_data)}åª',
                                    'è¿›åº¦': f'{(i+batch_size)*100//len(missing_codes) if missing_codes else 0}%'
                                })
                        
                        # æ›´æ–°ç»“æœ
                        fundamental_data.update(fetched_data)
                        
                        # ä¿å­˜æ–°è·å–çš„æ•°æ®åˆ°ç¼“å­˜
                        if fetched_data:
                            print(f"  ğŸ’¾ ä¿å­˜ {len(fetched_data)} åªè‚¡ç¥¨çš„åŸºæœ¬é¢æ•°æ®åˆ°ç¼“å­˜...")
                            futures_incremental_cache_manager.save_fundamental_data(fetched_data)
                            print(f"  âœ… ç¼“å­˜ä¿å­˜å®Œæˆ")
                    
                    fundamental_elapsed = time.time() - fundamental_start_time
                    print(f"  âœ… åŸºæœ¬é¢æ•°æ®è·å–å®Œæˆï¼ˆè€—æ—¶: {fundamental_elapsed:.1f}ç§’ï¼Œå…± {len(fundamental_data)} åªè‚¡ç¥¨ï¼‰")
                
            except Exception as e:
                logger.error(f"è·å–åŸºæœ¬é¢æ•°æ®å¤±è´¥: {e}")
                print(f"  âš ï¸  åŸºæœ¬é¢æ•°æ®è·å–å¤±è´¥: {e}")
                fundamental_data = {}
        
        # 5. æ·»åŠ åŸºæœ¬é¢æ•°æ®åˆ°ç»“æœä¸­ï¼ˆæ»¡è¶³ä¸ä¸æ»¡è¶³æ¡ä»¶çš„è‚¡ç¥¨å‡å†™å…¥åˆ¤æ–­æŒ‡æ ‡æ˜ç»†ï¼‰
        def _apply_fundamental(result: dict, fund_dict: dict) -> None:
            code = result.get('è‚¡ç¥¨ä»£ç ')
            if not code:
                result['PB'] = result['PE_TTM'] = result['ROE_TTM'] = None
                result['PBåˆ†ä½æ•°(5å¹´)'] = result['PEåˆ†ä½æ•°(5å¹´)'] = result['è‚¡æ¯ç‡(%)'] = result['è¿‘3å¹´å¹³å‡è‚¡æ¯ç‡(%)'] = None
                result['è¿‘3å¹´åˆ†çº¢å¹´ä»½'] = 'æ— '
                return
            # å…¼å®¹ 6 ä½ä¸å¸¦åç¼€çš„ code æŸ¥æ‰¾
            fund_data = fund_dict.get(code) or fund_dict.get(str(code).split('.')[0].zfill(6))
            if fund_data:
                result['PB'] = round(fund_data.get('pb_mrq'), 2) if fund_data.get('pb_mrq') is not None else None
                result['PE_TTM'] = round(fund_data.get('pe_ttm'), 2) if fund_data.get('pe_ttm') is not None else None
                result['ROE_TTM'] = round(fund_data.get('roe_ttm'), 2) if fund_data.get('roe_ttm') is not None else None
                result['PBåˆ†ä½æ•°(5å¹´)'] = fund_data.get('pb_percentile')
                result['PEåˆ†ä½æ•°(5å¹´)'] = fund_data.get('pe_percentile')
                result['è‚¡æ¯ç‡(%)'] = fund_data.get('dividend_yield')
                result['è¿‘3å¹´å¹³å‡è‚¡æ¯ç‡(%)'] = fund_data.get('dividend_yield_avg_3y')
                dividend_info = fund_data.get('dividend_info', [])
                if dividend_info:
                    dividend_years = []
                    for d in dividend_info[:3]:
                        div_date = d.get('date')
                        if div_date:
                            dividend_years.append(str(div_date.year) if hasattr(div_date, 'year') else str(div_date)[:4])
                    result['è¿‘3å¹´åˆ†çº¢å¹´ä»½'] = ','.join(dividend_years) if dividend_years else 'æ— '
                else:
                    result['è¿‘3å¹´åˆ†çº¢å¹´ä»½'] = 'æ— '
            else:
                result['PB'] = None
                result['PE_TTM'] = None
                result['ROE_TTM'] = None
                result['PBåˆ†ä½æ•°(5å¹´)'] = None
                result['PEåˆ†ä½æ•°(5å¹´)'] = None
                result['è‚¡æ¯ç‡(%)'] = None
                result['è¿‘3å¹´å¹³å‡è‚¡æ¯ç‡(%)'] = None
                result['è¿‘3å¹´åˆ†çº¢å¹´ä»½'] = 'æ— '

        for result in near_lowest_results:
            _apply_fundamental(result, fundamental_data)
        for result in not_near_lowest_results:
            _apply_fundamental(result, fundamental_data)
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        if not os.path.exists(output_dir):
            os.makedirs(output_dir, exist_ok=True)
        
        # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = os.path.join(output_dir, f"æ¥è¿‘å†å²æœ€ä½ä»·ç­›é€‰_{timestamp}.xlsx")
        
        # å¯¼å‡ºExcel
        print("ğŸ“Š å¯¼å‡ºExcelç»“æœ...")
        with pd.ExcelWriter(output_file, engine='openpyxl') as writer:
            # Sheet 1: æ¥è¿‘å†å²æœ€ä½ä»·çš„è‚¡ç¥¨
            if near_lowest_results:
                near_lowest_df = pd.DataFrame(near_lowest_results)
                # è¦æ±‚2ï¼šæ ¼å¼åŒ–è‚¡ç¥¨ä»£ç ï¼Œæ·»åŠ äº¤æ˜“æ‰€åç¼€
                if 'è‚¡ç¥¨ä»£ç ' in near_lowest_df.columns:
                    near_lowest_df['è‚¡ç¥¨ä»£ç '] = near_lowest_df['è‚¡ç¥¨ä»£ç '].apply(format_stock_code)
                # æŒ‰å½“å‰ä»·ç›¸å¯¹æœ€ä½ä»·æ¶¨å¹…æ’åºï¼ˆä»ä½åˆ°é«˜ï¼‰
                near_lowest_df = near_lowest_df.sort_values('å½“å‰ä»·ç›¸å¯¹æœ€ä½ä»·æ¶¨å¹…')
                near_lowest_df.to_excel(writer, sheet_name='æ¥è¿‘å†å²æœ€ä½ä»·', index=False)
                print(f"  âœ… æ¥è¿‘å†å²æœ€ä½ä»·: {len(near_lowest_df)} åª")
            
            # Sheet 2: ä¸æ»¡è¶³æ¡ä»¶çš„è‚¡ç¥¨
            if not_near_lowest_results:
                not_near_lowest_df = pd.DataFrame(not_near_lowest_results)
                # è¦æ±‚2ï¼šæ ¼å¼åŒ–è‚¡ç¥¨ä»£ç ï¼Œæ·»åŠ äº¤æ˜“æ‰€åç¼€
                if 'è‚¡ç¥¨ä»£ç ' in not_near_lowest_df.columns:
                    not_near_lowest_df['è‚¡ç¥¨ä»£ç '] = not_near_lowest_df['è‚¡ç¥¨ä»£ç '].apply(format_stock_code)
                # æŒ‰å½“å‰ä»·ç›¸å¯¹æœ€ä½ä»·æ¶¨å¹…æ’åº
                not_near_lowest_df = not_near_lowest_df.sort_values('å½“å‰ä»·ç›¸å¯¹æœ€ä½ä»·æ¶¨å¹…')
                not_near_lowest_df.to_excel(writer, sheet_name='ä¸æ»¡è¶³æ¡ä»¶', index=False)
                print(f"  âœ… ä¸æ»¡è¶³æ¡ä»¶: {len(not_near_lowest_df)} åª")
        
        elapsed_time = time.time() - start_time
        print("=" * 60)
        print(f"âœ… æ¥è¿‘å†å²æœ€ä½ä»·ç­›é€‰å®Œæˆï¼Œå…±åˆ†æ {len(valid_results)} åªè‚¡ç¥¨")
        print(f"è¾“å‡ºæ–‡ä»¶: {output_file}")
        print(f"æ€»è€—æ—¶: {elapsed_time:.2f} ç§’")
        print("=" * 60)
        
        return output_file
        
    except Exception as e:
        logger.error(f"è¿è¡Œæ¥è¿‘å†å²æœ€ä½ä»·ç­›é€‰æ—¶å‡ºé”™: {e}", exc_info=True)
        print(f"âŒ åˆ†æå¤±è´¥: {e}")
        return None

