#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
éœ‡è¡è‚¡ç¥¨è¯†åˆ«æ¨¡å—

åŠŸèƒ½ï¼šè¯†åˆ«å¤„äºéœ‡è¡å›è°ƒçŠ¶æ€çš„è‚¡ç¥¨
å®ç°æ€è·¯ï¼š
- ä½¿ç”¨MASSå‡çº¿æ’åˆ—æ‰“åˆ†æ¨¡å‹åˆ¤å®šä¸­ç­‰å¼ºåº¦ï¼ˆ40-60åˆ†ï¼‰
- ç”¨å‡çº¿æ–œç‡ç»å¯¹å€¼åˆ¤å®šæ¨ªç›˜çŠ¶æ€
- é€šè¿‡ä»·æ ¼å›´ç»•MA5é¢‘ç¹ç©¿è¶Šåˆ¤å®šéœ‡è¡è¡Œä¸º
- å¤ç”¨é¡¹ç›®ç°æœ‰æ•°æ®è·å–å’Œç¼“å­˜æœºåˆ¶
"""

import os
import time
import logging
from datetime import datetime, timedelta, date
from typing import Dict, List, Optional, Tuple

import numpy as np
import pandas as pd

from data_fetcher import JuyuanDataFetcher
from config import STOCK_LIST_LIMIT, MAX_TRADING_DAYS_AGO
from high_performance_threading import HighPerformanceThreadPool
from futures_incremental_cache_manager import futures_incremental_cache_manager
from cache_validator import validate_cache_data
from uptrend_rebound_analysis import calculate_mass_score, calculate_ma_slope

logger = logging.getLogger(__name__)


def count_ma5_crosses(close: pd.Series, ma5: pd.Series, lookback_days: int = 15) -> int:
    """
    ç»Ÿè®¡æœ€è¿‘Nå¤©å†…ï¼Œæ”¶ç›˜ä»·å¯¹MA5çš„ä¸Šä¸‹ç©¿è¶Šæ¬¡æ•°
    
    å‚æ•°ï¼š
    - close: æ”¶ç›˜ä»·åºåˆ—
    - ma5: MA5åºåˆ—
    - lookback_days: å›çœ‹å¤©æ•°
    
    è¿”å›ï¼š
    - ç©¿è¶Šæ¬¡æ•°ï¼ˆæ•´æ•°ï¼‰
    """
    try:
        if len(close) < lookback_days + 1 or len(ma5) < lookback_days + 1:
            return 0
        
        # å–æœ€è¿‘N+1å¤©çš„æ•°æ®ï¼ˆéœ€è¦æ¯”è¾ƒå‰ä¸€æ—¥ï¼‰
        recent_close = close.iloc[-(lookback_days+1):]
        recent_ma5 = ma5.iloc[-(lookback_days+1):]
        
        cross_count = 0
        
        for i in range(1, len(recent_close)):
            prev_close = recent_close.iloc[i-1]
            curr_close = recent_close.iloc[i]
            prev_ma5 = recent_ma5.iloc[i-1]
            curr_ma5 = recent_ma5.iloc[i]
            
            # ä¸Šç©¿ï¼šå‰ä¸€æ—¥ <= MA5ï¼Œå½“å‰ > MA5
            if prev_close <= prev_ma5 and curr_close > curr_ma5:
                cross_count += 1
            # ä¸‹ç©¿ï¼šå‰ä¸€æ—¥ >= MA5ï¼Œå½“å‰ < MA5
            elif prev_close >= prev_ma5 and curr_close < curr_ma5:
                cross_count += 1
        
        return cross_count
        
    except Exception as e:
        logger.error(f"è®¡ç®—MA5ç©¿è¶Šæ¬¡æ•°å¤±è´¥: {e}")
        return 0


def analyze_oscillation_stock(code: str, stock_data: pd.DataFrame,
                             mass_short_period: int = 60,
                             mass_long_period: int = 360,
                             mass_min: float = 40.0,
                             mass_max: float = 60.0,
                             ma5_slope_threshold: float = 0.0055,
                             ma10_slope_threshold: float = 0.0031,
                             lookback_days: int = 15,
                             min_cross_count: int = 2,
                             debug_mode: bool = False) -> Optional[Dict]:
    """
    åˆ†æå•åªè‚¡ç¥¨æ˜¯å¦ä¸ºéœ‡è¡è‚¡ç¥¨
    
    å‚æ•°ï¼š
    - code: è‚¡ç¥¨ä»£ç 
    - stock_data: è‚¡ç¥¨æ•°æ®DataFrameï¼ˆéœ€åŒ…å«Closeç­‰å­—æ®µï¼‰
    - mass_short_period: MASSçŸ­æœŸå‘¨æœŸï¼ˆé»˜è®¤60ï¼‰
    - mass_long_period: MASSé•¿æœŸå‘¨æœŸï¼ˆé»˜è®¤360ï¼‰
    - mass_min: MASSå¾—åˆ†ä¸‹é™ï¼ˆé»˜è®¤40ï¼‰
    - mass_max: MASSå¾—åˆ†ä¸Šé™ï¼ˆé»˜è®¤60ï¼‰
    - ma5_slope_threshold: MA5æ–œç‡ç»å¯¹å€¼é˜ˆå€¼ï¼ˆé»˜è®¤0.0055ï¼‰
    - ma10_slope_threshold: MA10æ–œç‡ç»å¯¹å€¼é˜ˆå€¼ï¼ˆé»˜è®¤0.0031ï¼‰
    - lookback_days: ç»Ÿè®¡ç©¿è¶Šæ¬¡æ•°çš„å›çœ‹å¤©æ•°ï¼ˆé»˜è®¤15ï¼‰
    - min_cross_count: æœ€å°ç©¿è¶Šæ¬¡æ•°ï¼ˆé»˜è®¤2ï¼‰
    - debug_mode: è°ƒè¯•æ¨¡å¼
    
    è¿”å›ï¼š
    - åŒ…å«åˆ†æç»“æœçš„å­—å…¸ï¼Œå¦‚æœä¸æ»¡è¶³æ¡ä»¶åˆ™è¿”å›None
    """
    try:
        if stock_data is None or stock_data.empty:
            return {
                'è‚¡ç¥¨ä»£ç ': code,
                'æ˜¯å¦éœ‡è¡': False,
                'MASSçŸ­æœŸ': None, 'MASSé•¿æœŸ': None, 'MA5æ–œç‡': None, 'MA10æ–œç‡': None,
                'ç©¿è¶Šæ¬¡æ•°': None, 'æ”¶ç›˜ä»·': None, 'MA5': None, 'MA10': None,
                'MASSæ¡ä»¶æ»¡è¶³': False, 'æ–œç‡æ¡ä»¶æ»¡è¶³': False, 'ç©¿è¶Šæ¡ä»¶æ»¡è¶³': False,
                'å¤±è´¥åŸå› ': 'æ•°æ®ä¸ºç©º',
                **({'è°ƒè¯•æ¨¡å¼': True, 'è°ƒè¯•_æ•°æ®çŠ¶æ€': 'æ•°æ®ä¸ºç©º'} if debug_mode else {}),
            }
        
        # ç¡®ä¿æŒ‰æ—¥æœŸå‡åº
        df = stock_data.sort_index().copy()
        
        # éœ€è¦è‡³å°‘360å¤©çš„æ•°æ®æ¥è®¡ç®—é•¿æœŸMASS
        min_required_days = max(mass_long_period, 100)
        if len(df) < min_required_days:
            return {
                'è‚¡ç¥¨ä»£ç ': code,
                'æ˜¯å¦éœ‡è¡': False,
                'MASSçŸ­æœŸ': None, 'MASSé•¿æœŸ': None, 'MA5æ–œç‡': None, 'MA10æ–œç‡': None,
                'ç©¿è¶Šæ¬¡æ•°': None, 'æ”¶ç›˜ä»·': None, 'MA5': None, 'MA10': None,
                'MASSæ¡ä»¶æ»¡è¶³': False, 'æ–œç‡æ¡ä»¶æ»¡è¶³': False, 'ç©¿è¶Šæ¡ä»¶æ»¡è¶³': False,
                'å¤±è´¥åŸå› ': f'æ•°æ®ä¸è¶³ï¼ˆ{len(df)}å¤© < {min_required_days}å¤©ï¼‰',
                **({'è°ƒè¯•æ¨¡å¼': True, 'è°ƒè¯•_æ•°æ®çŠ¶æ€': f'æ•°æ®ä¸è¶³ï¼ˆ{len(df)}å¤© < {min_required_days}å¤©ï¼‰', 'è°ƒè¯•_æ•°æ®å¤©æ•°': len(df)} if debug_mode else {}),
            }
        
        # æå–å…³é”®å­—æ®µ
        close = pd.to_numeric(df.get("Close"), errors="coerce")
        close = close.dropna()
        
        if close.empty:
            return {
                'è‚¡ç¥¨ä»£ç ': code,
                'æ˜¯å¦éœ‡è¡': False,
                'MASSçŸ­æœŸ': None, 'MASSé•¿æœŸ': None, 'MA5æ–œç‡': None, 'MA10æ–œç‡': None,
                'ç©¿è¶Šæ¬¡æ•°': None, 'æ”¶ç›˜ä»·': None, 'MA5': None, 'MA10': None,
                'MASSæ¡ä»¶æ»¡è¶³': False, 'æ–œç‡æ¡ä»¶æ»¡è¶³': False, 'ç©¿è¶Šæ¡ä»¶æ»¡è¶³': False,
                'å¤±è´¥åŸå› ': 'æ”¶ç›˜ä»·æ•°æ®ä¸ºç©º',
                **({'è°ƒè¯•æ¨¡å¼': True, 'è°ƒè¯•_æ•°æ®çŠ¶æ€': 'æ”¶ç›˜ä»·æ•°æ®ä¸ºç©º'} if debug_mode else {}),
            }
        
        # è®¡ç®—æŠ€æœ¯æŒ‡æ ‡
        # 1. MASSå‡çº¿æ’åˆ—æ‰“åˆ†
        mass_short = calculate_mass_score(close, mass_short_period)
        mass_long = calculate_mass_score(close, mass_long_period)
        
        # 2. å‡çº¿è®¡ç®—
        ma5 = close.rolling(5).mean()
        ma10 = close.rolling(10).mean()
        
        # 3. å‡çº¿æ–œç‡
        ma5_slope = calculate_ma_slope(ma5)
        ma10_slope = calculate_ma_slope(ma10)
        
        # 4. ç»Ÿè®¡ä»·æ ¼ç©¿è¶Šæ¬¡æ•°
        cross_count = count_ma5_crosses(close, ma5, lookback_days=lookback_days)
        
        # åˆ¤æ–­æ¡ä»¶
        # 1. MASSå¾—åˆ†åœ¨ä¸­ç­‰å¼ºåº¦åŒºé—´
        mass_ok = (mass_min <= mass_short <= mass_max and 
                  mass_min <= mass_long <= mass_max)
        
        # 2. å‡çº¿æ–œç‡ç»å¯¹å€¼å°ï¼ˆæ¥è¿‘æ¨ªç›˜ï¼‰
        slope_ok = (abs(ma5_slope) <= ma5_slope_threshold and 
                   abs(ma10_slope) <= ma10_slope_threshold)
        
        # 3. ä»·æ ¼å›´ç»•MA5é¢‘ç¹ç©¿è¶Š
        cross_ok = cross_count >= min_cross_count
        
        # ç»¼åˆåˆ¤æ–­
        is_oscillation = mass_ok and slope_ok and cross_ok
        
        # æ— è®ºæ˜¯å¦éœ‡è¡ï¼Œå‡è¿”å›åˆ¤æ–­æŒ‡æ ‡æ˜ç»†ï¼ˆä¾¿äºå¯¼å‡ºã€å¤ç›˜ï¼‰
        fail_parts = []
        if not mass_ok:
            fail_parts.append('MASSä¸åœ¨åŒºé—´')
        if not slope_ok:
            fail_parts.append('å‡çº¿æ–œç‡è¶…é˜ˆå€¼')
        if not cross_ok:
            fail_parts.append('ç©¿è¶Šæ¬¡æ•°ä¸è¶³')
        fail_reason = '; '.join(fail_parts) if fail_parts else None
        
        result = {
            'è‚¡ç¥¨ä»£ç ': code,
            'æ˜¯å¦éœ‡è¡': is_oscillation,
            'MASSçŸ­æœŸ': round(mass_short, 2),
            'MASSé•¿æœŸ': round(mass_long, 2),
            'MA5æ–œç‡': round(ma5_slope * 100, 4),  # è½¬æ¢ä¸ºç™¾åˆ†æ¯”
            'MA10æ–œç‡': round(ma10_slope * 100, 4),
            'MA5æ–œç‡ç»å¯¹å€¼': round(abs(ma5_slope) * 100, 4),
            'MA10æ–œç‡ç»å¯¹å€¼': round(abs(ma10_slope) * 100, 4),
            'ç©¿è¶Šæ¬¡æ•°': cross_count,
            'æ”¶ç›˜ä»·': round(float(close.iloc[-1]), 2),
            'MA5': round(float(ma5.iloc[-1]), 2),
            'MA10': round(float(ma10.iloc[-1]), 2),
            'MASSæ¡ä»¶æ»¡è¶³': mass_ok,
            'æ–œç‡æ¡ä»¶æ»¡è¶³': slope_ok,
            'ç©¿è¶Šæ¡ä»¶æ»¡è¶³': cross_ok,
            'å¤±è´¥åŸå› ': fail_reason,
        }
        if debug_mode:
            result['è°ƒè¯•æ¨¡å¼'] = True
            result['è°ƒè¯•_æ•°æ®å¤©æ•°'] = len(df)
        return result
        
    except Exception as e:
        logger.error(f"åˆ†æè‚¡ç¥¨ {code} éœ‡è¡çŠ¶æ€æ—¶å‡ºé”™: {e}")
        return {
            'è‚¡ç¥¨ä»£ç ': code,
            'æ˜¯å¦éœ‡è¡': False,
            'MASSçŸ­æœŸ': None, 'MASSé•¿æœŸ': None, 'MA5æ–œç‡': None, 'MA10æ–œç‡': None,
            'ç©¿è¶Šæ¬¡æ•°': None, 'æ”¶ç›˜ä»·': None, 'MA5': None, 'MA10': None,
            'MASSæ¡ä»¶æ»¡è¶³': False, 'æ–œç‡æ¡ä»¶æ»¡è¶³': False, 'ç©¿è¶Šæ¡ä»¶æ»¡è¶³': False,
            'å¤±è´¥åŸå› ': f'åˆ†æå¼‚å¸¸: {str(e)}',
        }


def run_oscillation_stock_analysis(limit: Optional[int] = None,
                                  max_days_ago: Optional[int] = None,
                                  mass_short_period: int = 60,
                                  mass_long_period: int = 360,
                                  mass_min: float = 40.0,
                                  mass_max: float = 60.0,
                                  ma5_slope_threshold: float = 0.0055,
                                  ma10_slope_threshold: float = 0.0031,
                                  lookback_days: int = 15,
                                  min_cross_count: int = 2,
                                  debug_mode: bool = False,
                                  output_dir: str = "outputs",
                                  cutoff_date: Optional[date] = None) -> Optional[str]:
    """
    ä¸€é”®æ‰§è¡Œ"éœ‡è¡è‚¡ç¥¨è¯†åˆ«"åˆ†æ
    
    å‚æ•°ï¼š
    - limit: è‚¡ç¥¨æ•°é‡é™åˆ¶ï¼ŒNone ä½¿ç”¨é…ç½®ä¸­çš„ STOCK_LIST_LIMIT
    - max_days_ago: æœ€å¤§å…è®¸è¡Œæƒ…æ»åå¤©æ•°ï¼ŒNone ä½¿ç”¨é…ç½®ä¸­çš„ MAX_TRADING_DAYS_AGO
    - mass_short_period: MASSçŸ­æœŸå‘¨æœŸï¼ˆé»˜è®¤60ï¼‰
    - mass_long_period: MASSé•¿æœŸå‘¨æœŸï¼ˆé»˜è®¤360ï¼‰
    - mass_min: MASSå¾—åˆ†ä¸‹é™ï¼ˆé»˜è®¤40ï¼‰
    - mass_max: MASSå¾—åˆ†ä¸Šé™ï¼ˆé»˜è®¤60ï¼‰
    - ma5_slope_threshold: MA5æ–œç‡ç»å¯¹å€¼é˜ˆå€¼ï¼ˆé»˜è®¤0.0055ï¼‰
    - ma10_slope_threshold: MA10æ–œç‡ç»å¯¹å€¼é˜ˆå€¼ï¼ˆé»˜è®¤0.0031ï¼‰
    - lookback_days: ç»Ÿè®¡ç©¿è¶Šæ¬¡æ•°çš„å›çœ‹å¤©æ•°ï¼ˆé»˜è®¤15ï¼‰
    - min_cross_count: æœ€å°ç©¿è¶Šæ¬¡æ•°ï¼ˆé»˜è®¤2ï¼‰
    - debug_mode: è°ƒè¯•æ¨¡å¼
    - output_dir: è¾“å‡ºç›®å½•
    
    è¿”å›ï¼š
    - è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœå¤±è´¥åˆ™è¿”å›None
    """
    from logger_config import init_logger
    
    session_logger = init_logger("logs")
    
    if limit is None:
        limit = STOCK_LIST_LIMIT
    if max_days_ago is None:
        max_days_ago = MAX_TRADING_DAYS_AGO
    
    print("=" * 60)
    print("éœ‡è¡è‚¡ç¥¨è¯†åˆ«")
    print("=" * 60)
    print(f"è‚¡ç¥¨æ•°é‡ä¸Šé™: {limit}")
    print(f"æœ€å¤§å…è®¸è¡Œæƒ…æ»åå¤©æ•°: {max_days_ago}")
    print(f"MASSçŸ­æœŸå‘¨æœŸ: {mass_short_period}å¤©")
    print(f"MASSé•¿æœŸå‘¨æœŸ: {mass_long_period}å¤©")
    print(f"MASSå¾—åˆ†èŒƒå›´: {mass_min} ~ {mass_max}åˆ†")
    print(f"MA5æ–œç‡é˜ˆå€¼: {ma5_slope_threshold*100:.4f}%")
    print(f"MA10æ–œç‡é˜ˆå€¼: {ma10_slope_threshold*100:.4f}%")
    print(f"ç©¿è¶Šç»Ÿè®¡å¤©æ•°: {lookback_days}å¤©")
    print(f"æœ€å°ç©¿è¶Šæ¬¡æ•°: {min_cross_count}æ¬¡")
    print("=" * 60)
    
    start_time = time.time()
    
    try:
        fetcher = JuyuanDataFetcher(use_connection_pool=True)
        
        # ç¡®å®šæˆªæ­¢æ—¥æœŸ
        if cutoff_date is not None:
            end_date = cutoff_date
            print(f"ğŸ“… å›æµ‹æ¨¡å¼ - æŒ‡å®šæˆªæ­¢æ—¥æœŸ: {end_date}")
        else:
            end_date = fetcher.get_latest_trading_date()
            print(f"ğŸ“… æ•°æ®åº“æœ€æ–°äº¤æ˜“æ—¥: {end_date}")
        print("=" * 60)
        
        # 1. è·å–æ´»è·ƒè‚¡ç¥¨åˆ—è¡¨
        print("ğŸ“Š è·å–æ´»è·ƒè‚¡ç¥¨åˆ—è¡¨ï¼ˆæ­£åœ¨æŸ¥è¯¢æ•°æ®åº“ï¼Œè¯·ç¨å€™...ï¼‰...")
        list_start_time = time.time()
        # å¦‚æœæŒ‡å®šäº†æˆªæ­¢æ—¥æœŸï¼Œä½¿ç”¨cutoff_dateå‚æ•°
        stock_info_list = fetcher.get_stock_list(limit=limit, max_days_ago=max_days_ago, cutoff_date=cutoff_date)
        list_elapsed = time.time() - list_start_time
        print(f"  âœ… è‚¡ç¥¨åˆ—è¡¨è·å–å®Œæˆï¼ˆè€—æ—¶: {list_elapsed:.1f}ç§’ï¼‰")
        
        # å…¼å®¹ä¸¤ç§è¿”å›æ ¼å¼
        if stock_info_list and isinstance(stock_info_list[0], dict):
            codes = [info["code"] for info in stock_info_list]
        else:
            codes = stock_info_list
        
        if not codes:
            print("âŒ æœªè·å–åˆ°ä»»ä½•æ´»è·ƒè‚¡ç¥¨")
            return None
        
        print(f"âœ… å®é™…è‚¡ç¥¨æ•°é‡: {len(codes)}")
        
        # 2. æ‰¹é‡è·å–è‚¡ç¥¨æ•°æ®ï¼ˆä½¿ç”¨å¢é‡ç¼“å­˜ï¼‰
        print("ğŸ“ˆ æ‰¹é‡è·å–è¡Œæƒ…æ•°æ®ï¼ˆå¢é‡ç¼“å­˜ + æ‰¹é‡SQL + å¤šçº¿ç¨‹å¹¶å‘ï¼‰...")
        
        # è®¡ç®—æ—¥æœŸèŒƒå›´ï¼ˆè‡³å°‘éœ€è¦mass_long_period + 100å¤©çš„æ•°æ®ï¼‰
        days_needed = max(mass_long_period + 100, 500)
        # end_date å·²åœ¨ä¸Šé¢ç¡®å®šï¼ˆcutoff_date æˆ–æœ€æ–°äº¤æ˜“æ—¥ï¼‰
        start_date = end_date - timedelta(days=days_needed + 30)
        
        # å°è¯•ä»ç¼“å­˜åŠ è½½ï¼ˆæ‰¹é‡ï¼‰
        print("  ğŸ” æ£€æŸ¥å¢é‡ç¼“å­˜...")
        print(f"  ğŸ“… è¯·æ±‚æ—¥æœŸèŒƒå›´: {start_date} è‡³ {end_date}ï¼ˆæ•°æ®åº“æœ€æ–°äº¤æ˜“æ—¥ï¼Œéœ€è¦{days_needed}å¤©æ•°æ®ï¼‰")
        cached_stock_data, missing_stock_codes = futures_incremental_cache_manager.load_stocks_data(
            codes, start_date, end_date
        )
        
        # æ£€æŸ¥ç¼“å­˜æ•°æ®çš„å®Œæ•´æ€§å’Œæ—¥æœŸèŒƒå›´ï¼ˆæ”¯æŒéƒ¨åˆ†å‘½ä¸­å¹¶å¢é‡è¡¥å…¨ï¼‰
        insufficient_data_codes = []
        partial_data_codes = {}
        valid_cached_data = {}
        
        for code, data in cached_stock_data.items():
            # ä½¿ç”¨ç»Ÿä¸€çš„ç¼“å­˜éªŒè¯å‡½æ•°
            validation_result = validate_cache_data(
                data=data,
                start_date=start_date,
                end_date=end_date,
                days_needed=days_needed
            )
            
            if validation_result.is_valid:
                # ç¼“å­˜æ•°æ®å®Œå…¨æœ‰æ•ˆ
                valid_cached_data[code] = data
            elif validation_result.is_partial:
                # ç¼“å­˜éƒ¨åˆ†æœ‰æ•ˆï¼Œéœ€è¦å¢é‡è¡¥å…¨
                partial_data_codes[code] = {
                    'cached_data': data,
                    'missing_start': validation_result.missing_start,
                    'missing_end': validation_result.missing_end,
                    'cache_start': validation_result.cache_start,
                    'cache_end': validation_result.cache_end
                }
            else:
                # ç¼“å­˜æ•°æ®æ— æ•ˆï¼Œéœ€è¦é‡æ–°è·å–å…¨éƒ¨æ•°æ®
                insufficient_data_codes.append(code)
        
        # åˆå¹¶éœ€è¦é‡æ–°è·å–çš„è‚¡ç¥¨ä»£ç 
        all_missing_codes = list(set(missing_stock_codes + insufficient_data_codes))
        cache_hit_count = len(valid_cached_data)
        partial_hit_count = len(partial_data_codes)
        cache_miss_count = len(all_missing_codes)
        
        print(f"  âœ… ç¼“å­˜å®Œå…¨æœ‰æ•ˆ: {cache_hit_count} åªè‚¡ç¥¨")
        if partial_hit_count > 0:
            print(f"  ğŸ”„ ç¼“å­˜éƒ¨åˆ†å‘½ä¸­: {partial_hit_count} åªè‚¡ç¥¨ï¼ˆå°†å¢é‡è¡¥å…¨ç¼ºå¤±æ—¥æœŸèŒƒå›´ï¼‰")
        print(f"  âš ï¸  éœ€è¦ä»æ•°æ®åº“è·å–: {cache_miss_count} åªè‚¡ç¥¨")
        
        # å¯¹ç¼ºå¤±æˆ–æ•°æ®ä¸è¶³çš„è‚¡ç¥¨ä»æ•°æ®åº“è·å–ï¼ˆæ‰¹é‡ï¼‰
        fetch_start_time = time.time()
        fetched_stock_data = {}
        
        # 1. å¤„ç†å®Œå…¨ç¼ºå¤±çš„è‚¡ç¥¨
        if all_missing_codes:
            print(f"  ğŸ“¥ ä»æ•°æ®åº“è·å–å®Œå…¨ç¼ºå¤±çš„ {len(all_missing_codes)} åªè‚¡ç¥¨æ•°æ®...")
            fully_missing_data = fetcher.batch_get_stock_data_with_adjustment(
                all_missing_codes,
                days=days_needed,
                time_config=None
            )
            if fully_missing_data:
                fetched_stock_data.update(fully_missing_data)
        
        # 2. å¤„ç†éƒ¨åˆ†å‘½ä¸­çš„è‚¡ç¥¨ï¼ˆå¢é‡è¡¥å…¨ï¼‰
        if partial_data_codes:
            print(f"  ğŸ”„ å¢é‡è¡¥å…¨éƒ¨åˆ†å‘½ä¸­çš„ {len(partial_data_codes)} åªè‚¡ç¥¨æ•°æ®...")
            date_range_groups = {}
            
            for code, partial_info in partial_data_codes.items():
                missing_start = partial_info.get('missing_start')
                missing_end = partial_info.get('missing_end')
                cache_start = partial_info.get('cache_start')
                cache_end = partial_info.get('cache_end')
                
                if missing_end:
                    fetch_start = cache_end + timedelta(days=1) if cache_end else start_date
                    fetch_end = missing_end
                else:
                    fetch_start = cache_end + timedelta(days=1) if cache_end else start_date
                    fetch_end = end_date
                
                fetch_start = max(fetch_start, start_date)
                fetch_end = min(fetch_end, end_date)
                
                if fetch_start <= fetch_end:
                    range_key = (fetch_start, fetch_end)
                    if range_key not in date_range_groups:
                        date_range_groups[range_key] = []
                    date_range_groups[range_key].append(code)
            
            for (fetch_start, fetch_end), codes_in_group in date_range_groups.items():
                days_to_fetch = (fetch_end - fetch_start).days + 30
                partial_fetched = fetcher.batch_get_stock_data_with_adjustment(
                    codes_in_group,
                    days=days_to_fetch,
                    time_config=None
                )
                if partial_fetched:
                    fetched_stock_data.update(partial_fetched)
        
        # åˆå¹¶éƒ¨åˆ†å‘½ä¸­çš„ç¼“å­˜æ•°æ®
        if partial_data_codes:
            for code, partial_info in partial_data_codes.items():
                cached_data = partial_info['cached_data']
                if code in fetched_stock_data:
                    new_data = fetched_stock_data[code]
                    if not new_data.empty:
                        combined_data = pd.concat([cached_data, new_data])
                        combined_data = combined_data.sort_index()
                        combined_data = combined_data[~combined_data.index.duplicated(keep='last')]
                        fetched_stock_data[code] = combined_data
                else:
                    fetched_stock_data[code] = cached_data
        
        if fetched_stock_data:
            fetch_elapsed = time.time() - fetch_start_time
            print(f"  âœ… æ•°æ®è·å–å®Œæˆï¼ˆè€—æ—¶: {fetch_elapsed:.1f}ç§’ï¼‰")
            
            # ä¿å­˜æ–°è·å–çš„æ•°æ®åˆ°ç¼“å­˜
            print(f"  ğŸ’¾ ä¿å­˜ {len(fetched_stock_data)} åªè‚¡ç¥¨çš„æ•°æ®åˆ°å¢é‡ç¼“å­˜...")
            futures_incremental_cache_manager.save_stocks_data(
                list(fetched_stock_data.keys()),
                fetched_stock_data,
                start_date,
                end_date
            )
            print(f"  âœ… ç¼“å­˜ä¿å­˜å®Œæˆ")
            
            # åˆå¹¶æœ‰æ•ˆç¼“å­˜æ•°æ®å’Œæ–°å¢æ•°æ®
            valid_cached_data.update(fetched_stock_data)
        
        all_stock_data = valid_cached_data
        
        if not all_stock_data:
            print("âŒ æœªè·å–åˆ°ä»»ä½•è‚¡ç¥¨è¡Œæƒ…æ•°æ®")
            return None
        
        print(f"âœ… æˆåŠŸè·å– {len(all_stock_data)} åªè‚¡ç¥¨çš„è¡Œæƒ…æ•°æ®")
        
        # 3. å¹¶è¡Œè®¡ç®—éœ‡è¡ä¿¡å·
        print("ğŸš€ å¹¶è¡Œè®¡ç®—éœ‡è¡è¯†åˆ«ä¿¡å·...")
        
        def _task_oscillation(code: str,
                              stock_data_dict: Dict[str, pd.DataFrame],
                              mass_short_period: int,
                              mass_long_period: int,
                              mass_min: float,
                              mass_max: float,
                              ma5_slope_threshold: float,
                              ma10_slope_threshold: float,
                              lookback_days: int,
                              min_cross_count: int,
                              debug_mode: bool = False) -> Optional[Dict]:
            """ä¾›é«˜æ€§èƒ½çº¿ç¨‹æ± è°ƒç”¨çš„ä»»åŠ¡å‡½æ•°"""
            df = stock_data_dict.get(code)
            if df is None or df.empty:
                return None
            return analyze_oscillation_stock(
                code=code,
                stock_data=df,
                mass_short_period=mass_short_period,
                mass_long_period=mass_long_period,
                mass_min=mass_min,
                mass_max=mass_max,
                ma5_slope_threshold=ma5_slope_threshold,
                ma10_slope_threshold=ma10_slope_threshold,
                lookback_days=lookback_days,
                min_cross_count=min_cross_count,
                debug_mode=debug_mode
            )
        
        thread_pool = HighPerformanceThreadPool(progress_desc="éœ‡è¡è‚¡ç¥¨è¯†åˆ«")
        
        tasks = list(all_stock_data.keys())
        results = thread_pool.execute_batch(
            tasks,
            _task_oscillation,
            all_stock_data,
            mass_short_period,
            mass_long_period,
            mass_min,
            mass_max,
            ma5_slope_threshold,
            ma10_slope_threshold,
            lookback_days,
            min_cross_count,
            debug_mode,
        )
        
        # æ±‡æ€»ç»“æœ
        valid_results = [r for r in results if r is not None]
        oscillation_results = [r for r in valid_results if r.get('æ˜¯å¦éœ‡è¡', False)]
        debug_results = [r for r in valid_results if r.get('è°ƒè¯•æ¨¡å¼', False)]
        
        if debug_mode:
            print(f"\nğŸ“Š è°ƒè¯•æ¨¡å¼ç»Ÿè®¡ä¿¡æ¯:")
            print(f"  æ€»è‚¡ç¥¨æ•°: {len(results)}")
            print(f"  æœ‰æ•ˆç»“æœ: {len(valid_results)} åª")
            print(f"  éœ‡è¡è‚¡ç¥¨: {len(oscillation_results)} åª")
            print(f"  è°ƒè¯•ç»“æœ: {len(debug_results)} åª")
        else:
            print(f"\nâœ… å‘ç° {len(oscillation_results)} åªéœ‡è¡è‚¡ç¥¨")
            if len(valid_results) > len(oscillation_results):
                print(f"ğŸ“‹ å¦æœ‰ {len(valid_results) - len(oscillation_results)} åªè‚¡ç¥¨ä¸æ»¡è¶³æ¡ä»¶")
        
        # ä½¿ç”¨ç»Ÿä¸€çš„å¯¼å‡ºå·¥å…·æ¨¡å—
        from export_utils import get_timestamped_output_path, format_stock_code_in_df
        
        # æ ¼å¼åŒ–è‚¡ç¥¨ä»£ç 
        if oscillation_results:
            oscillation_df = pd.DataFrame(oscillation_results)
            if not oscillation_df.empty and 'è‚¡ç¥¨ä»£ç ' in oscillation_df.columns:
                oscillation_df = format_stock_code_in_df(oscillation_df, code_column='è‚¡ç¥¨ä»£ç ')
        else:
            oscillation_df = pd.DataFrame()
        
        # å§‹ç»ˆå¯¼å‡ºæ‰€æœ‰è‚¡ç¥¨çš„åˆ¤æ–­æŒ‡æ ‡æ˜ç»†ï¼ˆå«ä¸ç¬¦åˆæ¡ä»¶çš„ï¼‰ï¼Œä¾¿äºå¤ç›˜
        all_df = pd.DataFrame(valid_results) if valid_results else pd.DataFrame()
        if not all_df.empty and 'è‚¡ç¥¨ä»£ç ' in all_df.columns:
            all_df = format_stock_code_in_df(all_df, code_column='è‚¡ç¥¨ä»£ç ')
        
        # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶åï¼ˆè‡ªåŠ¨åˆ›å»ºæ—¥æœŸæ–‡ä»¶å¤¹ï¼‰
        output_file = get_timestamped_output_path(output_dir, "éœ‡è¡è‚¡ç¥¨è¯†åˆ«.xlsx")
        
        # å¯¼å‡ºExcelï¼ˆopenpyxl è¦æ±‚è‡³å°‘æœ‰ä¸€ä¸ªå¯è§ sheetï¼Œæ•…åœ¨æ— æ•°æ®æ—¶å†™å…¥è¯´æ˜é¡µï¼‰
        print("ğŸ“Š å¯¼å‡ºExcelç»“æœ...")
        with pd.ExcelWriter(output_file, engine='openpyxl') as writer:
            # Sheet 1: éœ‡è¡è‚¡ç¥¨ï¼ˆå¦‚æœæœ‰ï¼‰
            if not oscillation_df.empty:
                # æŒ‰MASSçŸ­æœŸå¾—åˆ†æ’åº
                if 'MASSçŸ­æœŸ' in oscillation_df.columns:
                    oscillation_df = oscillation_df.sort_values('MASSçŸ­æœŸ', ascending=False)
                oscillation_df.to_excel(writer, sheet_name='éœ‡è¡è‚¡ç¥¨', index=False)
                print(f"  âœ… éœ‡è¡è‚¡ç¥¨: {len(oscillation_df)} åª")
            
            # Sheet 2: æ‰€æœ‰ç»“æœï¼ˆå«ä¸æ»¡è¶³æ¡ä»¶çš„åˆ¤æ–­æŒ‡æ ‡æ˜ç»†ï¼‰
            if not all_df.empty:
                if 'MASSçŸ­æœŸ' in all_df.columns:
                    all_df = all_df.sort_values('MASSçŸ­æœŸ', ascending=False)
                all_df.to_excel(writer, sheet_name='æ‰€æœ‰ç»“æœ', index=False)
                print(f"  âœ… æ‰€æœ‰ç»“æœ: {len(all_df)} åª")
            
            # æ— ä»»ä½•æœ‰æ•ˆç»“æœæ—¶ä¹Ÿå¿…é¡»å†™å…¥è‡³å°‘ä¸€é¡µï¼Œå¦åˆ™ openpyxl æŠ¥é”™: At least one sheet must be visible
            if oscillation_df.empty and all_df.empty:
                pd.DataFrame({'è¯´æ˜': ['æœªè·å–åˆ°æœ‰æ•ˆåˆ†æç»“æœï¼Œè¯·æ£€æŸ¥è‚¡ç¥¨åˆ—è¡¨ä¸æ•°æ®']}).to_excel(
                    writer, sheet_name='è¯´æ˜', index=False)
                print("  âš ï¸ æ— æœ‰æ•ˆç»“æœï¼Œå·²å¯¼å‡ºè¯´æ˜é¡µ")
        
        elapsed = time.time() - start_time
        print(f"\nâœ… éœ‡è¡è‚¡ç¥¨è¯†åˆ«å®Œæˆï¼Œå…±åˆ†æ {len(valid_results)} åªè‚¡ç¥¨")
        if oscillation_results:
            print(f"  - éœ‡è¡è‚¡ç¥¨: {len(oscillation_results)} åª")
        print(f"è¾“å‡ºæ–‡ä»¶: {output_file}")
        print(f"æ€»è€—æ—¶: {elapsed:.2f} ç§’")
        
        # è®°å½•ä¼šè¯
        session_logger.log_session_end(
            {
                "åŠŸèƒ½": "éœ‡è¡è‚¡ç¥¨è¯†åˆ«",
                "è‚¡ç¥¨æ•°é‡": len(codes),
                "æœ‰æ•ˆç»“æœæ•°": len(valid_results),
                "éœ‡è¡è‚¡ç¥¨æ•°": len(oscillation_results),
                "MASSçŸ­æœŸå‘¨æœŸ": mass_short_period,
                "MASSé•¿æœŸå‘¨æœŸ": mass_long_period,
                "MASSå¾—åˆ†èŒƒå›´": f"{mass_min}~{mass_max}",
                "è€—æ—¶": elapsed,
                "è¾“å‡ºæ–‡ä»¶": output_file,
            }
        )
        
        return output_file
        
    except Exception as e:
        logger.error(f"æ‰§è¡Œéœ‡è¡è‚¡ç¥¨è¯†åˆ«æ—¶å‡ºé”™: {e}", exc_info=True)
        print(f"âŒ éœ‡è¡è‚¡ç¥¨è¯†åˆ«å¤±è´¥: {e}")
        return None
    finally:
        fetcher.close()

