import pyodbc
import pandas as pd
import numpy as np
import time
import threading
from concurrent.futures import ThreadPoolExecutor
import logging
from datetime import datetime, timedelta, date
import warnings
from tqdm import tqdm
from typing import List, Dict, Optional
warnings.filterwarnings('ignore')

# è·å–logger
logger = logging.getLogger(__name__)

from juyuan_config import ODBC_CONFIG, DATABASE_CONFIG, TABLE_CONFIG, STOCK_FILTER
from db_schema import TABLES
from config import STOCK_LIST_LIMIT, DB_POOL_SIZE, DB_TIMEOUT
import concurrent.futures

# å…¨å±€è¿æ¥æ± ç®¡ç†ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰
_global_connection_pool = []
_global_pool_lock = threading.Lock()
_global_pool_initialized = False
_last_activity_time = None
_idle_timeout_thread = None
_idle_timeout_lock = threading.Lock()
IDLE_TIMEOUT_SECONDS = 20 * 60  # 20åˆ†é’Ÿç©ºé—²è¶…æ—¶

def _start_idle_timeout_monitor():
    """å¯åŠ¨ç©ºé—²è¶…æ—¶ç›‘æ§çº¿ç¨‹"""
    global _idle_timeout_thread
    
    with _idle_timeout_lock:
        if _idle_timeout_thread is not None and _idle_timeout_thread.is_alive():
            return  # å·²ç»å¯åŠ¨
        
        def monitor_loop():
            """ç›‘æ§å¾ªç¯ï¼šå®šæœŸæ£€æŸ¥è¿æ¥æ± æ˜¯å¦ç©ºé—²è¶…æ—¶"""
            global _last_activity_time, _global_pool_initialized
            while True:
                try:
                    time.sleep(60)  # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
                    
                    with _idle_timeout_lock:
                        if _last_activity_time is None:
                            continue
                        
                        idle_time = time.time() - _last_activity_time
                        
                        if idle_time >= IDLE_TIMEOUT_SECONDS:
                            # è¶…è¿‡20åˆ†é’Ÿæ²¡æœ‰æ´»åŠ¨ï¼Œå…³é—­æ‰€æœ‰è¿æ¥
                            with _global_pool_lock:
                                closed_count = 0
                                while _global_connection_pool:
                                    conn = _global_connection_pool.pop()
                                    try:
                                        conn.close()
                                        closed_count += 1
                                    except Exception as e:
                                        logger.debug(f"å…³é—­ç©ºé—²è¿æ¥æ—¶å‡ºé”™: {e}")
                                
                                if closed_count > 0:
                                    logger.info(f"è¿æ¥æ± ç©ºé—²è¶…æ—¶ï¼ˆ{idle_time/60:.1f}åˆ†é’Ÿï¼‰ï¼Œå·²å…³é—­ {closed_count} ä¸ªè¿æ¥")
                                
                                # é‡ç½®åˆå§‹åŒ–çŠ¶æ€ï¼Œä¸‹æ¬¡ä½¿ç”¨æ—¶é‡æ–°åˆ›å»º
                                _global_pool_initialized = False
                                
                                # é‡ç½®æœ€åæ´»åŠ¨æ—¶é—´
                                _last_activity_time = None
                
                except Exception as e:
                    logger.error(f"ç©ºé—²è¶…æ—¶ç›‘æ§çº¿ç¨‹å‡ºé”™: {e}")
        
        _idle_timeout_thread = threading.Thread(target=monitor_loop, daemon=True, name="ConnectionPoolIdleMonitor")
        _idle_timeout_thread.start()
        logger.debug("æ•°æ®åº“è¿æ¥æ± ç©ºé—²è¶…æ—¶ç›‘æ§çº¿ç¨‹å·²å¯åŠ¨")

def _update_activity_time():
    """æ›´æ–°æœ€åæ´»åŠ¨æ—¶é—´"""
    global _last_activity_time
    _last_activity_time = time.time()

def _get_global_connection_pool():
    """è·å–å…¨å±€è¿æ¥æ± """
    return _global_connection_pool

def _get_global_pool_lock():
    """è·å–å…¨å±€è¿æ¥æ± é”"""
    return _global_pool_lock

class JuyuanDataFetcher:
    def __init__(self, use_connection_pool=True, lazy_init_pool=True):
        self.conn = None
        self.use_connection_pool = use_connection_pool
        # ä½¿ç”¨å…¨å±€è¿æ¥æ± ï¼ˆæ‰€æœ‰å®ä¾‹å…±äº«ï¼‰
        self._connection_pool = _get_global_connection_pool()
        self._pool_lock = _get_global_pool_lock()
        self._lazy_init_pool = lazy_init_pool  # å»¶è¿Ÿåˆå§‹åŒ–è¿æ¥æ± 
        
        # å¯åŠ¨ç©ºé—²è¶…æ—¶ç›‘æ§ï¼ˆåªå¯åŠ¨ä¸€æ¬¡ï¼‰
        _start_idle_timeout_monitor()
        
        # æ›´æ–°æ´»åŠ¨æ—¶é—´
        _update_activity_time()
        
        # å¦‚æœä¸éœ€è¦å»¶è¿Ÿåˆå§‹åŒ–ï¼Œç«‹å³åˆå§‹åŒ–è¿æ¥æ± 
        if use_connection_pool and not lazy_init_pool:
            self._init_connection_pool()

    def _init_connection_pool(self):
        """åˆå§‹åŒ–è¿æ¥æ± ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼Œåªåœ¨éœ€è¦æ—¶åˆ›å»ºè¿æ¥ï¼‰"""
        global _global_pool_initialized
        
        if _global_pool_initialized:
            return
        
        # ä½¿ç”¨è¾ƒå°çš„åˆå§‹è¿æ¥æ± å¤§å°ï¼ŒæŒ‰éœ€æ‰©å±•
        initial_pool_size = min(DB_POOL_SIZE, 10)  # åˆå§‹åªåˆ›å»º10ä¸ªè¿æ¥
        
        logger.info(f"åˆå§‹åŒ–æ•°æ®åº“è¿æ¥æ± ï¼Œåˆå§‹å¤§å°: {initial_pool_size}ï¼ˆæŒ‰éœ€æ‰©å±•åˆ°{DB_POOL_SIZE}ï¼‰")
        
        from tqdm import tqdm
        with tqdm(total=initial_pool_size, desc="åˆ›å»ºæ•°æ®åº“è¿æ¥", unit="ä¸ª", ncols=80) as pbar:
            for i in range(initial_pool_size):
                conn = self._create_connection()
                if conn:
                    self._connection_pool.append(conn)
                pbar.update(1)
        
        _global_pool_initialized = True
        logger.info(f"è¿æ¥æ± åˆå§‹åŒ–å®Œæˆï¼Œå¯ç”¨è¿æ¥æ•°: {len(self._connection_pool)}")
        
        # æ›´æ–°æ´»åŠ¨æ—¶é—´
        _update_activity_time()

    def _create_connection(self):
        """åˆ›å»ºæ•°æ®åº“è¿æ¥"""
        try:
            # å°è¯•å¤šä¸ªå¯èƒ½çš„é©±åŠ¨åç§°ï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰
            possible_drivers = [
                ODBC_CONFIG['driver'],  # é…ç½®çš„é©±åŠ¨
                "ODBC Driver 18 for SQL Server",  # æ ‡å‡†åç§°
                "ODBC Driver 17 for SQL Server",  # æ—§ç‰ˆæœ¬
                "SQL Server",  # æ—§ç‰ˆé©±åŠ¨
                "SQL Server Native Client 11.0",  # å¦ä¸€ä¸ªå¯èƒ½çš„é©±åŠ¨
            ]
            
            last_error = None
            for driver_name in possible_drivers:
                try:
                    conn_str = (
                        f"DRIVER={{{driver_name}}};"
                        f"SERVER={DATABASE_CONFIG['server']},{DATABASE_CONFIG['port']};"
                        f"DATABASE={DATABASE_CONFIG['database']};"
                        f"UID={DATABASE_CONFIG['user']};"
                        f"PWD={DATABASE_CONFIG['password']};"
                        f"TrustServerCertificate=yes;"
                        f"Connection Timeout={DB_TIMEOUT};"
                        f"Command Timeout=60;"
                    )
                    conn = pyodbc.connect(conn_str, autocommit=True)
                    logger.info(f"æˆåŠŸä½¿ç”¨é©±åŠ¨è¿æ¥: {driver_name}")
                    return conn
                except Exception as e:
                    last_error = e
                    logger.debug(f"å°è¯•é©±åŠ¨ {driver_name} å¤±è´¥: {e}")
                    continue
            
            # æ‰€æœ‰é©±åŠ¨éƒ½å¤±è´¥ï¼ŒæŠ›å‡ºæœ€åä¸€ä¸ªé”™è¯¯
            logger.error(f"æ‰€æœ‰é©±åŠ¨å°è¯•å¤±è´¥ï¼Œæœ€åä¸€ä¸ªé”™è¯¯: {last_error}")
            raise last_error
        except Exception as e:
            logger.error(f"åˆ›å»ºæ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
            return None

    def _get_connection(self):
        """ä»è¿æ¥æ± è·å–è¿æ¥"""
        # æ›´æ–°æ´»åŠ¨æ—¶é—´ï¼ˆæ¯æ¬¡ä½¿ç”¨è¿æ¥æ—¶ï¼‰
        _update_activity_time()
        
        if not self.use_connection_pool:
            return self._create_connection()
        
        # å»¶è¿Ÿåˆå§‹åŒ–è¿æ¥æ± ï¼ˆåªåœ¨ç¬¬ä¸€æ¬¡ä½¿ç”¨æ—¶åˆå§‹åŒ–ï¼‰
        global _global_pool_initialized
        if self._lazy_init_pool and not _global_pool_initialized:
            self._init_connection_pool()
        
        with self._pool_lock:
            if self._connection_pool:
                conn = self._connection_pool.pop()
                _update_activity_time()  # è·å–è¿æ¥æ—¶æ›´æ–°æ´»åŠ¨æ—¶é—´
                return conn
            else:
                # è¿æ¥æ± ä¸ºç©ºï¼Œåˆ›å»ºæ–°è¿æ¥ï¼ˆæŒ‰éœ€æ‰©å±•ï¼‰
                conn = self._create_connection()
                _update_activity_time()  # åˆ›å»ºè¿æ¥æ—¶æ›´æ–°æ´»åŠ¨æ—¶é—´
                return conn

    def _return_connection(self, conn):
        """å½’è¿˜è¿æ¥åˆ°è¿æ¥æ± """
        # æ›´æ–°æ´»åŠ¨æ—¶é—´ï¼ˆå½’è¿˜è¿æ¥æ—¶ï¼‰
        _update_activity_time()
        
        if not self.use_connection_pool or not conn:
            return
        
        try:
            # æµ‹è¯•è¿æ¥æ˜¯å¦è¿˜æœ‰æ•ˆ
            conn.execute("SELECT 1")
            with self._pool_lock:
                self._connection_pool.append(conn)
                _update_activity_time()  # å½’è¿˜è¿æ¥æ—¶æ›´æ–°æ´»åŠ¨æ—¶é—´
        except:
            # è¿æ¥æ— æ•ˆï¼Œåˆ›å»ºæ–°è¿æ¥
            new_conn = self._create_connection()
            if new_conn:
                with self._pool_lock:
                    self._connection_pool.append(new_conn)
                    _update_activity_time()  # åˆ›å»ºæ–°è¿æ¥æ—¶æ›´æ–°æ´»åŠ¨æ—¶é—´

    def connect(self):
        if self.conn is not None:
            return self.conn
        self.conn = self._get_connection()
        return self.conn

    def close(self):
        if self.conn:
            self._return_connection(self.conn)
            self.conn = None
    
    def cleanup(self, force=False):
        """
        æ¸…ç†æ‰€æœ‰è¿æ¥æ± ä¸­çš„è¿æ¥
        
        Args:
            force: å¦‚æœä¸ºTrueï¼Œç«‹å³å…³é—­æ‰€æœ‰è¿æ¥ï¼›å¦‚æœä¸ºFalseï¼Œä¸å…³é—­ï¼ˆç”±ç©ºé—²è¶…æ—¶æœºåˆ¶ç®¡ç†ï¼‰
        """
        # å¦‚æœforce=Falseï¼Œä¸ç«‹å³å…³é—­è¿æ¥ï¼Œè®©ç©ºé—²è¶…æ—¶æœºåˆ¶ç®¡ç†
        if not force:
            # åªå…³é—­å½“å‰å®ä¾‹ä½¿ç”¨çš„è¿æ¥
            if self.conn:
                try:
                    self._return_connection(self.conn)
                except:
                    pass
                self.conn = None
            return
        
        # force=Trueæ—¶ï¼Œç«‹å³å…³é—­æ‰€æœ‰è¿æ¥
        with self._pool_lock:
            closed_count = 0
            while self._connection_pool:
                conn = self._connection_pool.pop()
                try:
                    conn.close()
                    closed_count += 1
                except Exception as e:
                    logger.warning(f"å…³é—­è¿æ¥æ—¶å‡ºé”™: {e}")
            
            if closed_count > 0:
                logger.info(f"å¼ºåˆ¶æ¸…ç†è¿æ¥æ± : å…³é—­äº† {closed_count} ä¸ªè¿æ¥")
            
            # é‡ç½®åˆå§‹åŒ–çŠ¶æ€
            global _global_pool_initialized
            _global_pool_initialized = False
        
        # åŒæ—¶å…³é—­å½“å‰ä½¿ç”¨çš„è¿æ¥
        if self.conn:
            try:
                self.conn.close()
            except:
                pass
            self.conn = None
        
        # é‡ç½®æœ€åæ´»åŠ¨æ—¶é—´
        global _last_activity_time
        _last_activity_time = None
    
    def __del__(self):
        """ææ„å‡½æ•°ï¼šç¡®ä¿å®ä¾‹é”€æ¯æ—¶å…³é—­æ‰€æœ‰è¿æ¥"""
        try:
            self.cleanup()
        except:
            # å¿½ç•¥ææ„å‡½æ•°ä¸­çš„é”™è¯¯ï¼Œé¿å…å½±å“ç¨‹åºæ­£å¸¸é€€å‡º
            pass

    def query(self, sql, params=None, max_retries=None):
        if max_retries is None:
            from config import MAX_RETRIES
            max_retries = MAX_RETRIES
        """ä¼˜åŒ–çš„æŸ¥è¯¢æ–¹æ³•ï¼Œæ”¯æŒè¿æ¥æ± """
        # æ›´æ–°æ´»åŠ¨æ—¶é—´ï¼ˆæ¯æ¬¡æŸ¥è¯¢æ—¶ï¼‰
        _update_activity_time()
        
        conn = None
        retry_count = 0
        
        while retry_count < max_retries:
            try:
                # è·å–è¿æ¥
                conn = self._get_connection()
                if not conn:
                    raise Exception("æ— æ³•è·å–æ•°æ®åº“è¿æ¥")
                
                # å‚æ•°ç±»å‹è½¬æ¢ï¼Œé¿å…numpyç±»å‹
                if params is not None:
                    def to_native(x):
                        if isinstance(x, (np.integer,)):
                            return int(x)
                        if isinstance(x, (np.floating,)):
                            return float(x)
                        if isinstance(x, (np.str_,)):
                            return str(x)
                        return x
                    params = [to_native(x) for x in params]
                
                start_time = time.time()
                df = pd.read_sql(sql, conn, params=params)
                query_time = time.time() - start_time
                
                # åªåœ¨æŸ¥è¯¢æ—¶é—´è¶…è¿‡10ç§’æ—¶è®°å½•åˆ°debugæ—¥å¿—ï¼ˆä¸æ˜¾ç¤ºåœ¨æ§åˆ¶å°ï¼Œé¿å…å¹²æ‰°è¿›åº¦æ¡ï¼‰
                if query_time > 10:
                    logger.debug(f"æŸ¥è¯¢è€—æ—¶è¾ƒé•¿: {query_time:.2f}ç§’")
                
                # æŸ¥è¯¢æˆåŠŸï¼Œæ›´æ–°æ´»åŠ¨æ—¶é—´
                _update_activity_time()
                
                return df
                
            except Exception as e:
                retry_count += 1
                logger.error(f"SQLæ‰§è¡Œå‡ºé”™ï¼Œç¬¬{retry_count}æ¬¡é‡è¯•: {e}")
                
                if retry_count >= max_retries:
                    raise
                else:
                    from config import RETRY_DELAY
                    time.sleep(RETRY_DELAY)
                    
            finally:
                if conn:
                    self._return_connection(conn)

    def get_stock_list(self, limit=None, market=None, max_days_ago=30, cutoff_date=None, use_cache=True):
        """
        è·å–è‚¡ç¥¨åˆ—è¡¨ï¼Œæ’é™¤åœç‰Œæˆ–é€€å¸‚è‚¡ç¥¨ï¼ˆé«˜æ€§èƒ½å¤šçº¿ç¨‹ç‰ˆæœ¬ï¼‰
        Args:
            limit: è‚¡ç¥¨æ•°é‡é™åˆ¶
            market: å¸‚åœºç­›é€‰ï¼ˆSZ/SSï¼‰ï¼ŒNoneè¡¨ç¤ºè·å–æ‰€æœ‰å¸‚åœºï¼ˆä¼šå¹¶è¡ŒæŸ¥è¯¢ï¼‰
            max_days_ago: æœ€å¤§å…è®¸çš„è¡Œæƒ…æ—¥æœŸæ»åå¤©æ•°ï¼Œè¶…è¿‡æ­¤å¤©æ•°çš„è‚¡ç¥¨å°†è¢«æ’é™¤ï¼ˆåœç‰Œ/é€€å¸‚ï¼‰
            cutoff_date: æ•°æ®æˆªæ­¢æ—¥æœŸï¼ŒNoneè¡¨ç¤ºä½¿ç”¨å½“å‰æ—¥æœŸå‡å»max_days_ago
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜ï¼ˆé»˜è®¤Trueï¼‰
        """
        import time
        from functools import lru_cache
        from datetime import datetime
        
        if limit is None:
            limit = STOCK_LIST_LIMIT
        elif limit == -1:  # ä½¿ç”¨-1è¡¨ç¤ºè·å–å…¨å¸‚åœº
            limit = 99999  # è®¾ç½®ä¸€ä¸ªå¾ˆå¤§çš„æ•°å­—æ¥è·å–å…¨å¸‚åœº
        
        # è·å–æˆªæ­¢æ—¥æœŸ
        if cutoff_date is None:
            current_date = pd.Timestamp.now().date()
            cutoff_date = current_date - pd.Timedelta(days=max_days_ago)
        else:
            # ä½¿ç”¨æŒ‡å®šçš„æˆªæ­¢æ—¥æœŸ
            cutoff_date = pd.Timestamp(cutoff_date).date()
        
        # ç¼“å­˜é”®ï¼ˆåŸºäºå‚æ•°ï¼‰
        cache_key = f"stock_list_{limit}_{market}_{max_days_ago}_{cutoff_date}"
        
        # ç®€å•çš„å†…å­˜ç¼“å­˜ï¼ˆå¯ä»¥æ‰©å±•åˆ°æ–‡ä»¶ç¼“å­˜ï¼‰
        if not hasattr(self, '_stock_list_cache'):
            self._stock_list_cache = {}
            self._stock_list_cache_time = {}
        
        # æ£€æŸ¥ç¼“å­˜ï¼ˆç¼“å­˜æœ‰æ•ˆæœŸ5åˆ†é’Ÿï¼‰
        if use_cache and cache_key in self._stock_list_cache:
            cache_time = self._stock_list_cache_time.get(cache_key, 0)
            if time.time() - cache_time < 300:  # 5åˆ†é’Ÿç¼“å­˜
                logger.debug(f"[get_stock_list] ä½¿ç”¨ç¼“å­˜: {len(self._stock_list_cache[cache_key])} åªè‚¡ç¥¨")
                return self._stock_list_cache[cache_key]
        
        start_time = time.time()
        
        # å¦‚æœæŒ‡å®šäº†å¸‚åœºï¼Œç›´æ¥æŸ¥è¯¢
        if market:
            stock_info = self._get_stock_list_single_market(limit, market, cutoff_date)
        else:
            # æœªæŒ‡å®šå¸‚åœºï¼Œå¹¶è¡ŒæŸ¥è¯¢SZå’ŒSSä¸¤ä¸ªå¸‚åœº
            stock_info = self._get_stock_list_parallel(limit, cutoff_date)
        
        elapsed = time.time() - start_time
        
        # ä¿å­˜åˆ°ç¼“å­˜
        if use_cache:
            self._stock_list_cache[cache_key] = stock_info
            self._stock_list_cache_time[cache_key] = time.time()
        
        logger.info(f"[get_stock_list] è·å–å®Œæˆ: {len(stock_info)} åªè‚¡ç¥¨ï¼Œè€—æ—¶: {elapsed:.2f}ç§’")
        
        return stock_info
    
    def _get_stock_list_single_market(self, limit, market, cutoff_date):
        """è·å–å•ä¸ªå¸‚åœºçš„è‚¡ç¥¨åˆ—è¡¨"""
        # å…ˆç»Ÿè®¡å„ä¸ªæ¡ä»¶è¿‡æ»¤çš„è‚¡ç¥¨æ•°é‡ï¼ˆç”¨äºè°ƒè¯•ï¼‰
        market_condition = ""
        if market.upper() == "SZ":
            market_condition = " AND SecuMarket=90"
        elif market.upper() == "SS":
            market_condition = " AND SecuMarket=83"
        
        # ç»Ÿè®¡1: è¯¥å¸‚åœºçš„æ‰€æœ‰è‚¡ç¥¨ï¼ˆåœ¨SecuMainä¸­ï¼‰
        stats_sql1 = f"""
        SELECT COUNT(*) as cnt
        FROM {TABLE_CONFIG['stock_info_table']} s
        WHERE SecuCategory = 1 {market_condition}
        """
        try:
            df_stats1 = self.query(stats_sql1)
            total_in_market = df_stats1.iloc[0]['cnt'] if not df_stats1.empty else 0
        except:
            total_in_market = 0
        
        # ç»Ÿè®¡2: åº”ç”¨STOCK_FILTERåçš„æ•°é‡
        stats_sql2 = f"""
        SELECT COUNT(*) as cnt
        FROM {TABLE_CONFIG['stock_info_table']} s
        WHERE {STOCK_FILTER} {market_condition}
        """
        try:
            df_stats2 = self.query(stats_sql2)
            after_stock_filter = df_stats2.iloc[0]['cnt'] if not df_stats2.empty else 0
        except:
            after_stock_filter = 0
        
        # ç»Ÿè®¡3: æ’é™¤Xå¼€å¤´åçš„æ•°é‡
        stats_sql3 = f"""
        SELECT COUNT(*) as cnt
        FROM {TABLE_CONFIG['stock_info_table']} s
        WHERE {STOCK_FILTER}
        AND s.{TABLE_CONFIG['code_field']} NOT LIKE 'X%' {market_condition}
        """
        try:
            df_stats3 = self.query(stats_sql3)
            after_x_filter = df_stats3.iloc[0]['cnt'] if not df_stats3.empty else 0
        except:
            after_x_filter = 0
        
        # ç»Ÿè®¡4: æœ‰äº¤æ˜“è®°å½•çš„è‚¡ç¥¨æ•°é‡
        stats_sql4 = f"""
        SELECT COUNT(DISTINCT s.{TABLE_CONFIG['inner_code_field']}) as cnt
        FROM {TABLE_CONFIG['stock_info_table']} s
        INNER JOIN (
            SELECT q.{TABLE_CONFIG['inner_code_field']}, MAX(q.{TABLE_CONFIG['date_field']}) as latest_trading_day
            FROM {TABLE_CONFIG['daily_quote_table']} q
            GROUP BY q.{TABLE_CONFIG['inner_code_field']}
        ) latest ON s.{TABLE_CONFIG['inner_code_field']} = latest.{TABLE_CONFIG['inner_code_field']}
        WHERE {STOCK_FILTER}
        AND s.{TABLE_CONFIG['code_field']} NOT LIKE 'X%' {market_condition}
        """
        try:
            df_stats4 = self.query(stats_sql4)
            with_trading_data = df_stats4.iloc[0]['cnt'] if not df_stats4.empty else 0
        except:
            with_trading_data = 0
        
        # ç»Ÿè®¡5: æ´»è·ƒåº¦ç­›é€‰åçš„æ•°é‡ï¼ˆæœ€æ–°äº¤æ˜“æ—¥ >= cutoff_dateï¼‰
        stats_sql5 = f"""
        SELECT COUNT(DISTINCT s.{TABLE_CONFIG['inner_code_field']}) as cnt
        FROM {TABLE_CONFIG['stock_info_table']} s
        INNER JOIN (
            SELECT q.{TABLE_CONFIG['inner_code_field']}, MAX(q.{TABLE_CONFIG['date_field']}) as latest_trading_day
            FROM {TABLE_CONFIG['daily_quote_table']} q
            GROUP BY q.{TABLE_CONFIG['inner_code_field']}
        ) latest ON s.{TABLE_CONFIG['inner_code_field']} = latest.{TABLE_CONFIG['inner_code_field']}
        WHERE {STOCK_FILTER}
        AND latest.latest_trading_day >= '{cutoff_date}'
        AND s.{TABLE_CONFIG['code_field']} NOT LIKE 'X%' {market_condition}
        """
        try:
            df_stats5 = self.query(stats_sql5)
            after_activity_filter = df_stats5.iloc[0]['cnt'] if not df_stats5.empty else 0
        except:
            after_activity_filter = 0
        
        print(f"  ğŸ“Š [{market}å¸‚åœº] è‚¡ç¥¨ç­›é€‰ç»Ÿè®¡:")
        print(f"     å¸‚åœºæ€»è‚¡ç¥¨æ•°: {total_in_market}")
        print(f"     STOCK_FILTERå: {after_stock_filter} (æ’é™¤: {total_in_market - after_stock_filter})")
        print(f"     æ’é™¤Xå¼€å¤´å: {after_x_filter} (æ’é™¤: {after_stock_filter - after_x_filter})")
        print(f"     æœ‰äº¤æ˜“è®°å½•: {with_trading_data} (æ’é™¤: {after_x_filter - with_trading_data})")
        print(f"     æ´»è·ƒåº¦ç­›é€‰å(>={cutoff_date}): {after_activity_filter} (æ’é™¤: {with_trading_data - after_activity_filter})")
        
        # å®é™…æŸ¥è¯¢
        sql = f"""
        SELECT TOP {limit} s.{TABLE_CONFIG['code_field']}, s.{TABLE_CONFIG['inner_code_field']}, s.SecuMarket
        FROM {TABLE_CONFIG['stock_info_table']} s
        INNER JOIN (
            SELECT q.{TABLE_CONFIG['inner_code_field']}, MAX(q.{TABLE_CONFIG['date_field']}) as latest_trading_day
            FROM {TABLE_CONFIG['daily_quote_table']} q
            GROUP BY q.{TABLE_CONFIG['inner_code_field']}
        ) latest ON s.{TABLE_CONFIG['inner_code_field']} = latest.{TABLE_CONFIG['inner_code_field']}
        WHERE {STOCK_FILTER}
        AND latest.latest_trading_day >= '{cutoff_date}'
        AND s.{TABLE_CONFIG['code_field']} NOT LIKE 'X%'
        """
        
        if market.upper() == "SZ":
            sql += " AND SecuMarket=90"
        elif market.upper() == "SS":
            sql += " AND SecuMarket=83"
        
        sql += f" ORDER BY s.{TABLE_CONFIG['code_field']}"
        
        logger.debug(f"[get_stock_list_single_market SQL]: {sql}")
        
        # æ·»åŠ æŸ¥è¯¢æ—¶é—´ç»Ÿè®¡
        import time
        query_start = time.time()
        df = self.query(sql)
        query_time = time.time() - query_start
        
        result_count = len(df)
        print(f"     å®é™…è·å–æ•°é‡: {result_count} (limit={limit})")
        
        if query_time > 10:  # å¦‚æœæŸ¥è¯¢è¶…è¿‡10ç§’ï¼Œè®°å½•è­¦å‘Š
            logger.warning(f"[get_stock_list_single_market] æŸ¥è¯¢è€—æ—¶: {query_time:.2f}ç§’")
        
        return self._format_stock_info(df)
    
    def _get_stock_list_parallel(self, limit, cutoff_date):
        """å¹¶è¡Œè·å–SZå’ŒSSä¸¤ä¸ªå¸‚åœºçš„è‚¡ç¥¨åˆ—è¡¨"""
        from concurrent.futures import ThreadPoolExecutor, as_completed
        
        # è®¡ç®—æ¯ä¸ªå¸‚åœºçš„limitï¼ˆå¦‚æœlimitä¸æ˜¯-1ï¼Œåˆ™å¹³å‡åˆ†é…ï¼‰
        if limit == 99999:  # å…¨å¸‚åœº
            sz_limit = 99999
            ss_limit = 99999
        else:
            # å¹³å‡åˆ†é…ï¼Œä½†æ¯ä¸ªå¸‚åœºè‡³å°‘è·å–limitåªï¼ˆå› ä¸ºå¯èƒ½ä¸€ä¸ªå¸‚åœºè‚¡ç¥¨å°‘ï¼‰
            sz_limit = limit
            ss_limit = limit
        
        all_stock_info = []
        
        def query_market(market_name, market_limit):
            """æŸ¥è¯¢å•ä¸ªå¸‚åœº"""
            try:
                return self._get_stock_list_single_market(market_limit, market_name, cutoff_date)
            except Exception as e:
                logger.error(f"æŸ¥è¯¢{market_name}å¸‚åœºå¤±è´¥: {e}")
                return []
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡ŒæŸ¥è¯¢ä¸¤ä¸ªå¸‚åœº
        with ThreadPoolExecutor(max_workers=2) as executor:
            futures = {
                executor.submit(query_market, "SZ", sz_limit): "SZ",
                executor.submit(query_market, "SS", ss_limit): "SS"
            }
            
            for future in as_completed(futures):
                market_name = futures[future]
                try:
                    market_stocks = future.result()
                    all_stock_info.extend(market_stocks)
                    logger.debug(f"[get_stock_list_parallel] {market_name}å¸‚åœº: {len(market_stocks)} åªè‚¡ç¥¨")
                except Exception as e:
                    logger.error(f"[get_stock_list_parallel] {market_name}å¸‚åœºæŸ¥è¯¢å¤±è´¥: {e}")
        
        # æŒ‰ä»£ç æ’åºç¡®ä¿ç¡®å®šæ€§å’Œå¸‚åœºå…¬å¹³æ€§ï¼Œå†æˆªæ–­
        all_stock_info.sort(key=lambda x: x.get('code', ''))
        before_limit = len(all_stock_info)
        if limit != 99999 and len(all_stock_info) > limit:
            all_stock_info = all_stock_info[:limit]
            after_limit = len(all_stock_info)
            print(f"  ğŸ“Š [æ±‡æ€»] åº”ç”¨limité™åˆ¶: {before_limit} -> {after_limit} (æ’é™¤: {before_limit - after_limit})")
        else:
            print(f"  ğŸ“Š [æ±‡æ€»] æœªåº”ç”¨limité™åˆ¶ï¼Œæ€»æ•°: {before_limit}")
        
        return all_stock_info
    
    def _format_stock_info(self, df):
        """æ ¼å¼åŒ–è‚¡ç¥¨ä¿¡æ¯"""
        if df.empty:
            return []
        
        stock_info = []
        for _, row in df.iterrows():
            code = str(row[TABLE_CONFIG['code_field']]).zfill(6)
            market = row['SecuMarket']
            
            # æ ¹æ®SecuMarketå€¼ç¡®å®šäº¤æ˜“æ‰€æ ‡è¯†
            if market == 90:  # æ·±åœ³
                exchange_suffix = '.SZ'
            elif market == 83:  # ä¸Šæµ·
                exchange_suffix = '.SH'
            else:
                exchange_suffix = '.UNKNOWN'  # æœªçŸ¥äº¤æ˜“æ‰€
            
            stock_info.append({
                'code': code,
                'exchange': exchange_suffix,
                'full_code': f"{code}{exchange_suffix}"
            })
        
        return stock_info

    def get_latest_trading_date(self):
        """
        è·å–æ•°æ®åº“ä¸­çš„æœ€æ–°äº¤æ˜“æ—¥
        Returns:
            date: æœ€æ–°äº¤æ˜“æ—¥ï¼Œå¦‚æœæŸ¥è¯¢å¤±è´¥åˆ™è¿”å›å½“å‰æ—¥æœŸ
        """
        try:
            sql = f"""
            SELECT MAX({TABLE_CONFIG['date_field']}) as latest_date
            FROM {TABLE_CONFIG['daily_quote_table']}
            """
            df = self.query(sql)
            if not df.empty and df.iloc[0]['latest_date'] is not None:
                latest_date = pd.Timestamp(df.iloc[0]['latest_date']).date()
                logger.info(f"æ•°æ®åº“æœ€æ–°äº¤æ˜“æ—¥: {latest_date}")
                return latest_date
            else:
                # å¦‚æœæŸ¥è¯¢å¤±è´¥ï¼Œè¿”å›å½“å‰æ—¥æœŸ
                current_date = pd.Timestamp.now().date()
                logger.warning(f"æ— æ³•è·å–æœ€æ–°äº¤æ˜“æ—¥ï¼Œä½¿ç”¨å½“å‰æ—¥æœŸ: {current_date}")
                return current_date
        except Exception as e:
            logger.error(f"è·å–æœ€æ–°äº¤æ˜“æ—¥å¤±è´¥: {e}")
            # å¦‚æœæŸ¥è¯¢å¤±è´¥ï¼Œè¿”å›å½“å‰æ—¥æœŸ
            current_date = pd.Timestamp.now().date()
            logger.warning(f"ä½¿ç”¨å½“å‰æ—¥æœŸä½œä¸ºé»˜è®¤å€¼: {current_date}")
            return current_date

    def batch_get_stock_data(self, stock_codes, days=365):
        """æ‰¹é‡è·å–è‚¡ç¥¨æ•°æ®ï¼Œä½¿ç”¨å¤šçº¿ç¨‹å¹¶è¡Œå¤„ç†"""
        if not stock_codes:
            return {}
        
        logger.info(f"å¼€å§‹æ‰¹é‡è·å– {len(stock_codes)} åªè‚¡ç¥¨çš„è¡Œæƒ…æ•°æ®...")
        start_time = time.time()
        
        # åˆ†æ‰¹å¤„ç†ï¼Œé¿å…SQLè¯­å¥è¿‡é•¿ï¼ˆä¼˜åŒ–æ‰¹å¤„ç†å¤§å°æé«˜ç½‘ç»œååé‡ï¼‰
        batch_size = 500  # æ¯æ‰¹å¤„ç†500åªè‚¡ç¥¨ï¼Œå¤§å¹…æé«˜ç½‘ç»œåˆ©ç”¨ç‡
        batches = []
        
        for i in range(0, len(stock_codes), batch_size):
            batch_codes_subset = stock_codes[i:i + batch_size]
            batches.append((i//batch_size + 1, batch_codes_subset))
        
        logger.info(f"å°† {len(stock_codes)} åªè‚¡ç¥¨åˆ†ä¸º {len(batches)} æ‰¹å¤„ç†ï¼Œæ¯æ‰¹ {batch_size} åª")
        
        # ä½¿ç”¨å¤šçº¿ç¨‹å¹¶è¡Œå¤„ç†æ‰¹æ¬¡
        all_results = {}
        processed_count = 0
        
        # åˆ›å»ºæ€»ä½“è¿›åº¦æ¡
        total_pbar = tqdm(total=len(stock_codes), desc='æ•°æ®è·å–æ€»è¿›åº¦', ncols=100, unit='åª', position=0)
        
        def process_batch(batch_info):
            """å¤„ç†å•ä¸ªæ‰¹æ¬¡çš„è‚¡ç¥¨æ•°æ®"""
            batch_id, batch_codes_subset = batch_info
            thread_name = threading.current_thread().name
            batch_results = {}
            
            try:
                logger.info(f"[çº¿ç¨‹ {thread_name}] å¼€å§‹å¤„ç†æ‰¹æ¬¡ {batch_id}ï¼ŒåŒ…å« {len(batch_codes_subset)} åªè‚¡ç¥¨")
                
                # æ„å»ºæ‰¹é‡æŸ¥è¯¢SQL
                codes_str = ','.join([f"'{code}'" for code in batch_codes_subset])
                sql = f"""
                SELECT 
                    s.{TABLE_CONFIG['code_field']},
                    q.{TABLE_CONFIG['date_field']} as [Date],
                    q.{TABLE_CONFIG['open_field']} as [Open],
                    q.{TABLE_CONFIG['high_field']} as [High],
                    q.{TABLE_CONFIG['low_field']} as [Low],
                    q.{TABLE_CONFIG['close_field']} as [Close],
                    q.{TABLE_CONFIG['volume_field']} as [Volume],
                    q.{TABLE_CONFIG['turnover_rate_field']} as [TurnoverRate],
                    q.{TABLE_CONFIG['negotiable_mv_field']} as [NegotiableMV],
                    p.SurgedLimit as SurgedLimit,
                    p.DeclineLimit as DeclineLimit
                FROM {TABLE_CONFIG['stock_info_table']} s
                INNER JOIN {TABLE_CONFIG['daily_quote_table']} q 
                    ON s.{TABLE_CONFIG['inner_code_field']} = q.{TABLE_CONFIG['inner_code_field']}
                LEFT JOIN QT_PerformanceData p 
                    ON q.{TABLE_CONFIG['inner_code_field']} = p.InnerCode 
                    AND q.{TABLE_CONFIG['date_field']} = p.TradingDay
                WHERE s.{TABLE_CONFIG['code_field']} IN ({codes_str})
                AND {STOCK_FILTER}
                ORDER BY s.{TABLE_CONFIG['code_field']}, q.{TABLE_CONFIG['date_field']} DESC
                """
                
                # è°ƒè¯•ï¼šæ‰“å°SQLï¼ˆä»…ç¬¬ä¸€ä¸ªæ‰¹æ¬¡ï¼‰
                if batch_id == 1:
                    print(f"\n{'='*80}")
                    logger.debug(f"[è°ƒè¯•] batch_get_stock_data SQLæŸ¥è¯¢ï¼ˆæ‰¹æ¬¡{batch_id}ï¼‰")
                    logger.debug(f"è‚¡ç¥¨ä»£ç ç¤ºä¾‹: {batch_codes_subset[:5]}... (å…±{len(batch_codes_subset)}åª)")
                    logger.debug(f"å®é™…æ‰§è¡Œçš„SQL:\n{sql}")
                
                df = self.query(sql)
                
                # è°ƒè¯•ï¼šæ£€æŸ¥æŸ¥è¯¢ç»“æœçš„åˆ—ï¼ˆä»…ç¬¬ä¸€ä¸ªæ‰¹æ¬¡ï¼‰
                if batch_id == 1 and not df.empty:
                    logger.debug(f"[è°ƒè¯•] æŸ¥è¯¢ç»“æœç»Ÿè®¡:")
                    logger.debug(f"  åˆ—å: {list(df.columns)}")
                    logger.debug(f"  è¡Œæ•°: {len(df)}")
                    
                    if 'SurgedLimit' in df.columns:
                        surged_limit_count = df['SurgedLimit'].notna().sum()
                        surged_limit_1_count = (df['SurgedLimit'] == 1).sum()
                        logger.debug(f"  SurgedLimitå­—æ®µç»Ÿè®¡: éç©ºæ•°é‡={surged_limit_count}/{len(df)}, å€¼ä¸º1æ•°é‡={surged_limit_1_count}")
                    else:
                        logger.warning(f"æŸ¥è¯¢ç»“æœä¸­ç¼ºå°‘SurgedLimitåˆ—ï¼å¯ç”¨åˆ—: {list(df.columns)}")
                
                if df.empty:
                    logger.warning(f"[çº¿ç¨‹ {thread_name}] æ‰¹æ¬¡ {batch_id} æŸ¥è¯¢ç»“æœä¸ºç©º")
                    return batch_results
                
                # æŒ‰è‚¡ç¥¨ä»£ç åˆ†ç»„
                for code in batch_codes_subset:
                    stock_data = df[df[TABLE_CONFIG['code_field']] == code].copy()
                    if not stock_data.empty:
                        stock_data = stock_data.sort_values('Date')
                        stock_data = stock_data.set_index('Date')
                        # åªä¿ç•™æœ€è¿‘dayså¤©çš„æ•°æ®ï¼ˆä¸å•ç‹¬è·å–ä¿æŒä¸€è‡´ï¼‰
                        stock_data = stock_data.tail(days)
                        
                        # æ•°æ®ç±»å‹è½¬æ¢
                        numeric_columns = ['Open', 'High', 'Low', 'Close', 'Volume', 'TurnoverRate', 'NegotiableMV']
                        for col in numeric_columns:
                            if col in stock_data.columns:
                                stock_data[col] = pd.to_numeric(stock_data[col], errors='coerce')
                        
                        # SurgedLimitå’ŒDeclineLimitè½¬æ¢ä¸ºæ•´æ•°ï¼ˆ0æˆ–1ï¼‰ï¼Œä¿ç•™NULLå€¼
                        # é‡è¦ï¼šä¿ç•™NULLå€¼ï¼Œä¸è¦å¡«å……ä¸º0ï¼Œè¿™æ ·å¯ä»¥åŒºåˆ†"æ•°æ®ç¼ºå¤±"å’Œ"éæ¶¨åœ"
                        if 'SurgedLimit' in stock_data.columns:
                            stock_data['SurgedLimit'] = pd.to_numeric(stock_data['SurgedLimit'], errors='coerce')
                            # åªå¯¹æœ‰é™å€¼ï¼ˆéNaNã€éinfï¼‰è½¬æ¢ä¸ºæ•´æ•°ï¼Œä¿ç•™NaNå’Œinfä¸ºNaN
                            finite_mask = np.isfinite(stock_data['SurgedLimit'])
                            stock_data.loc[finite_mask, 'SurgedLimit'] = stock_data.loc[finite_mask, 'SurgedLimit'].astype(int)
                        else:
                            # å¦‚æœåˆ—ä¸å­˜åœ¨ï¼Œæ·»åŠ NULLå€¼ï¼ˆè¡¨ç¤ºç¼ºå¤±ï¼‰
                            stock_data['SurgedLimit'] = pd.NA
                        
                        if 'DeclineLimit' in stock_data.columns:
                            stock_data['DeclineLimit'] = pd.to_numeric(stock_data['DeclineLimit'], errors='coerce')
                            # åªå¯¹æœ‰é™å€¼ï¼ˆéNaNã€éinfï¼‰è½¬æ¢ä¸ºæ•´æ•°ï¼Œä¿ç•™NaNå’Œinfä¸ºNaN
                            finite_mask = np.isfinite(stock_data['DeclineLimit'])
                            stock_data.loc[finite_mask, 'DeclineLimit'] = stock_data.loc[finite_mask, 'DeclineLimit'].astype(int)
                        else:
                            stock_data['DeclineLimit'] = pd.NA
                        
                        # åªä¿ç•™æœ‰å®Œæ•´æ•°æ®çš„è¡Œ
                        stock_data = stock_data.dropna(subset=['Close', 'Volume'])
                        
                        if not stock_data.empty:
                            batch_results[code] = stock_data
                
                logger.info(f"[çº¿ç¨‹ {thread_name}] æ‰¹æ¬¡ {batch_id} å®Œæˆï¼Œè·å–åˆ° {len(batch_results)} åªè‚¡ç¥¨æ•°æ®")
                return batch_results
                
            except Exception as e:
                logger.error(f"[çº¿ç¨‹ {thread_name}] æ‰¹æ¬¡ {batch_id} å¤„ç†å¤±è´¥: {e}")
                return batch_results
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†
        from config import MAX_WORKERS, MAX_CONCURRENT_BATCHES
        max_workers = min(MAX_WORKERS, len(batches), MAX_CONCURRENT_BATCHES)  # é™åˆ¶æœ€å¤§å¹¶å‘æ•°
        logger.info(f"ä½¿ç”¨ {max_workers} ä¸ªçº¿ç¨‹å¹¶è¡Œå¤„ç†æ‰¹æ¬¡ï¼ˆé™åˆ¶æœ€å¤§å¹¶å‘æ•°ï¼‰")
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰æ‰¹æ¬¡ä»»åŠ¡
            future_to_batch = {executor.submit(process_batch, batch_info): batch_info for batch_info in batches}
            
            # å¤„ç†å®Œæˆçš„ä»»åŠ¡
            for future in concurrent.futures.as_completed(future_to_batch):
                batch_info = future_to_batch[future]
                
                try:
                    batch_results = future.result()
                    all_results.update(batch_results)
                    processed_count += len(batch_results)
                    
                    # æ›´æ–°è¿›åº¦æ¡
                    total_pbar.update(len(batch_results))
                    
                    # æ˜¾ç¤ºè¯¦ç»†è¿›åº¦ä¿¡æ¯
                    elapsed_time = time.time() - start_time
                    if processed_count > 0:
                        avg_time_per_stock = elapsed_time / processed_count
                        remaining_stocks = len(stock_codes) - processed_count
                        estimated_remaining_time = remaining_stocks * avg_time_per_stock
                        
                        total_pbar.set_postfix({
                            'å·²è·å–': f'{processed_count}/{len(stock_codes)}',
                            'è¿›åº¦': f'{processed_count/len(stock_codes)*100:.1f}%',
                            'è€—æ—¶': f'{elapsed_time:.1f}s',
                            'å¹³å‡': f'{avg_time_per_stock:.3f}s/åª',
                            'å‰©ä½™': f'{estimated_remaining_time:.1f}s',
                            'æœ¬æ‰¹': f'{len(batch_results)}åª'
                        })
                    
                except Exception as e:
                    logger.error(f"æ‰¹æ¬¡ {batch_info[0]} å¤„ç†å¤±è´¥: {e}")
                    total_pbar.update(len(batch_info[1]))
        
        total_pbar.close()
        
        total_time = time.time() - start_time
        logger.info(f"æ‰¹é‡è·å–å®Œæˆï¼Œæ€»è€—æ—¶: {total_time:.1f}ç§’")
        logger.info(f"æˆåŠŸè·å– {len(all_results)} åªè‚¡ç¥¨çš„æ•°æ®")
        logger.info(f"å¹³å‡æ¯åªè‚¡ç¥¨è·å–æ—¶é—´: {total_time/len(stock_codes):.3f}ç§’")
        
        return all_results

    def get_capital_flow_data(self, secu_code: str, trading_date: date) -> Optional[pd.DataFrame]:
        """
        è·å–è‚¡ç¥¨æŒ‡å®šæ—¥æœŸçš„èµ„é‡‘æµå‘æ•°æ®ï¼ˆå•åªè‚¡ç¥¨ï¼‰
        
        å‚æ•°:
        - secu_code: è‚¡ç¥¨ä»£ç 
        - trading_date: äº¤æ˜“æ—¥æœŸ
        
        è¿”å›:
        - DataFrameåŒ…å«èµ„é‡‘æµå‘æ•°æ®ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è¿”å›None
        """
        try:
            # å…ˆæŸ¥InnerCode
            sql_inner = f"""
            SELECT {TABLE_CONFIG['inner_code_field']} 
            FROM {TABLE_CONFIG['stock_info_table']} 
            WHERE {TABLE_CONFIG['code_field']} = ? AND {STOCK_FILTER}
            """
            df_inner = self.query(sql_inner, params=[secu_code])
            if df_inner.empty:
                return None
            
            inner_code = df_inner[TABLE_CONFIG['inner_code_field']].iloc[0]
            
            # æŸ¥è¯¢èµ„é‡‘æµå‘æ•°æ®
            sql = """
            SELECT 
                ValueRange,
                BuyValue as æµå…¥é‡‘é¢,
                SellValue as æµå‡ºé‡‘é¢,
                BuyVolume as æµå…¥é‡,
                SellVolume as æµå‡ºé‡
            FROM QT_TradingCapitalFlow
            WHERE InnerCode = ? 
                AND TradingDate = ?
                AND QuoteType = 1
            ORDER BY ValueRange
            """
            
            df = self.query(sql, params=[inner_code, trading_date])
            
            if df.empty:
                return None
            
            return df
            
        except Exception as e:
            logger.debug(f"è·å–è‚¡ç¥¨ {secu_code} åœ¨ {trading_date} çš„èµ„é‡‘æµå‘æ•°æ®å¤±è´¥: {e}")
            return None
    
    def batch_get_capital_flow_data(self, stock_date_pairs: List[tuple]) -> Dict[tuple, pd.DataFrame]:
        """
        æ‰¹é‡è·å–å¤šåªè‚¡ç¥¨åœ¨å¤šä¸ªæ—¥æœŸçš„èµ„é‡‘æµå‘æ•°æ®
        
        å‚æ•°:
        - stock_date_pairs: [(è‚¡ç¥¨ä»£ç , äº¤æ˜“æ—¥æœŸ), ...] åˆ—è¡¨
        
        è¿”å›:
        - Dict: {(è‚¡ç¥¨ä»£ç , äº¤æ˜“æ—¥æœŸ): DataFrame, ...}
        """
        if not stock_date_pairs:
            return {}
        
        try:
            # è·å–æ‰€æœ‰è‚¡ç¥¨çš„InnerCode
            stock_codes = list(set([code for code, _ in stock_date_pairs]))
            codes_str = ','.join([f"'{code}'" for code in stock_codes])
            
            sql_inner = f"""
            SELECT {TABLE_CONFIG['code_field']}, {TABLE_CONFIG['inner_code_field']}
            FROM {TABLE_CONFIG['stock_info_table']}
            WHERE {TABLE_CONFIG['code_field']} IN ({codes_str})
            AND {STOCK_FILTER}
            """
            
            df_inner = self.query(sql_inner)
            if df_inner.empty:
                return {}
            
            code_to_inner = dict(zip(
                df_inner[TABLE_CONFIG['code_field']].astype(str),
                df_inner[TABLE_CONFIG['inner_code_field']]
            ))
            
            # æ„å»ºæ‰¹é‡æŸ¥è¯¢SQL
            # ä½¿ç”¨INNER JOINæ‰¹é‡æŸ¥è¯¢
            inner_codes = list(code_to_inner.values())
            inner_codes_str = ','.join([str(ic) for ic in inner_codes])
            
            # è·å–æ‰€æœ‰æ—¥æœŸ
            dates = list(set([d for _, d in stock_date_pairs]))
            dates_str = ','.join([f"'{d}'" for d in dates])
            
            sql = f"""
            SELECT 
                s.{TABLE_CONFIG['code_field']} as SecuCode,
                cf.TradingDate,
                cf.ValueRange,
                cf.BuyValue as æµå…¥é‡‘é¢,
                cf.SellValue as æµå‡ºé‡‘é¢,
                cf.BuyVolume as æµå…¥é‡,
                cf.SellVolume as æµå‡ºé‡
            FROM QT_TradingCapitalFlow cf
            INNER JOIN {TABLE_CONFIG['stock_info_table']} s
                ON cf.InnerCode = s.{TABLE_CONFIG['inner_code_field']}
            WHERE cf.InnerCode IN ({inner_codes_str})
                AND cf.TradingDate IN ({dates_str})
                AND cf.QuoteType = 1
            ORDER BY s.{TABLE_CONFIG['code_field']}, cf.TradingDate, cf.ValueRange
            """
            
            df = self.query(sql)
            
            if df.empty:
                return {}
            
            # æŒ‰(è‚¡ç¥¨ä»£ç , äº¤æ˜“æ—¥æœŸ)åˆ†ç»„
            result = {}
            for (code, date), group_df in df.groupby(['SecuCode', 'TradingDate']):
                # ç¡®ä¿æ—¥æœŸç±»å‹ä¸€è‡´
                if isinstance(date, pd.Timestamp):
                    date_val = date.date()
                else:
                    date_val = date
                
                code_str = str(code).zfill(6)
                result[(code_str, date_val)] = group_df[['ValueRange', 'æµå…¥é‡‘é¢', 'æµå‡ºé‡‘é¢', 'æµå…¥é‡', 'æµå‡ºé‡']].copy()
            
            return result
            
        except Exception as e:
            logger.debug(f"æ‰¹é‡è·å–èµ„é‡‘æµå‘æ•°æ®å¤±è´¥: {e}")
            return {}
    
    def get_stock_data(self, secu_code, days=365):
        # å…ˆæŸ¥InnerCode
        sql_inner = f"""
        SELECT {TABLE_CONFIG['inner_code_field']} FROM {TABLE_CONFIG['stock_info_table']} WHERE {TABLE_CONFIG['code_field']} = ? AND {STOCK_FILTER}
        """
        df_inner = self.query(sql_inner, params=[secu_code])
        if df_inner.empty:
            return pd.DataFrame()
        inner_code = df_inner[TABLE_CONFIG['inner_code_field']].iloc[0]
        
        # åŒæ—¶è·å–è¡Œæƒ…æ•°æ®å’Œæ¶¨åœè·Œåœæ•°æ®
        sql = f"""
        SELECT TOP {days}
            q.{TABLE_CONFIG['date_field']} as [Date],
            q.{TABLE_CONFIG['open_field']} as [Open],
            q.{TABLE_CONFIG['high_field']} as [High],
            q.{TABLE_CONFIG['low_field']} as [Low],
            q.{TABLE_CONFIG['close_field']} as [Close],
            q.{TABLE_CONFIG['volume_field']} as [Volume],
            q.{TABLE_CONFIG['turnover_rate_field']} as [TurnoverRate],
            q.{TABLE_CONFIG['negotiable_mv_field']} as [NegotiableMV],
            p.SurgedLimit as SurgedLimit,
            p.DeclineLimit as DeclineLimit
        FROM {TABLE_CONFIG['daily_quote_table']} q
        LEFT JOIN QT_PerformanceData p 
            ON q.{TABLE_CONFIG['inner_code_field']} = p.InnerCode 
            AND q.{TABLE_CONFIG['date_field']} = p.TradingDay
        WHERE q.{TABLE_CONFIG['inner_code_field']} = ?
        ORDER BY q.{TABLE_CONFIG['date_field']} DESC
        """
        df = self.query(sql, params=[inner_code])
        if not df.empty:
            df = df.sort_values('Date')
            df = df.set_index('Date')
            numeric_columns = ['Open', 'High', 'Low', 'Close', 'Volume', 'TurnoverRate', 'NegotiableMV']
            for col in numeric_columns:
                if col in df.columns:
                    df[col] = pd.to_numeric(df[col], errors='coerce')
            # SurgedLimitå’ŒDeclineLimitè½¬æ¢ä¸ºæ•´æ•°ï¼ˆ0æˆ–1ï¼‰ï¼Œä¿ç•™NULLå€¼
            # é‡è¦ï¼šåªå¯¹æœ‰é™å€¼ï¼ˆéNaNã€éinfï¼‰è½¬æ¢ä¸ºæ•´æ•°ï¼Œä¿ç•™ç¼ºå¤±ä¸ºNaN
            if 'SurgedLimit' in df.columns:
                df['SurgedLimit'] = pd.to_numeric(df['SurgedLimit'], errors='coerce')
                # åªå¯¹æœ‰é™å€¼è¿›è¡Œæ•´æ•°è½¬æ¢ï¼Œä¿ç•™ NaN å’Œ inf ä¸º NaN
                finite_mask = np.isfinite(df['SurgedLimit'])
                df.loc[finite_mask, 'SurgedLimit'] = df.loc[finite_mask, 'SurgedLimit'].astype(int)
            if 'DeclineLimit' in df.columns:
                df['DeclineLimit'] = pd.to_numeric(df['DeclineLimit'], errors='coerce')
                # åªå¯¹æœ‰é™å€¼è¿›è¡Œæ•´æ•°è½¬æ¢ï¼Œä¿ç•™ NaN å’Œ inf ä¸º NaN
                finite_mask = np.isfinite(df['DeclineLimit'])
                df.loc[finite_mask, 'DeclineLimit'] = df.loc[finite_mask, 'DeclineLimit'].astype(int)
            df = df.dropna(subset=['Close', 'Volume'])
        return df

    def get_fundamental_data(self, start_date, end_date):
        sql = f"""
        SELECT
            s.SecuCode, s.SecuAbbr, i.FirstIndustryName, i.SecondIndustryName, i.ThirdIndustryName,
            m.*,
            m.EndDate
        FROM LC_MainIndexNew m
        LEFT JOIN LC_ExgIndustry i ON m.CompanyCode = i.CompanyCode
        LEFT JOIN SecuMain s ON m.CompanyCode = s.CompanyCode
        WHERE i.Standard = '38'
          AND s.SecuCategory = 1
          AND m.EndDate >= '{start_date}'
          AND m.EndDate <= '{end_date}'
        """
        df = self.query(sql)
        if 'SecuAbbr' in df.columns:
            df = df[~df['SecuAbbr'].str.contains('ST', case=False, na=False)]
        return df

    def query_table(self, table_key, fields=None, where=None, limit=100, order_by=None, params=None):
        schema = TABLES[table_key]
        table = schema['table']
        if fields is None:
            fields = schema['fields']
        fields_str = ', '.join(fields)
        sql = f"SELECT TOP {limit} {fields_str} FROM {table}"
        if where:
            sql += f" WHERE {where}"
        if order_by:
            sql += f" ORDER BY {order_by}"
        return self.query(sql, params=params)

    def get_stock_basic_info(self, secu_code):
        """
        è·å–è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯
        Args:
            secu_code: è‚¡ç¥¨ä»£ç 
        Returns:
            dict: è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯ï¼ŒåŒ…å«SecuCode, SecuAbbr, CompanyCode, InnerCode, FirstIndustryName, SecondIndustryName, ThirdIndustryName
        """
        try:
            sql = f"""
            SELECT TOP 1
                s.SecuCode, s.SecuAbbr, s.CompanyCode, s.InnerCode,
                i.FirstIndustryName, i.SecondIndustryName, i.ThirdIndustryName
            FROM SecuMain s
            LEFT JOIN LC_ExgIndustry i ON s.CompanyCode = i.CompanyCode
                AND i.Standard = '38'
                AND i.IfPerformed = 1
            WHERE s.SecuCode = ? AND s.SecuCategory = 1
            """
            df = self.query(sql, params=[secu_code])
            
            if df.empty:
                return None
            
            # è¿”å›ç¬¬ä¸€è¡Œæ•°æ®ä½œä¸ºå­—å…¸
            return df.iloc[0].to_dict()
            
        except Exception as e:
            logger.error(f"è·å–è‚¡ç¥¨ {secu_code} åŸºæœ¬ä¿¡æ¯æ—¶å‡ºé”™: {e}")
            return None
    
    def batch_get_stock_basic_info(self, secu_codes: List[str]) -> Dict[str, Dict]:
        """
        æ‰¹é‡è·å–è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯ï¼ˆé«˜æ€§èƒ½ç‰ˆæœ¬ï¼‰
        Args:
            secu_codes: è‚¡ç¥¨ä»£ç åˆ—è¡¨
        Returns:
            dict: {è‚¡ç¥¨ä»£ç : è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯å­—å…¸}
        """
        if not secu_codes:
            return {}
        
        try:
            # æ„å»ºINå­å¥
            codes_str = ','.join([f"'{code}'" for code in secu_codes])
            
            sql = f"""
            SELECT
                s.SecuCode, s.SecuAbbr, s.CompanyCode, s.InnerCode,
                i.FirstIndustryName, i.SecondIndustryName, i.ThirdIndustryName
            FROM SecuMain s
            LEFT JOIN LC_ExgIndustry i ON s.CompanyCode = i.CompanyCode
                AND i.Standard = '38'
                AND i.IfPerformed = 1
            WHERE s.SecuCode IN ({codes_str}) 
            AND s.SecuCategory = 1
            """
            
            df = self.query(sql)
            
            if df.empty:
                logger.warning(f"æ‰¹é‡è·å–è‚¡ç¥¨ä¿¡æ¯ï¼šæŸ¥è¯¢è¿”å›ç©ºç»“æœï¼Œè‚¡ç¥¨ä»£ç : {secu_codes[:10]}")
                return {}
            
            # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
            result = {}
            for _, row in df.iterrows():
                code = str(row['SecuCode']).strip()
                # å¤„ç†æµ®ç‚¹æ•°æ ¼å¼
                if '.' in code:
                    code = code.split('.')[0]
                code = code.zfill(6)
                result[code] = row.to_dict()
            
            # æ£€æŸ¥æ˜¯å¦æœ‰è‚¡ç¥¨ä»£ç æœªæ‰¾åˆ°
            found_codes = set(result.keys())
            requested_codes = set([str(c).strip().zfill(6) for c in secu_codes])
            missing_codes = requested_codes - found_codes
            if missing_codes:
                logger.warning(f"æ‰¹é‡è·å–è‚¡ç¥¨ä¿¡æ¯ï¼š{len(missing_codes)} åªè‚¡ç¥¨æœªæ‰¾åˆ°ï¼Œç¤ºä¾‹: {list(missing_codes)[:10]}")
            
            return result
            
        except Exception as e:
            logger.error(f"æ‰¹é‡è·å–è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯æ—¶å‡ºé”™: {e}")
            return {}

    def get_adjusting_factors(self, inner_codes):
        """æ‰¹é‡è·å–å¤æƒå› å­æ•°æ®ï¼ˆä¼˜åŒ–ï¼šå¢å¤§æ‰¹æ¬¡ï¼Œå‡å°‘æŸ¥è¯¢æ¬¡æ•°ï¼‰"""
        if not inner_codes:
            return {}
        
        logger.info(f"å¼€å§‹è·å– {len(inner_codes)} åªè‚¡ç¥¨çš„å¤æƒå› å­æ•°æ®...")
        start_time = time.time()
        
        # æ€§èƒ½ä¼˜åŒ–ï¼šå¢å¤§æ‰¹æ¬¡å¤§å°ï¼Œå‡å°‘æŸ¥è¯¢æ¬¡æ•°ï¼Œå……åˆ†åˆ©ç”¨ç½‘ç»œå¸¦å®½
        batch_size = 500  # æ¯æ‰¹å¤„ç†500åªè‚¡ç¥¨ï¼ˆå¤§å¹…å¢åŠ æ‰¹æ¬¡å¤§å°ï¼‰
        all_results = {}
        
        for i in range(0, len(inner_codes), batch_size):
            batch_codes = inner_codes[i:i + batch_size]
            batch_num = i // batch_size + 1
            total_batches = (len(inner_codes) + batch_size - 1) // batch_size
            logger.info(f"å¤„ç†å¤æƒå› å­æ‰¹æ¬¡ {batch_num}/{total_batches}ï¼ŒåŒ…å« {len(batch_codes)} åªè‚¡ç¥¨")
            
            # æ„å»ºæ‰¹é‡æŸ¥è¯¢SQL
            codes_str = ','.join([str(code) for code in batch_codes])
            sql = f"""
            SELECT 
                InnerCode,
                ExDiviDate,
                AdjustingFactor,
                AdjustingConst,
                RatioAdjustingFactor
            FROM QT_AdjustingFactor
            WHERE InnerCode IN ({codes_str})
            ORDER BY InnerCode, ExDiviDate DESC
            """
            
            try:
                batch_start = time.time()
                df = self.query(sql)
                batch_query_time = time.time() - batch_start
                
                if df.empty:
                    logger.warning(f"å¤æƒå› å­æ‰¹æ¬¡ {batch_num} æŸ¥è¯¢ç»“æœä¸ºç©ºï¼ˆè€—æ—¶: {batch_query_time:.2f}ç§’ï¼‰")
                    continue
                
                # æŒ‰InnerCodeåˆ†ç»„
                for inner_code in batch_codes:
                    stock_factors = df[df['InnerCode'] == inner_code].copy()
                    if not stock_factors.empty:
                        # æŒ‰æ—¥æœŸæ’åºï¼ˆä»æ–°åˆ°æ—§ï¼‰
                        stock_factors = stock_factors.sort_values('ExDiviDate', ascending=False)
                        all_results[inner_code] = stock_factors
                
                batch_time = time.time() - start_time
                logger.info(f"å¤æƒå› å­æ‰¹æ¬¡ {batch_num} å®Œæˆï¼ˆæŸ¥è¯¢è€—æ—¶: {batch_query_time:.2f}ç§’ï¼‰ï¼Œå·²è·å– {len(all_results)} åªè‚¡ç¥¨å¤æƒå› å­")
                
            except Exception as e:
                logger.error(f"å¤æƒå› å­æ‰¹æ¬¡ {batch_num} è·å–å¤±è´¥: {e}")
                continue
        
        total_time = time.time() - start_time
        logger.info(f"å¤æƒå› å­æ•°æ®è·å–å®Œæˆï¼Œæ€»è€—æ—¶: {total_time:.1f}ç§’ï¼ŒæˆåŠŸè·å– {len(all_results)} åªè‚¡ç¥¨å¤æƒå› å­")
        
        return all_results

    def calculate_adjusted_prices(self, stock_data, adjusting_factors):
        """è®¡ç®—å¤æƒä»·æ ¼ï¼ˆå‰å¤æƒï¼‰"""
        if stock_data.empty or not adjusting_factors:
            return stock_data
        
        adjusted_data = stock_data.copy()
        inner_code = stock_data.iloc[0]['InnerCode'] if 'InnerCode' in stock_data.columns else None
        
        if not inner_code or inner_code not in adjusting_factors:
            return stock_data
        
        factors = adjusting_factors[inner_code]
        
        # å¯¹æ¯ä¸ªäº¤æ˜“æ—¥è®¡ç®—å¤æƒä»·æ ¼
        for idx, row in adjusted_data.iterrows():
            trading_date = row.name  # å‡è®¾ç´¢å¼•æ˜¯æ—¥æœŸ
            
            # æ‰¾åˆ°è¯¥æ—¥æœŸä¹‹å‰æœ€è¿‘çš„å¤æƒå› å­
            applicable_factors = factors[factors['ExDiviDate'] <= trading_date]
            if not applicable_factors.empty:
                # ä½¿ç”¨æœ€æ–°çš„å¤æƒå› å­
                factor = applicable_factors.iloc[0]
                
                # å‰å¤æƒä»·æ ¼è®¡ç®—
                if pd.notna(factor['AdjustingFactor']) and pd.notna(factor['AdjustingConst']):
                    adjusted_data.loc[idx, 'Close'] = (
                        row['Close'] * factor['AdjustingFactor'] + factor['AdjustingConst']
                    )
                    adjusted_data.loc[idx, 'Open'] = (
                        row['Open'] * factor['AdjustingFactor'] + factor['AdjustingConst']
                    )
                    adjusted_data.loc[idx, 'High'] = (
                        row['High'] * factor['AdjustingFactor'] + factor['AdjustingConst']
                    )
                    adjusted_data.loc[idx, 'Low'] = (
                        row['Low'] * factor['AdjustingFactor'] + factor['AdjustingConst']
                    )
        
        return adjusted_data

    def batch_get_stock_data_with_adjustment(self, stock_codes, days=365, time_config=None):
        """æ‰¹é‡è·å–è‚¡ç¥¨æ•°æ®å¹¶è®¡ç®—å¤æƒä»·æ ¼"""
        if not stock_codes:
            return {}
        
        logger.info(f"å¼€å§‹æ‰¹é‡è·å– {len(stock_codes)} åªè‚¡ç¥¨çš„è¡Œæƒ…æ•°æ®ï¼ˆå«å¤æƒä»·æ ¼ï¼‰...")
        start_time = time.time()
        
        # æ ¹æ®æ—¶é—´é…ç½®è°ƒæ•´æ•°æ®è·å–èŒƒå›´
        if time_config is not None:
            # è®¡ç®—éœ€è¦è·å–çš„æ•°æ®å¤©æ•°
            # ä¼˜å…ˆä½¿ç”¨data_start_dateå’Œdata_end_dateï¼ˆæ¨¡å—17ä½¿ç”¨ï¼‰
            # å¦‚æœæ²¡æœ‰ï¼Œåˆ™ä½¿ç”¨crash_start_dateå’Œcrash_end_dateï¼ˆæ¨¡å—2ä½¿ç”¨ï¼Œå‘åå…¼å®¹ï¼‰
            if hasattr(time_config, 'data_start_date') and hasattr(time_config, 'data_end_date'):
                start_date = pd.Timestamp(time_config.data_start_date)
                end_date = pd.Timestamp(time_config.data_end_date)
                analysis_date = end_date
                days_needed = (end_date - start_date).days + 100  # å¤šè·å–100å¤©ä½œä¸ºç¼“å†²
            elif hasattr(time_config, 'crash_start_date') and hasattr(time_config, 'crash_end_date'):
                analysis_date = pd.Timestamp(time_config.analysis_date) if hasattr(time_config, 'analysis_date') else pd.Timestamp(time_config.crash_end_date)
                crash_start_date = pd.Timestamp(time_config.crash_start_date)
                days_needed = (analysis_date - crash_start_date).days + 100  # å¤šè·å–100å¤©ä½œä¸ºç¼“å†²
            else:
                # å¦‚æœæ²¡æœ‰æ—¥æœŸé…ç½®ï¼Œä½¿ç”¨analysis_date
                analysis_date = pd.Timestamp(time_config.analysis_date) if hasattr(time_config, 'analysis_date') else pd.Timestamp.now()
                days_needed = days + 100  # ä½¿ç”¨ä¼ å…¥çš„dayså‚æ•°ï¼Œå¤šè·å–100å¤©ä½œä¸ºç¼“å†²
            
            days = max(days, days_needed)
            logger.info(f"æ ¹æ®æ—¶é—´é…ç½®è°ƒæ•´æ•°æ®è·å–èŒƒå›´: {days} å¤©")
        
        # é¦–å…ˆè·å–æ‰€æœ‰è‚¡ç¥¨çš„InnerCode
        # æ³¨æ„ï¼šè¿™é‡Œä¸åº”è¯¥å†æ¬¡åº”ç”¨STOCK_FILTERï¼Œå› ä¸ºä¼ å…¥çš„stock_codeså·²ç»æ˜¯ç­›é€‰è¿‡çš„
        codes_str = ','.join([f"'{code}'" for code in stock_codes])
        sql_inner_codes = f"""
        SELECT SecuCode, InnerCode
        FROM SecuMain
        WHERE SecuCode IN ({codes_str})
        """
        
        df_inner_codes = self.query(sql_inner_codes)
        if df_inner_codes.empty:
            logger.warning(f"æœªæ‰¾åˆ°æœ‰æ•ˆçš„è‚¡ç¥¨InnerCodeï¼ˆè¾“å…¥çš„{len(stock_codes)}åªè‚¡ç¥¨åœ¨SecuMainä¸­éƒ½ä¸å­˜åœ¨ï¼‰")
            return {}
        
        # æ£€æŸ¥æ˜¯å¦æœ‰è‚¡ç¥¨åœ¨SecuMainä¸­ä¸å­˜åœ¨
        found_codes = set(df_inner_codes['SecuCode'].astype(str))
        input_codes = set([str(code).zfill(6) for code in stock_codes])
        missing_codes = input_codes - found_codes
        if missing_codes:
            logger.warning(f"ä»¥ä¸‹{len(missing_codes)}åªè‚¡ç¥¨åœ¨SecuMainä¸­ä¸å­˜åœ¨: {list(missing_codes)[:10]}{'...' if len(missing_codes) > 10 else ''}")
        
        # åˆ›å»ºè‚¡ç¥¨ä»£ç åˆ°InnerCodeçš„æ˜ å°„
        code_to_inner = dict(zip(df_inner_codes['SecuCode'].astype(str), df_inner_codes['InnerCode']))
        inner_codes = list(code_to_inner.values())
        
        logger.info(f"[batch_get_stock_data_with_adjustment] è¾“å…¥{len(stock_codes)}åªè‚¡ç¥¨ï¼Œæ‰¾åˆ°{len(code_to_inner)}åªè‚¡ç¥¨çš„InnerCode")
        
        # è·å–å¤æƒå› å­æ•°æ®
        adjusting_factors = self.get_adjusting_factors(inner_codes)
        
        # åˆ†æ‰¹å¤„ç†ï¼Œé¿å…SQLè¯­å¥è¿‡é•¿
        # æé™ä¼˜åŒ–ï¼šä½¿ç”¨éå¸¸å°çš„æ‰¹æ¬¡å¤§å°ï¼Œæœ€å¤§åŒ–å¹¶è¡Œåº¦ï¼Œå‹æ¦¨ç¡¬ä»¶æ€§èƒ½
        # ç›®æ ‡ï¼šäº§ç”Ÿå°½å¯èƒ½å¤šçš„æ‰¹æ¬¡ï¼Œå……åˆ†åˆ©ç”¨å¤šçº¿ç¨‹å¹¶è¡Œ
        total_stocks = len(stock_codes)
        from config import MAX_WORKERS
        
        # æé™ä¼˜åŒ–ç­–ç•¥ï¼šæ‰¹æ¬¡å¤§å° = 10-20åªè‚¡ç¥¨ï¼Œäº§ç”Ÿå¤§é‡æ‰¹æ¬¡
        # è¿™æ ·å¯ä»¥è®©æ›´å¤šçº¿ç¨‹åŒæ—¶å·¥ä½œï¼Œæœ€å¤§åŒ–ç¡¬ä»¶åˆ©ç”¨ç‡
        if days > 800:  # è¶…è¿‡800å¤©ï¼ˆçº¦3å¹´ï¼‰ï¼Œä½¿ç”¨å¾ˆå°æ‰¹æ¬¡
            batch_size = 15  # æå°æ‰¹æ¬¡ï¼Œæœ€å¤§åŒ–å¹¶è¡Œåº¦
        elif days > 400:  # 1-3å¹´æ•°æ®
            batch_size = 20  # å°æ‰¹æ¬¡
        else:  # 1å¹´ä»¥å†…æ•°æ®
            batch_size = 25  # å°æ‰¹æ¬¡ï¼Œä½†æ¯”é•¿æœŸæ•°æ®ç¨å¤§
        batches = []
        # åªå¤„ç†èƒ½è·å–åˆ°InnerCodeçš„è‚¡ç¥¨
        valid_stock_codes = [code for code in stock_codes if str(code).zfill(6) in code_to_inner]
        total_batches = (len(valid_stock_codes) + batch_size - 1) // batch_size
        
        for i in range(0, len(valid_stock_codes), batch_size):
            batch_codes = valid_stock_codes[i:i + batch_size]
            batches.append({
                'batch_id': i // batch_size + 1,
                'total_batches': total_batches,
                'codes': batch_codes,
                'code_to_inner': code_to_inner,
                'adjusting_factors': adjusting_factors,
                'days': days,
                'stock_codes': stock_codes,  # ä¿å­˜åŸå§‹è¾“å…¥çš„è‚¡ç¥¨åˆ—è¡¨ï¼Œç”¨äºè°ƒè¯•è¾“å‡º
                'time_config': time_config
            })
        
        def process_batch(batch_info):
            """å¤„ç†å•ä¸ªæ‰¹æ¬¡"""
            batch_id = batch_info['batch_id']
            total_batches = batch_info['total_batches']
            batch_codes = batch_info['codes']
            code_to_inner = batch_info['code_to_inner']
            adjusting_factors = batch_info['adjusting_factors']
            days = batch_info['days']
            time_config = batch_info.get('time_config')
            
            thread_name = threading.current_thread().name
            logger.info(f"[çº¿ç¨‹ {thread_name}] å¤„ç†æ‰¹æ¬¡ {batch_id}/{total_batches}ï¼ŒåŒ…å« {len(batch_codes)} åªè‚¡ç¥¨")
            
            batch_results = {}
            
            # æ„å»ºæ‰¹é‡æŸ¥è¯¢SQL
            codes_str = ','.join([f"'{code}'" for code in batch_codes])
            
            # æ ¹æ®æ—¶é—´é…ç½®è°ƒæ•´SQLæŸ¥è¯¢
            if time_config is not None:
                # ä¼˜å…ˆä½¿ç”¨data_start_dateå’Œdata_end_dateï¼ˆæ¨¡å—17ä½¿ç”¨ï¼‰
                # å¦‚æœæ²¡æœ‰ï¼Œåˆ™ä½¿ç”¨crash_start_dateå’Œcrash_end_dateï¼ˆæ¨¡å—2ä½¿ç”¨ï¼Œå‘åå…¼å®¹ï¼‰
                if hasattr(time_config, 'data_start_date') and hasattr(time_config, 'data_end_date'):
                    start_date = pd.Timestamp(time_config.data_start_date)
                    end_date = pd.Timestamp(time_config.data_end_date)
                elif hasattr(time_config, 'crash_start_date') and hasattr(time_config, 'crash_end_date'):
                    start_date = pd.Timestamp(time_config.crash_start_date)
                    end_date = pd.Timestamp(time_config.crash_end_date)
                else:
                    # å¦‚æœæ²¡æœ‰æ—¥æœŸé…ç½®ï¼Œä½¿ç”¨analysis_date
                    analysis_date = pd.Timestamp(time_config.analysis_date) if hasattr(time_config, 'analysis_date') else pd.Timestamp.now()
                    start_date = analysis_date - pd.Timedelta(days=days + 30)
                    end_date = analysis_date + pd.Timedelta(days=10)
                
                # è®¡ç®—æŸ¥è¯¢çš„æ—¥æœŸèŒƒå›´ï¼Œç¡®ä¿åŒ…å«æ‰€éœ€çš„æ—¶é—´æ®µ
                query_start_date = start_date - pd.Timedelta(days=50)  # å¤šè·å–50å¤©ä½œä¸ºç¼“å†²
                query_end_date = end_date + pd.Timedelta(days=50)  # å¤šè·å–50å¤©ä½œä¸ºç¼“å†²
                
                sql = f"""
                SELECT 
                    s.{TABLE_CONFIG['code_field']},
                    s.{TABLE_CONFIG['inner_code_field']},
                    q.{TABLE_CONFIG['date_field']} as [Date],
                    q.{TABLE_CONFIG['open_field']} as [Open],
                    q.{TABLE_CONFIG['high_field']} as [High],
                    q.{TABLE_CONFIG['low_field']} as [Low],
                    q.{TABLE_CONFIG['close_field']} as [Close],
                    q.{TABLE_CONFIG['volume_field']} as [Volume],
                    q.{TABLE_CONFIG['turnover_rate_field']} as [TurnoverRate],
                    q.{TABLE_CONFIG['negotiable_mv_field']} as [NegotiableMV],
                    q.BetaLargeCapIndex as BetaLargeCapIndex,
                    q.BetaCompositeIndex as BetaCompositeIndex,
                    q.BetaSYWGIndustryIndex as BetaSYWGIndustryIndex,
                    q.BetaMidCapIndex as BetaMidCapIndex,
                    q.AlphaLargeCapIndex as AlphaLargeCapIndex,
                    q.AlphaCompositeIndex as AlphaCompositeIndex,
                    q.AlphaSYWGIndustryIndex as AlphaSYWGIndustryIndex,
                    q.AlphMidCapIndex as AlphMidCapIndex,
                    q.YearVolatilityByDay as YearVolatilityByDay,
                    q.YearVolatilityByWeek as YearVolatilityByWeek,
                    q.YearSharpeRatio as YearSharpeRatio,
                    q.MarketIndexRORArithAvg as MarketIndexRORArithAvg,
                    q.MarketIndexRORGeomMean as MarketIndexRORGeomMean,
                    p.SurgedLimit as SurgedLimit,
                    p.DeclineLimit as DeclineLimit
                FROM {TABLE_CONFIG['stock_info_table']} s
                INNER JOIN {TABLE_CONFIG['daily_quote_table']} q 
                    ON s.{TABLE_CONFIG['inner_code_field']} = q.{TABLE_CONFIG['inner_code_field']}
                LEFT JOIN QT_PerformanceData p 
                    ON q.{TABLE_CONFIG['inner_code_field']} = p.InnerCode 
                    AND q.{TABLE_CONFIG['date_field']} = p.TradingDay
                WHERE s.{TABLE_CONFIG['code_field']} IN ({codes_str})
                AND q.{TABLE_CONFIG['date_field']} >= CAST('{query_start_date.date()}' AS DATE)
                AND q.{TABLE_CONFIG['date_field']} <= CAST('{query_end_date.date()}' AS DATE)
                ORDER BY s.{TABLE_CONFIG['code_field']}, q.{TABLE_CONFIG['date_field']} DESC
                """
                
                # è°ƒè¯•ï¼šæ‰“å°SQLï¼ˆä»…ç¬¬ä¸€ä¸ªæ‰¹æ¬¡ï¼‰
                if batch_id == 1:
                    logger.debug(f"[è°ƒè¯•] batch_get_stock_data_with_adjustment SQLæŸ¥è¯¢")
                    # è®¡ç®—å®é™…ä¼ å…¥çš„è‚¡ç¥¨æ€»æ•°
                    actual_total_stocks = len(code_to_inner)  # å®é™…èƒ½è·å–åˆ°InnerCodeçš„è‚¡ç¥¨æ•°
                    input_total = len(stock_codes) if 'stock_codes' in batch_info else len(batch_codes)
                    logger.debug(f"è¾“å…¥è‚¡ç¥¨æ•°: {input_total}åªï¼Œæ‰¾åˆ°InnerCode: {actual_total_stocks}åªï¼Œæ€»æ‰¹æ¬¡æ•°: {total_batches}æ‰¹")
                    print(f"å½“å‰æ‰¹æ¬¡: {batch_id}/{total_batches}")
                    print(f"æœ¬æ‰¹æ¬¡è‚¡ç¥¨æ•°: {len(batch_codes)}åª")
                    print(f"æ‰¹æ¬¡å¤§å°: {len(batch_codes)}åª/æ‰¹ (æ ¹æ®æ•°æ®å¤©æ•°åŠ¨æ€è°ƒæ•´)")
                    print(f"è‚¡ç¥¨ä»£ç : {batch_codes[:5]}... (å…±{len(batch_codes)}åª)")
                    logger.debug(f"æ—¥æœŸèŒƒå›´: {query_start_date.date()} è‡³ {query_end_date.date()}")
                    logger.debug(f"å®é™…æ‰§è¡Œçš„SQL:\n{sql}")
                
                df = self.query(sql)
                
                # è°ƒè¯•ï¼šæ£€æŸ¥æŸ¥è¯¢ç»“æœï¼ˆä»…ç¬¬ä¸€ä¸ªæ‰¹æ¬¡ï¼Œå³ä½¿ä¸ºç©ºä¹Ÿè¾“å‡ºï¼‰
                if batch_id == 1:
                    logger.debug(f"[è°ƒè¯•] æŸ¥è¯¢ç»“æœç»Ÿè®¡:")
                    if df.empty:
                        logger.warning(f"æŸ¥è¯¢ç»“æœä¸ºç©ºï¼å¯èƒ½çš„åŸå› : 1. æ—¥æœŸèŒƒå›´é—®é¢˜ 2. è‚¡ç¥¨ä»£ç ä¸å­˜åœ¨ 3. STOCK_FILTERç­›é€‰æ¡ä»¶å¤ªä¸¥æ ¼")
                        # æµ‹è¯•æŸ¥è¯¢1ï¼šæ£€æŸ¥SecuMain
                        test_sql1 = f"""
                        SELECT TOP 5
                            s.{TABLE_CONFIG['code_field']},
                            COUNT(*) as record_count
                        FROM {TABLE_CONFIG['stock_info_table']} s
                        WHERE s.{TABLE_CONFIG['code_field']} IN ({codes_str})
                        AND {STOCK_FILTER}
                        GROUP BY s.{TABLE_CONFIG['code_field']}
                        """
                        # æµ‹è¯•æŸ¥è¯¢2ï¼šæ£€æŸ¥QT_StockPerformanceï¼ˆå»æ‰æ—¥æœŸé™åˆ¶ï¼‰
                        test_sql2 = f"""
                        SELECT TOP 5
                            s.{TABLE_CONFIG['code_field']},
                            MIN(q.{TABLE_CONFIG['date_field']}) as min_date,
                            MAX(q.{TABLE_CONFIG['date_field']}) as max_date,
                            COUNT(*) as record_count
                        FROM {TABLE_CONFIG['stock_info_table']} s
                        INNER JOIN {TABLE_CONFIG['daily_quote_table']} q 
                            ON s.{TABLE_CONFIG['inner_code_field']} = q.{TABLE_CONFIG['inner_code_field']}
                        WHERE s.{TABLE_CONFIG['code_field']} IN ({codes_str})
                        AND {STOCK_FILTER}
                        GROUP BY s.{TABLE_CONFIG['code_field']}
                        """
                        logger.debug(f"æµ‹è¯•1: æ£€æŸ¥SecuMainä¸­çš„è‚¡ç¥¨")
                        try:
                            test_df1 = self.query(test_sql1)
                            if not test_df1.empty:
                                logger.debug(f"æµ‹è¯•1ç»“æœ: æ‰¾åˆ° {len(test_df1)} åªè‚¡ç¥¨åœ¨SecuMainä¸­")
                            else:
                                logger.warning(f"æµ‹è¯•1ç»“æœ: è¿™ {len(batch_codes)} åªè‚¡ç¥¨éƒ½ä¸æ»¡è¶³STOCK_FILTERæ¡ä»¶")
                        except Exception as e:
                            logger.warning(f"æµ‹è¯•1æŸ¥è¯¢å¤±è´¥: {e}")
                        
                        logger.debug(f"æµ‹è¯•2: æ£€æŸ¥QT_StockPerformanceä¸­çš„è¡Œæƒ…æ•°æ®ï¼ˆæ— æ—¥æœŸé™åˆ¶ï¼‰")
                        try:
                            test_df2 = self.query(test_sql2)
                            if not test_df2.empty:
                                logger.debug(f"æµ‹è¯•2ç»“æœ: æ‰¾åˆ° {len(test_df2)} åªè‚¡ç¥¨æœ‰è¡Œæƒ…æ•°æ®")
                                # æ£€æŸ¥æ—¥æœŸèŒƒå›´
                                if 'min_date' in test_df2.columns and 'max_date' in test_df2.columns:
                                    min_date = test_df2['min_date'].min()
                                    max_date = test_df2['max_date'].max()
                                    # ç¡®ä¿æ—¥æœŸç±»å‹ä¸€è‡´ï¼ˆè½¬æ¢ä¸ºdateå¯¹è±¡è¿›è¡Œæ¯”è¾ƒï¼‰
                                    if isinstance(min_date, pd.Timestamp):
                                        min_date = min_date.date()
                                    if isinstance(max_date, pd.Timestamp):
                                        max_date = max_date.date()
                                    logger.debug(f"è¡Œæƒ…æ•°æ®æ—¥æœŸèŒƒå›´: {min_date} è‡³ {max_date}")
                                    logger.debug(f"æŸ¥è¯¢æ—¥æœŸèŒƒå›´: {query_start_date.date()} è‡³ {query_end_date.date()}")
                                    if query_end_date.date() > max_date:
                                        logger.warning(f"æŸ¥è¯¢ç»“æŸæ—¥æœŸ {query_end_date.date()} è¶…è¿‡æœ€å¤§è¡Œæƒ…æ—¥æœŸ {max_date}")
                                    if query_start_date.date() < min_date:
                                        logger.warning(f"æŸ¥è¯¢å¼€å§‹æ—¥æœŸ {query_start_date.date()} æ—©äºæœ€å°è¡Œæƒ…æ—¥æœŸ {min_date}")
                                    # æ£€æŸ¥æ—¥æœŸèŒƒå›´æ˜¯å¦å®Œå…¨ä¸é‡å 
                                    if query_end_date.date() < min_date or query_start_date.date() > max_date:
                                        logger.warning(f"æŸ¥è¯¢æ—¥æœŸèŒƒå›´ä¸æ•°æ®æ—¥æœŸèŒƒå›´å®Œå…¨ä¸é‡å ï¼è¿™äº›å¯èƒ½æ˜¯æ–°ä¸Šå¸‚è‚¡ç¥¨ï¼Œåœ¨æŸ¥è¯¢æ—¥æœŸèŒƒå›´å†…æ²¡æœ‰æ•°æ®ã€‚")
                            else:
                                logger.warning(f"æµ‹è¯•2ç»“æœ: è¿™ {len(batch_codes)} åªè‚¡ç¥¨åœ¨QT_StockPerformanceä¸­æ²¡æœ‰æ•°æ®")
                        except Exception as e:
                            logger.warning(f"æµ‹è¯•2æŸ¥è¯¢å¤±è´¥: {e}")
                    else:
                        logger.debug(f"åˆ—å: {list(df.columns)}, è¡Œæ•°: {len(df)}")
                        if 'SurgedLimit' in df.columns:
                            surged_limit_count = df['SurgedLimit'].notna().sum()
                            surged_limit_1_count = (df['SurgedLimit'] == 1).sum()
                            logger.debug(f"SurgedLimitå­—æ®µç»Ÿè®¡: éç©ºæ•°é‡={surged_limit_count}/{len(df)}, å€¼ä¸º1æ•°é‡={surged_limit_1_count}")
                        else:
                            logger.warning(f"æŸ¥è¯¢ç»“æœä¸­ç¼ºå°‘SurgedLimitåˆ—ï¼å¯ç”¨åˆ—: {list(df.columns)}")
            else:
                # ä¼˜åŒ–ï¼šä½¿ç”¨çª—å£å‡½æ•°é™åˆ¶æ¯åªè‚¡ç¥¨åªè·å–æœ€è¿‘dayså¤©çš„æ•°æ®ï¼Œå¤§å¹…æå‡æŸ¥è¯¢é€Ÿåº¦
                # è®¡ç®—æˆªæ­¢æ—¥æœŸï¼ˆæœ€è¿‘dayså¤©ï¼‰
                from datetime import date, timedelta
                # ä¼˜åŒ–ï¼šä½¿ç”¨æ•°æ®åº“æœ€æ–°äº¤æ˜“æ—¥ï¼Œè€Œä¸æ˜¯å½“å‰è‡ªç„¶æ—¥ï¼ˆæå‡å‡†ç¡®æ€§ï¼‰
                try:
                    end_date = self.get_latest_trading_date()
                except:
                    end_date = date.today()
                start_date = end_date - timedelta(days=days + 30)  # å¤šè·å–30å¤©ä½œä¸ºç¼“å†²
                
                # æ€§èƒ½ä¼˜åŒ–ï¼šä½¿ç”¨æ›´é«˜æ•ˆçš„SQLæŸ¥è¯¢
                # 1. å…ˆè¿‡æ»¤æ—¥æœŸèŒƒå›´ï¼Œå‡å°‘çª—å£å‡½æ•°è®¡ç®—çš„æ•°æ®é‡
                # 2. ä½¿ç”¨TOPè€Œä¸æ˜¯ROW_NUMBERï¼Œåœ¨æŸäº›æƒ…å†µä¸‹æ›´å¿«
                # 3. ä¼˜åŒ–JOINé¡ºåºï¼Œå…ˆè¿‡æ»¤å†JOIN
                sql = f"""
                WITH FilteredQuotes AS (
                    SELECT 
                        q.{TABLE_CONFIG['inner_code_field']},
                        q.{TABLE_CONFIG['date_field']} as [Date],
                        q.{TABLE_CONFIG['open_field']} as [Open],
                        q.{TABLE_CONFIG['high_field']} as [High],
                        q.{TABLE_CONFIG['low_field']} as [Low],
                        q.{TABLE_CONFIG['close_field']} as [Close],
                        q.{TABLE_CONFIG['volume_field']} as [Volume],
                        q.{TABLE_CONFIG['turnover_rate_field']} as [TurnoverRate],
                        q.{TABLE_CONFIG['negotiable_mv_field']} as [NegotiableMV],
                        q.BetaLargeCapIndex as BetaLargeCapIndex,
                        q.BetaCompositeIndex as BetaCompositeIndex,
                        q.BetaSYWGIndustryIndex as BetaSYWGIndustryIndex,
                        q.BetaMidCapIndex as BetaMidCapIndex,
                        q.AlphaLargeCapIndex as AlphaLargeCapIndex,
                        q.AlphaCompositeIndex as AlphaCompositeIndex,
                        q.AlphaSYWGIndustryIndex as AlphaSYWGIndustryIndex,
                        q.AlphMidCapIndex as AlphMidCapIndex,
                        q.YearVolatilityByDay as YearVolatilityByDay,
                        q.YearVolatilityByWeek as YearVolatilityByWeek,
                        q.YearSharpeRatio as YearSharpeRatio,
                        q.MarketIndexRORArithAvg as MarketIndexRORArithAvg,
                        q.MarketIndexRORGeomMean as MarketIndexRORGeomMean
                    FROM {TABLE_CONFIG['daily_quote_table']} q
                    WHERE q.{TABLE_CONFIG['date_field']} >= CAST('{start_date}' AS DATE)
                    AND q.{TABLE_CONFIG['date_field']} <= CAST('{end_date}' AS DATE)
                ),
                RankedData AS (
                    SELECT 
                        s.{TABLE_CONFIG['code_field']} as SecuCode,
                        s.{TABLE_CONFIG['inner_code_field']} as InnerCode,
                        fq.Date,
                        fq.[Open],
                        fq.[High],
                        fq.[Low],
                        fq.[Close],
                        fq.[Volume],
                        fq.[TurnoverRate],
                        fq.[NegotiableMV],
                        fq.BetaLargeCapIndex,
                        fq.BetaCompositeIndex,
                        fq.BetaSYWGIndustryIndex,
                        fq.BetaMidCapIndex,
                        fq.AlphaLargeCapIndex,
                        fq.AlphaCompositeIndex,
                        fq.AlphaSYWGIndustryIndex,
                        fq.AlphMidCapIndex,
                        fq.YearVolatilityByDay,
                        fq.YearVolatilityByWeek,
                        fq.YearSharpeRatio,
                        fq.MarketIndexRORArithAvg,
                        fq.MarketIndexRORGeomMean,
                        p.SurgedLimit as SurgedLimit,
                        p.DeclineLimit as DeclineLimit,
                        ROW_NUMBER() OVER (PARTITION BY s.{TABLE_CONFIG['code_field']} ORDER BY fq.Date DESC) as rn
                    FROM {TABLE_CONFIG['stock_info_table']} s
                    INNER JOIN FilteredQuotes fq 
                        ON s.{TABLE_CONFIG['inner_code_field']} = fq.{TABLE_CONFIG['inner_code_field']}
                    LEFT JOIN QT_PerformanceData p 
                        ON fq.{TABLE_CONFIG['inner_code_field']} = p.InnerCode 
                        AND fq.Date = p.TradingDay
                    WHERE s.{TABLE_CONFIG['code_field']} IN ({codes_str})
                    AND {STOCK_FILTER}
                )
                SELECT 
                    SecuCode,
                    InnerCode,
                    Date,
                    [Open],
                    [High],
                    [Low],
                    [Close],
                    [Volume],
                    [TurnoverRate],
                    [NegotiableMV],
                    BetaLargeCapIndex,
                    BetaCompositeIndex,
                    BetaSYWGIndustryIndex,
                    BetaMidCapIndex,
                    AlphaLargeCapIndex,
                    AlphaCompositeIndex,
                    AlphaSYWGIndustryIndex,
                    AlphMidCapIndex,
                    YearVolatilityByDay,
                    YearVolatilityByWeek,
                    YearSharpeRatio,
                    MarketIndexRORArithAvg,
                    MarketIndexRORGeomMean,
                    SurgedLimit,
                    DeclineLimit
                FROM RankedData
                WHERE rn <= {days + 30}
                ORDER BY SecuCode, Date DESC
                """
            
            try:
                df = self.query(sql)
                if df.empty:
                    return batch_results
                
                # ç»Ÿä¸€å­—æ®µåï¼šSQLè¿”å›çš„å¯èƒ½æ˜¯åˆ«åï¼Œéœ€è¦æ˜ å°„å›æ ‡å‡†å­—æ®µå
                if 'SecuCode' in df.columns:
                    df = df.rename(columns={'SecuCode': TABLE_CONFIG['code_field']})
                if 'InnerCode' in df.columns:
                    df = df.rename(columns={'InnerCode': TABLE_CONFIG['inner_code_field']})
                
                # æŒ‰è‚¡ç¥¨ä»£ç åˆ†ç»„
                for code in batch_codes:
                    stock_data = df[df[TABLE_CONFIG['code_field']] == code].copy()
                    if not stock_data.empty:
                        stock_data = stock_data.sort_values('Date')
                        stock_data = stock_data.set_index('Date')
                        
                        # æ ¹æ®æ—¶é—´é…ç½®è°ƒæ•´æ•°æ®èŒƒå›´
                        if time_config is not None:
                            # ç¡®ä¿æ•°æ®åŒ…å«æ‰€éœ€çš„æ—¶é—´èŒƒå›´
                            # ä¼˜å…ˆä½¿ç”¨data_start_dateå’Œdata_end_dateï¼ˆæ¨¡å—17ä½¿ç”¨ï¼‰
                            # å¦‚æœæ²¡æœ‰ï¼Œåˆ™ä½¿ç”¨crash_start_dateå’Œcrash_end_dateï¼ˆæ¨¡å—2ä½¿ç”¨ï¼Œå‘åå…¼å®¹ï¼‰
                            if hasattr(time_config, 'data_start_date') and hasattr(time_config, 'data_end_date'):
                                start_date = pd.Timestamp(time_config.data_start_date)
                                end_date = pd.Timestamp(time_config.data_end_date)
                                analysis_date = end_date
                            elif hasattr(time_config, 'crash_start_date') and hasattr(time_config, 'crash_end_date'):
                                analysis_date = pd.Timestamp(time_config.analysis_date) if hasattr(time_config, 'analysis_date') else pd.Timestamp(time_config.crash_end_date)
                                start_date = pd.Timestamp(time_config.crash_start_date)
                            else:
                                # å¦‚æœæ²¡æœ‰æ—¥æœŸé…ç½®ï¼Œä½¿ç”¨analysis_date
                                analysis_date = pd.Timestamp(time_config.analysis_date) if hasattr(time_config, 'analysis_date') else pd.Timestamp.now()
                                start_date = analysis_date - pd.Timedelta(days=365)
                                end_date = analysis_date
                            
                            # ç­›é€‰åŒ…å«æ‰€éœ€æ—¶é—´èŒƒå›´çš„æ•°æ®
                            mask = (stock_data.index >= start_date - pd.Timedelta(days=30)) & (stock_data.index <= end_date + pd.Timedelta(days=30))
                            stock_data = stock_data[mask]
                        
                        # æ•°æ®ç±»å‹è½¬æ¢
                        numeric_columns = ['Open', 'High', 'Low', 'Close', 'Volume', 'TurnoverRate', 'NegotiableMV']
                        for col in numeric_columns:
                            if col in stock_data.columns:
                                stock_data[col] = pd.to_numeric(stock_data[col], errors='coerce')
                        
                        # SurgedLimitå’ŒDeclineLimitè½¬æ¢ä¸ºæ•´æ•°ï¼ˆ0æˆ–1ï¼‰
                        # é‡è¦ï¼šä¿ç•™NULLå€¼ï¼Œä¸è¦å¡«å……ä¸º0ï¼Œè¿™æ ·å¯ä»¥åŒºåˆ†"æ•°æ®ç¼ºå¤±"å’Œ"éæ¶¨åœ"
                        # æ•°æ®æ ¡éªŒï¼šç¡®ä¿SurgedLimitå­—æ®µå­˜åœ¨
                        if 'SurgedLimit' in stock_data.columns:
                            # è½¬æ¢ä¸ºæ•°å€¼ç±»å‹ï¼Œä½†ä¿ç•™NULLå€¼ï¼ˆä¸å¡«å……ä¸º0ï¼‰
                            stock_data['SurgedLimit'] = pd.to_numeric(stock_data['SurgedLimit'], errors='coerce')
                            # åªå¯¹æœ‰é™å€¼ï¼ˆéNaNã€éinfï¼‰è½¬æ¢ä¸ºæ•´æ•°ï¼Œä¿ç•™NaNå’Œinfä¸ºNaN
                            try:
                                finite_mask = np.isfinite(stock_data['SurgedLimit'])
                                stock_data.loc[finite_mask, 'SurgedLimit'] = stock_data.loc[finite_mask, 'SurgedLimit'].astype(int)
                            except (ValueError, TypeError) as e:
                                # å¦‚æœè½¬æ¢å¤±è´¥ï¼Œä¿ç•™ä¸ºNaNï¼ˆå¯èƒ½æ˜¯æ•°æ®å¼‚å¸¸ï¼‰
                                logger.debug(f"è‚¡ç¥¨ {code} çš„SurgedLimitè½¬æ¢å¤±è´¥: {e}ï¼Œä¿ç•™ä¸ºNaN")
                                stock_data['SurgedLimit'] = pd.NA
                        else:
                            # å¦‚æœç¼ºå°‘SurgedLimitå­—æ®µï¼Œè®°å½•è­¦å‘Šå¹¶è·³è¿‡ï¼ˆé¿å…æµªè´¹æ—¶é—´ï¼‰
                            logger.debug(f"è‚¡ç¥¨ {code} çš„æ•°æ®ç¼ºå°‘SurgedLimitå­—æ®µï¼Œè·³è¿‡è¯¥è‚¡ç¥¨")
                            continue
                        
                        if 'DeclineLimit' in stock_data.columns:
                            # åŒæ ·ä¿ç•™NULLå€¼
                            stock_data['DeclineLimit'] = pd.to_numeric(stock_data['DeclineLimit'], errors='coerce')
                            # åªå¯¹æœ‰é™å€¼ï¼ˆéNaNã€éinfï¼‰è½¬æ¢ä¸ºæ•´æ•°ï¼Œä¿ç•™NaNå’Œinfä¸ºNaN
                            try:
                                finite_mask = np.isfinite(stock_data['DeclineLimit'])
                                stock_data.loc[finite_mask, 'DeclineLimit'] = stock_data.loc[finite_mask, 'DeclineLimit'].astype(int)
                            except (ValueError, TypeError) as e:
                                # å¦‚æœè½¬æ¢å¤±è´¥ï¼Œä¿ç•™ä¸ºNaNï¼ˆå¯èƒ½æ˜¯æ•°æ®å¼‚å¸¸ï¼‰
                                logger.debug(f"è‚¡ç¥¨ {code} çš„DeclineLimitè½¬æ¢å¤±è´¥: {e}ï¼Œä¿ç•™ä¸ºNaN")
                                stock_data['DeclineLimit'] = pd.NA
                        else:
                            stock_data['DeclineLimit'] = pd.NA  # ä½¿ç”¨pd.NAè¡¨ç¤ºç¼ºå¤±
                        
                        # åªä¿ç•™æœ‰å®Œæ•´æ•°æ®çš„è¡Œ
                        stock_data = stock_data.dropna(subset=['Close', 'Volume'])
                        
                        # æ•°æ®æ ¡éªŒï¼šç¡®ä¿SurgedLimitå­—æ®µæœ‰æœ‰æ•ˆå€¼ï¼ˆè‡³å°‘æœ‰ä¸€äº›éç©ºå€¼ï¼‰
                        if not stock_data.empty:
                            # éªŒè¯SurgedLimitå­—æ®µæ˜¯å¦æœ‰æ•ˆï¼ˆè‡³å°‘æœ‰ä¸€äº›æ•°æ®æœ‰å€¼ï¼‰
                            surged_limit_valid = stock_data['SurgedLimit'].notna().any()
                            if not surged_limit_valid:
                                # åªè®°å½•åˆ°æ—¥å¿—ï¼Œä¸æ‰“å°åˆ°æ§åˆ¶å°ï¼ˆé¿å…åˆ·å±ï¼‰
                                logger.debug(f"è‚¡ç¥¨ {code} çš„SurgedLimitå­—æ®µå…¨éƒ¨ä¸ºNULLï¼Œæ•°æ®å¯èƒ½ä¸å®Œæ•´")
                            
                            # è®¡ç®—å¤æƒä»·æ ¼
                            inner_code = code_to_inner.get(code)
                            if inner_code and inner_code in adjusting_factors:
                                stock_data = self.calculate_adjusted_prices(stock_data, adjusting_factors)
                            
                            batch_results[code] = stock_data
                
                logger.info(f"[çº¿ç¨‹ {thread_name}] æ‰¹æ¬¡ {batch_id} å®Œæˆï¼Œè·å– {len(batch_results)} åªè‚¡ç¥¨æ•°æ®")
                return batch_results
                
            except Exception as e:
                # åªè®°å½•åˆ°æ—¥å¿—ï¼Œä¸æ‰“å°åˆ°æ§åˆ¶å°ï¼ˆé¿å…åˆ·å±ï¼‰
                logger.error(f"[çº¿ç¨‹ {thread_name}] æ‰¹æ¬¡ {batch_id} æ‰¹é‡è·å–è‚¡ç¥¨æ•°æ®å¤±è´¥: {e}", exc_info=True)
                return batch_results
        
        # ä½¿ç”¨å¤šçº¿ç¨‹å¹¶è¡Œå¤„ç†æ‰¹æ¬¡
        import concurrent.futures
        import threading
        
        from config import MAX_WORKERS, MAX_CONCURRENT_BATCHES
        # æé™ä¼˜åŒ–ï¼šä½¿ç”¨æå°æ‰¹æ¬¡ + æé™é«˜å¹¶å‘ï¼Œæœ€å¤§åŒ–ç¡¬ä»¶åˆ©ç”¨ç‡
        # æ‰¹æ¬¡å¤§å°å·²ç»å¾ˆå°ï¼ˆ15-25åªï¼‰ï¼Œå¯ä»¥å¤§å¹…æé«˜å¹¶å‘æ•°
        # ç›®æ ‡ï¼šè®©å°½å¯èƒ½å¤šçš„çº¿ç¨‹åŒæ—¶å·¥ä½œï¼Œå‹æ¦¨ç¡¬ä»¶æ€§èƒ½
        max_workers = min(MAX_WORKERS * 5, len(batches), MAX_CONCURRENT_BATCHES, 500)  # æé™æé«˜å¹¶å‘æ•°
        logger.info(f"ä½¿ç”¨ {max_workers} ä¸ªçº¿ç¨‹å¹¶è¡Œå¤„ç† {len(batches)} ä¸ªæ‰¹æ¬¡ï¼ˆæ‰¹æ¬¡å¤§å°: {batch_size}ï¼Œæé™é«˜å¹¶å‘æ¨¡å¼ï¼‰")
        
        all_results = {}
        
        # ä½¿ç”¨tqdmæ˜¾ç¤ºå®æ—¶è¿›åº¦æ¡
        from tqdm import tqdm
        import sys
        
        # ç¡®ä¿è¿›åº¦æ¡èƒ½å¤Ÿæ­£ç¡®æ˜¾ç¤ºï¼ˆåˆ·æ–°è¾“å‡ºï¼‰
        # ä½¿ç”¨printè€Œä¸æ˜¯loggerï¼Œç¡®ä¿è¿›åº¦æ¡å‰æœ‰æç¤ºä¿¡æ¯
        print(f"\n  ğŸ“¥ å¼€å§‹æ‰¹é‡è·å–æ•°æ®: {len(batches)} ä¸ªæ‰¹æ¬¡ï¼Œ{max_workers} ä¸ªçº¿ç¨‹å¹¶è¡Œå¤„ç†...", flush=True)
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_batch = {executor.submit(process_batch, batch_info): batch_info for batch_info in batches}
            
            # åˆ›å»ºè¿›åº¦æ¡ï¼ˆä¼˜åŒ–æ˜¾ç¤ºæ ¼å¼ï¼Œæ˜¾ç¤ºæ­£åœ¨æ‰§è¡Œçš„æ‰¹æ¬¡ï¼‰
            # ä½¿ç”¨mininterval=0.1ç¡®ä¿è¿›åº¦æ¡é¢‘ç¹æ›´æ–°
            pbar = tqdm(
                total=len(batches), 
                desc='ğŸ“¥ æ‰¹é‡è·å–è‚¡ç¥¨æ•°æ®', 
                ncols=120, 
                unit='æ‰¹',
                mininterval=0.1,  # æœ€å°æ›´æ–°é—´éš”0.1ç§’
                maxinterval=1.0,  # æœ€å¤§æ›´æ–°é—´éš”1ç§’
                bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}] {postfix}',
                dynamic_ncols=True,
                file=sys.stdout  # ç¡®ä¿è¾“å‡ºåˆ°æ ‡å‡†è¾“å‡º
            )
            
            # è·Ÿè¸ªæ­£åœ¨æ‰§è¡Œå’Œå·²å®Œæˆçš„ä»»åŠ¡
            completed_count = 0
            running_batches = set()
            last_update_time = time.time()
            
            # å¤„ç†å®Œæˆçš„ä»»åŠ¡
            for future in concurrent.futures.as_completed(future_to_batch):
                batch_info = future_to_batch[future]
                batch_id = batch_info['batch_id']
                
                # ä»æ­£åœ¨æ‰§è¡Œåˆ—è¡¨ä¸­ç§»é™¤
                running_batches.discard(batch_id)
                
                try:
                    batch_results = future.result()
                    all_results.update(batch_results)
                    completed_count += 1
                    
                    # æ›´æ–°è¿›åº¦æ¡
                    pbar.update(1)
                    
                    # æ˜¾ç¤ºæ›´è¯¦ç»†çš„è¿›åº¦ä¿¡æ¯ï¼ŒåŒ…æ‹¬æ­£åœ¨æ‰§è¡Œçš„æ‰¹æ¬¡æ•°
                    total_stocks = len(all_results)
                    running_count = len([f for f in future_to_batch.keys() if not f.done()])
                    
                    # è®¡ç®—é€Ÿåº¦ï¼ˆæ‰¹æ¬¡/ç§’ï¼‰
                    current_time = time.time()
                    elapsed_time = current_time - start_time
                    if elapsed_time > 0:
                        rate = completed_count / elapsed_time
                    else:
                        rate = 0
                    
                    pbar.set_postfix_str(
                        f'å·²è·å–={total_stocks}åª | å·²å®Œæˆ={completed_count}/{len(batches)} | '
                        f'æ‰§è¡Œä¸­={running_count}æ‰¹ | è¿›åº¦={completed_count*100//len(batches)}% | '
                        f'é€Ÿåº¦={rate:.1f}æ‰¹/ç§’'
                    )
                    
                    # å¼ºåˆ¶åˆ·æ–°è¾“å‡º
                    pbar.refresh()
                    
                except Exception as e:
                    logger.error(f"æ‰¹æ¬¡ {batch_id} å¤„ç†å‡ºé”™: {e}")
                    completed_count += 1
                    pbar.update(1)
                    pbar.refresh()
            
            pbar.close()
            print()  # æ¢è¡Œï¼Œç¡®ä¿è¿›åº¦æ¡å…³é—­åè¾“å‡ºæ¸…æ™°
        
        total_time = time.time() - start_time
        
        # æœ€ç»ˆæ•°æ®æ ¡éªŒï¼šç¡®ä¿æ‰€æœ‰è¿”å›çš„æ•°æ®éƒ½åŒ…å«SurgedLimitå­—æ®µ
        validated_results = {}
        invalid_count = 0
        for code, data in all_results.items():
            if data is None or data.empty:
                invalid_count += 1
                continue
            if 'SurgedLimit' not in data.columns:
                logger.warning(f"æ•°æ®æ ¡éªŒå¤±è´¥: è‚¡ç¥¨ {code} çš„æ•°æ®ç¼ºå°‘SurgedLimitå­—æ®µï¼Œå·²æ’é™¤")
                invalid_count += 1
                continue
            validated_results[code] = data
        
        if invalid_count > 0:
            logger.warning(f"æ•°æ®æ ¡éªŒ: {invalid_count} åªè‚¡ç¥¨çš„æ•°æ®æ— æ•ˆï¼ˆç¼ºå°‘SurgedLimitå­—æ®µï¼‰ï¼Œå·²æ’é™¤")
        
        logger.info(f"æ‰¹é‡æ•°æ®è·å–å®Œæˆï¼Œæ€»è€—æ—¶: {total_time:.1f}ç§’ï¼ŒæˆåŠŸè·å– {len(validated_results)} åªè‚¡ç¥¨æ•°æ®ï¼ˆæ ¡éªŒåï¼‰")
        
        return validated_results 

    def get_futures_data(self, option_code: str, days: int = 1250) -> pd.DataFrame:
        """
        è·å–å•†å“æœŸè´§æ•°æ®
        
        Args:
            option_code: å“ç§ä»£ç ï¼Œå¦‚'313'è¡¨ç¤ºé»„é‡‘
            days: è·å–å¤©æ•°ï¼Œé»˜è®¤1250å¤©ï¼ˆçº¦5å¹´ï¼‰
            
        Returns:
            pd.DataFrame: æœŸè´§æ•°æ®ï¼ŒåŒ…å«æ—¥æœŸã€ä»·æ ¼ç­‰ä¿¡æ¯
        """
        try:
            # è®¡ç®—å¼€å§‹æ—¥æœŸ
            end_date = datetime.now()
            start_date = end_date - timedelta(days=days)
            
            sql = """
            SELECT 
                EndDate as TradingDay,
                OptionCode,
                ContractName,
                ClosePrice,
                OpenPrice,
                HighPrice,
                LowPrice,
                Volume,
                Turnover,
                SettlePrice
            FROM Fut_DailyQuote 
            WHERE OptionCode = ?
                AND EndDate >= ?
                AND EndDate <= ?
                AND ClosePrice IS NOT NULL
                AND ClosePrice > 0
            ORDER BY EndDate ASC
            """
            
            params = [option_code, start_date.strftime('%Y-%m-%d'), end_date.strftime('%Y-%m-%d')]
            
            conn = self._get_connection()
            if not conn:
                logger.error("æ— æ³•è·å–æ•°æ®åº“è¿æ¥")
                return pd.DataFrame()
            
            df = pd.read_sql(sql, conn, params=params)
            self._return_connection(conn)
            
            if df.empty:
                logger.warning(f"æœªæ‰¾åˆ°OptionCode={option_code}çš„æœŸè´§æ•°æ®")
                return pd.DataFrame()
            
            # æ•°æ®é¢„å¤„ç†
            df['TradingDay'] = pd.to_datetime(df['TradingDay'])
            df = df.set_index('TradingDay')
            
            # å»é‡ï¼Œä¿ç•™æ¯ä¸ªäº¤æ˜“æ—¥çš„æœ€æ–°æ•°æ®
            df = df.groupby(df.index).last()
            
            logger.info(f"æˆåŠŸè·å–OptionCode={option_code}çš„æœŸè´§æ•°æ®ï¼Œå…±{len(df)}æ¡è®°å½•")
            return df
            
        except Exception as e:
            logger.error(f"è·å–æœŸè´§æ•°æ®å¤±è´¥: {e}")
            return pd.DataFrame()
    
    def get_all_futures_options(self) -> List[str]:
        """
        è·å–æ‰€æœ‰æœŸè´§å“ç§ä»£ç 
        
        Returns:
            List[str]: æœŸè´§å“ç§ä»£ç åˆ—è¡¨
        """
        try:
            sql = """
            SELECT DISTINCT OptionCode
            FROM Fut_DailyQuote 
            WHERE OptionCode IS NOT NULL
                AND OptionCode != ''
            ORDER BY OptionCode
            """
            
            conn = self._get_connection()
            if not conn:
                logger.error("æ— æ³•è·å–æ•°æ®åº“è¿æ¥")
                return []
            
            df = pd.read_sql(sql, conn)
            self._return_connection(conn)
            
            if df.empty:
                logger.warning("æœªæ‰¾åˆ°ä»»ä½•æœŸè´§å“ç§")
                return []
            
            option_codes = df['OptionCode'].astype(str).tolist()
            logger.info(f"æˆåŠŸè·å– {len(option_codes)} ä¸ªæœŸè´§å“ç§")
            return option_codes
            
        except Exception as e:
            logger.error(f"è·å–æœŸè´§å“ç§åˆ—è¡¨å¤±è´¥: {e}")
            return []

    def get_futures_basic_info(self, option_code: str) -> dict:
        """
        è·å–æœŸè´§åŸºæœ¬ä¿¡æ¯
        
        Args:
            option_code: å“ç§ä»£ç 
            
        Returns:
            dict: æœŸè´§åŸºæœ¬ä¿¡æ¯
        """
        try:
            sql = """
            SELECT TOP 1
                OptionCode,
                ContractName,
                Exchange,
                Term
            FROM Fut_DailyQuote 
            WHERE OptionCode = ?
            """
            
            conn = self._get_connection()
            if not conn:
                return {}
            
            cursor = conn.cursor()
            cursor.execute(sql, (option_code,))
            row = cursor.fetchone()
            self._return_connection(conn)
            
            if row:
                return {
                    'OptionCode': row[0],
                    'ContractName': row[1],
                    'Exchange': row[2],
                    'Term': row[3]
                }
            return {}
            
        except Exception as e:
            logger.error(f"è·å–æœŸè´§åŸºæœ¬ä¿¡æ¯å¤±è´¥: {e}")
            return {}

    def get_prev_trading_days(self, end_date, count=1):
        """
        è·å–æŒ‡å®šæ—¥æœŸä¹‹å‰çš„ N ä¸ªäº¤æ˜“æ—¥ï¼ˆä¸å« end_date å½“å¤©ï¼‰ã€‚
        end_date: str 'YYYY-MM-DD' æˆ– date
        count: éœ€è¦çš„äº¤æ˜“æ—¥æ•°é‡
        Returns: list of str ['YYYY-MM-DD', ...] ä»è¿‘åˆ°è¿œ
        """
        try:
            if hasattr(end_date, 'strftime'):
                end_date = end_date.strftime('%Y-%m-%d')
            sql = f"""
            SELECT DISTINCT TOP {count + 1} q.TradingDay
            FROM {TABLE_CONFIG['daily_quote_table']} q
            WHERE q.TradingDay < ?
            ORDER BY q.TradingDay DESC
            """
            df = self.query(sql, params=[end_date])
            if df.empty:
                return []
            dates = pd.to_datetime(df['TradingDay']).dt.strftime('%Y-%m-%d').tolist()
            return dates[:count] if count else dates
        except Exception as e:
            logger.error(f"get_prev_trading_days å¤±è´¥: {e}")
            return []

    def get_trading_days_inclusive(self, end_date, count=60):
        """
        è·å–æˆªæ­¢åˆ° end_dateï¼ˆå«ï¼‰çš„æœ€è¿‘ count ä¸ªäº¤æ˜“æ—¥ï¼Œä»æ—©åˆ°æ™šæ’åºã€‚
        """
        try:
            if hasattr(end_date, 'strftime'):
                end_date = end_date.strftime('%Y-%m-%d')
            sql = f"""
            SELECT DISTINCT TOP {count} q.TradingDay
            FROM {TABLE_CONFIG['daily_quote_table']} q
            WHERE q.TradingDay <= ?
            ORDER BY q.TradingDay DESC
            """
            df = self.query(sql, params=[end_date])
            if df.empty:
                return []
            dates = pd.to_datetime(df['TradingDay']).dt.strftime('%Y-%m-%d').tolist()
            return list(reversed(dates))
        except Exception as e:
            logger.error(f"get_trading_days_inclusive å¤±è´¥: {e}")
            return []

    def get_all_stocks_daily_with_preclose(self, trading_day, exclude_suspended=True):
        """
        å…¨å¸‚åœºå•æ—¥è¡Œæƒ…ï¼ˆå«æ˜¨æ”¶ï¼‰ï¼Œç”¨äºå¸‚åœºæƒ…ç»ªç­‰ã€‚å‰”é™¤å½“å¤©æ— æˆäº¤çš„è‚¡ç¥¨ï¼ˆè§†ä¸ºåœç‰Œï¼‰ã€‚
        è¿”å› DataFrame åˆ—: SecuCode, Date, Open, High, Low, Close, PreClose
        """
        try:
            prev_days = self.get_prev_trading_days(trading_day, count=1)
            if not prev_days:
                logger.warning(f"æ— æ³•è·å– {trading_day} çš„å‰ä¸€äº¤æ˜“æ—¥")
                return pd.DataFrame()
            prev_day = prev_days[0]
            sql = f"""
            SELECT
                s.{TABLE_CONFIG['code_field']} AS SecuCode,
                q.{TABLE_CONFIG['date_field']} AS [Date],
                q.{TABLE_CONFIG['open_field']} AS [Open],
                q.{TABLE_CONFIG['high_field']} AS [High],
                q.{TABLE_CONFIG['low_field']} AS [Low],
                q.{TABLE_CONFIG['close_field']} AS [Close]
            FROM {TABLE_CONFIG['stock_info_table']} s
            INNER JOIN {TABLE_CONFIG['daily_quote_table']} q
                ON s.{TABLE_CONFIG['inner_code_field']} = q.{TABLE_CONFIG['inner_code_field']}
            WHERE {STOCK_FILTER}
              AND q.{TABLE_CONFIG['date_field']} IN (?, ?)
              AND q.ClosePrice IS NOT NULL AND q.ClosePrice > 0
            ORDER BY s.{TABLE_CONFIG['code_field']}, q.{TABLE_CONFIG['date_field']}
            """
            df = self.query(sql, params=[prev_day, trading_day])
            if df.empty:
                return pd.DataFrame()
            df['Date'] = pd.to_datetime(df['Date']).dt.strftime('%Y-%m-%d')
            # æŒ‰è‚¡ç¥¨åˆ†ç»„ï¼Œå–æ˜¨æ”¶ä¸ä»Šæ”¶
            prev = df[df['Date'] == prev_day][['SecuCode', 'Close']].rename(columns={'Close': 'PreClose'})
            curr = df[df['Date'] == str(trading_day)][['SecuCode', 'Date', 'Open', 'High', 'Low', 'Close']]
            curr = curr.merge(prev, on='SecuCode', how='inner')
            for col in ['Open', 'High', 'Low', 'Close', 'PreClose']:
                curr[col] = pd.to_numeric(curr[col], errors='coerce')
            curr = curr.dropna(subset=['Close', 'PreClose'])
            if exclude_suspended:
                curr = curr[(curr['Open'] > 0) & (curr['Close'].notna())]
            logger.info(f"get_all_stocks_daily_with_preclose: {trading_day} å…± {len(curr)} åªè‚¡ç¥¨")
            return curr
        except Exception as e:
            logger.error(f"get_all_stocks_daily_with_preclose å¤±è´¥: {e}", exc_info=True)
            return pd.DataFrame()

    def get_all_stocks_returns_for_dates(self, date_list, on_progress=None):
        """
        æ‰¹é‡è·å–å¤šæ—¥å…¨å¸‚åœºè¡Œæƒ…å¹¶è®¡ç®—æ—¥æ”¶ç›Šç‡ Rã€‚
        date_list: å·²æŒ‰æ—¶é—´å‡åºæ’åˆ—çš„äº¤æ˜“æ—¥åˆ—è¡¨ ['YYYY-MM-DD', ...]
        on_progress: å¯é€‰å›è°ƒ on_progress(batch_idx, total_batches, rows_so_far, batch_label)
        æŒ‰ 120 å¤©åˆ†æ‰¹ BETWEEN æŸ¥è¯¢ï¼Œå¤šçº¿ç¨‹å¹¶è¡Œï¼ˆå—è¿æ¥æ± å¤§å°çº¦æŸï¼‰ã€‚
        """
        if not date_list:
            return pd.DataFrame()
        try:
            from concurrent.futures import ThreadPoolExecutor, as_completed
            import threading as _th

            date_set = set(date_list)

            base_cols = f"""
                    s.{TABLE_CONFIG['code_field']} AS SecuCode,
                    q.{TABLE_CONFIG['date_field']} AS [Date],
                    q.{TABLE_CONFIG['open_field']} AS [Open],
                    q.{TABLE_CONFIG['high_field']} AS [High],
                    q.{TABLE_CONFIG['low_field']} AS [Low],
                    q.{TABLE_CONFIG['close_field']} AS [Close],
                    q.TurnoverValue AS Amount,
                    q.{TABLE_CONFIG['turnover_rate_field']} AS TurnoverRate,
                    q.{TABLE_CONFIG['negotiable_mv_field']} AS NegotiableMV"""
            base_from = f"""
                FROM {TABLE_CONFIG['stock_info_table']} s
                INNER JOIN {TABLE_CONFIG['daily_quote_table']} q
                    ON s.{TABLE_CONFIG['inner_code_field']} = q.{TABLE_CONFIG['inner_code_field']}
                WHERE {STOCK_FILTER}
                  AND q.ClosePrice IS NOT NULL AND q.ClosePrice > 0"""
            sql_tpl = f"""SELECT {base_cols} {base_from}
                  AND q.{TABLE_CONFIG['date_field']} BETWEEN ? AND ?
                ORDER BY s.{TABLE_CONFIG['code_field']}, q.{TABLE_CONFIG['date_field']}"""

            pool_size = len(self._connection_pool) if self._connection_pool else 4
            usable_conns = min(pool_size, 6)

            if len(date_list) <= 600:
                batches = [(date_list[0], date_list[-1])]
            else:
                target_batches = min(usable_conns, 4)
                BATCH_DAYS = max(200, len(date_list) // target_batches + 1)
                batches = []
                for i in range(0, len(date_list), BATCH_DAYS):
                    chunk = date_list[i:i + BATCH_DAYS]
                    batches.append((chunk[0], chunk[-1]))

            total_batches = len(batches)
            workers = min(usable_conns, total_batches)
            logger.info(f"get_all_stocks_returns_for_dates: {len(date_list)} å¤©, "
                        f"åˆ† {total_batches} æ‰¹, {workers} çº¿ç¨‹å¹¶è¡ŒæŸ¥è¯¢")

            _done_count = [0]
            _total_rows = [0]
            _lock = _th.Lock()

            def _fetch_one(idx_batch):
                idx, (d_min, d_max) = idx_batch
                part = self.query(sql_tpl, params=[d_min, d_max])
                part_rows = len(part) if part is not None else 0
                with _lock:
                    _done_count[0] += 1
                    _total_rows[0] += part_rows
                    done = _done_count[0]
                    rows = _total_rows[0]
                label = f'{d_min} ~ {d_max}'
                logger.info(f"  æ‰¹æ¬¡å®Œæˆ {done}/{total_batches}: {label} â†’ {part_rows:,} è¡Œ")
                if on_progress:
                    on_progress(done, total_batches, rows, label)
                return part

            results = []
            with ThreadPoolExecutor(max_workers=workers) as executor:
                futures = {executor.submit(_fetch_one, (i, b)): i
                           for i, b in enumerate(batches)}
                for future in as_completed(futures):
                    part = future.result()
                    if part is not None and not part.empty:
                        results.append(part)

            if not results:
                return pd.DataFrame()

            df = pd.concat(results, ignore_index=True)
            df['Date'] = pd.to_datetime(df['Date']).dt.strftime('%Y-%m-%d')
            for col in ['Open', 'High', 'Low', 'Close', 'Amount', 'TurnoverRate', 'NegotiableMV']:
                if col in df.columns:
                    df[col] = pd.to_numeric(df[col], errors='coerce')
            df = df.sort_values(['SecuCode', 'Date'])
            df['PreClose'] = df.groupby('SecuCode')['Close'].shift(1)
            df = df.dropna(subset=['PreClose', 'Close'])
            df['R'] = df['Close'] / df['PreClose'] - 1
            df = df[df['Date'].isin(date_set)]
            logger.info(f"get_all_stocks_returns_for_dates: å®Œæˆ, å…± {len(df):,} æ¡æœ‰æ•ˆè®°å½•")
            return df
        except Exception as e:
            logger.error(f"get_all_stocks_returns_for_dates å¤±è´¥: {e}", exc_info=True)
            return pd.DataFrame()

    def get_index_quote_for_dates(self, inner_codes, start_date, end_date):
        """
        è·å–æŒ‡æ•°åœ¨æ—¥æœŸåŒºé—´å†…çš„è¡Œæƒ…ã€‚QT_IndexQuote + SecuMainã€‚
        è¿”å› DataFrame: InnerCode, SecuAbbr, ChiName, TradingDay, PrevClosePrice, OpenPrice, HighPrice, LowPrice, ClosePrice, TurnoverValue, ChangePCT
        """
        try:
            codes_str = ','.join([str(c) for c in inner_codes])
            sql = f"""
            SELECT
                qti.InnerCode,
                sm.SecuAbbr,
                sm.ChiName,
                qti.TradingDay,
                qti.PrevClosePrice,
                qti.OpenPrice,
                qti.HighPrice,
                qti.LowPrice,
                qti.ClosePrice,
                qti.TurnoverValue,
                qti.ChangePCT
            FROM QT_IndexQuote qti
            LEFT JOIN SecuMain sm ON qti.InnerCode = sm.InnerCode
            WHERE sm.SecuCategory = 4
              AND sm.ListedState = 1
              AND qti.TradingDay >= ?
              AND qti.TradingDay <= ?
              AND qti.InnerCode IN ({codes_str})
            ORDER BY qti.InnerCode, qti.TradingDay
            """
            df = self.query(sql, params=[start_date, end_date])
            if not df.empty:
                df['TradingDay'] = pd.to_datetime(df['TradingDay']).dt.strftime('%Y-%m-%d')
            return df
        except Exception as e:
            logger.error(f"get_index_quote_for_dates å¤±è´¥: {e}", exc_info=True)
            return pd.DataFrame()

    def get_broad_index_quotes(self, trading_day):
        """
        è·å–å®½åŸº/ç­–ç•¥æŒ‡æ•°å½“æ—¥è¡Œæƒ…ï¼ˆç”¨æˆ·æä¾›çš„ InnerCode åˆ—è¡¨ï¼‰ã€‚
        è¿”å› DataFrame: InnerCode, SecuAbbr, ChiName, TradingDay, PrevClosePrice, OpenPrice, HighPrice, LowPrice, ClosePrice, TurnoverValue, ChangePCT, Category
        """
        inner_codes = [
            1, 1055, 11089, 3145, 4978,
            46, 30, 4074, 39144, 36324,
            6973, 4078, 3036, 7544, 39376, 31398, 48542, 19475, 217313,
            3469, 3471, 4089, 225892, 303968
        ]
        df = self.get_index_quote_for_dates(inner_codes, trading_day, trading_day)
        if df.empty:
            return df
        def _cat(ic):
            if ic in (1, 1055, 11089, 3145, 4978):
                return 'å¸‚åœºç»¼åˆåŸºå‡†'
            if ic in (46, 30, 4074, 39144, 36324):
                return 'è§„æ¨¡ä¸é£æ ¼'
            if ic in (6973, 4078, 3036, 7544, 39376, 31398, 48542, 19475, 217313):
                return 'ä¸»é¢˜ä¸ç­–ç•¥'
            if ic in (3469, 3471, 4089, 225892, 303968):
                return 'å…¶ä»–å¸¸ç”¨å®½åŸºä¸ç­–ç•¥'
            return 'å…¶ä»–'
        df['Category'] = df['InnerCode'].map(_cat)
        return df

    def get_broad_index_series_for_ma(self, trading_day, lookback_days=20):
        """
        è·å–å®½åŸºæŒ‡æ•°æœ€è¿‘ lookback_days ä¸ªäº¤æ˜“æ—¥è¡Œæƒ…ï¼Œç”¨äºè®¡ç®—å‡çº¿ï¼ˆå¦‚ MA20ï¼‰ã€‚
        è¿”å› DataFrame: InnerCode, SecuAbbr, ChiName, TradingDay, ClosePrice, ...ï¼ˆæŒ‰ InnerCode, TradingDay å‡åºï¼‰
        """
        inner_codes = [
            1, 1055, 11089, 3145, 4978,
            46, 30, 4074, 39144, 36324,
            6973, 4078, 3036, 7544, 39376, 31398, 48542, 19475, 217313,
            3469, 3471, 4089, 225892, 303968
        ]
        dates = self.get_trading_days_inclusive(trading_day, count=lookback_days)
        if not dates:
            return pd.DataFrame()
        start_date, end_date = dates[0], dates[-1]
        df = self.get_index_quote_for_dates(inner_codes, start_date, end_date)
        if df.empty or 'ClosePrice' not in df.columns:
            return df
        df['ClosePrice'] = pd.to_numeric(df['ClosePrice'], errors='coerce')
        return df